{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2415796d-09d6-4732-80d6-b592d076cc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/velox-217533/anaconda3/envs/fau_snn_torch-cuda/lib/python3.12/site-packages/brevitas/graph/equalize.py:69: UserWarning: fast_hadamard_transform package not found, using standard pytorch kernels\n",
      "  warnings.warn(\"fast_hadamard_transform package not found, using standard pytorch kernels\")\n",
      "/home/velox-217533/anaconda3/envs/fau_snn_torch-cuda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen, utils, surrogate\n",
    "from snntorch.functional import quant\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "# from brevitas.nn import QuantConv1d, QuantIdentity, QuantLinear, BatchNorm1dToQuantScaleBias\n",
    "from brevitas.quant import Int8WeightPerTensorFloat, Int8ActPerTensorFloat, Int32Bias\n",
    "from brevitas.export import export_qonnx\n",
    "from brevitas.core.scaling      import ScalingImplType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.quant        import QuantType\n",
    "\n",
    "\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "#from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv, copy, glob\n",
    "import imblearn, imblearn.over_sampling\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize, normalize\n",
    "from scipy.signal import butter, lfilter, freqz\n",
    "\n",
    "import os, sys, time, datetime, argparse, json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8f3951-b67e-455c-87c6-3fe3354ab33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # for multi-GPU setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2845e89-5418-494b-8ced-c441c5b0e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_row_to_range(row, low=-4.0, high=4.0):\n",
    "#     rmin, rmax = row.min(), row.max()\n",
    "#     denom = (rmax - rmin) if rmax > rmin else 1e-8\n",
    "#     return low + (row - rmin) * (high - low) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018f70e-05a4-45b2-a8c4-b80d1a89cc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e300c1f1-eb95-44ad-9447-7be1b8a18ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_data(\n",
    "    mitbih_train_path='/...../data/mitbih_processed_intra_patient_4class_180_center90_filtered/train',\n",
    "    folders=None, random_state=42,\n",
    "    max_files_per_folder=None\n",
    "):\n",
    "    if folders is None:\n",
    "        folders = ['normal', 'sveb', 'veb', 'f']\n",
    "    label_mapping = {'normal': 0, 'sveb': 1, 'veb': 2, 'f': 3}\n",
    "    \n",
    "    def scan_folder(base_path):\n",
    "        X_parts, y_parts = [], []\n",
    "        expected_cols = None\n",
    "    \n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            csvs = sorted(\n",
    "                glob.glob(os.path.join(folder_path, '*.csv')) +\n",
    "                glob.glob(os.path.join(folder_path, '*.CSV'))\n",
    "            )\n",
    "            if not csvs:\n",
    "                print(f'Warning: no CSVs found in: {folder_path}')\n",
    "                continue\n",
    "    \n",
    "            for fpath in csvs:\n",
    "                df = pd.read_csv(\n",
    "                    fpath, dtype=np.float32, engine='c',\n",
    "                    usecols=lambda c: c != 'Unnamed: 0'\n",
    "                )\n",
    "                if df.shape[0] == 0:\n",
    "                    df = pd.read_csv(fpath, header=None, dtype=np.float32, engine='c')\n",
    "    \n",
    "                df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "                arr = df.to_numpy(copy=False)\n",
    "                if expected_cols is None:\n",
    "                    expected_cols = arr.shape[1]\n",
    "                elif arr.shape[1] != expected_cols:\n",
    "                    raise ValueError(\n",
    "                        f'Inconsistent column count: {fpath} has {arr.shape[1]}, expected {expected_cols}'\n",
    "                    )\n",
    "    \n",
    "                X_parts.append(arr)\n",
    "                y_parts.extend([label_mapping[folder]] * arr.shape[0])\n",
    "    \n",
    "        if not X_parts:\n",
    "            raise FileNotFoundError(f'No usable CSV rows under {base_path} (folders={folders})')\n",
    "    \n",
    "        X = np.vstack(X_parts).astype(np.float32, copy=False)\n",
    "        y = np.asarray(y_parts, dtype=np.int64)\n",
    "        return X, y\n",
    "    \n",
    "    # Load and shuffle\n",
    "    X_mit, y_mit = scan_folder(mitbih_train_path)\n",
    "    print(f'Loaded: MIT-BIH {X_mit.shape}')\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    perm = rng.permutation(len(y_mit))\n",
    "    X = X_mit[perm]\n",
    "    y = y_mit[perm]\n",
    "    \n",
    "    # OPTION 3: NO SCALING - data already z-score normalized from preprocessing\n",
    "    # Data will have mean≈0, std≈1, range≈[-3, 3]\n",
    "    print(f\"Data range after z-score normalization: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "    print(f\"Data mean: {X.mean():.4f}, std: {X.std():.4f}\")\n",
    "    \n",
    "    # Handle any NaNs just in case\n",
    "    X = np.nan_to_num(X, nan=0.0).astype(np.float32, copy=False)\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    X_train_tensor = torch.from_numpy(X).unsqueeze(1)   # (N, 1, L)\n",
    "    y_train = torch.from_numpy(y)                       # (N,)\n",
    "    \n",
    "    print(\"Class distribution:\", Counter(y_train.tolist()))\n",
    "    print(\"X train shape:\", tuple(X_train_tensor.shape))\n",
    "    print(\"y_train shape:\", tuple(y_train.shape))\n",
    "    print(\"Data tensor loaded successfully\")\n",
    "    \n",
    "    return X_train_tensor, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154e7548-ff76-4f5d-bfa0-c4ac03c31ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csnn_datasets(X_train, y_train, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Create TensorDataset objects for SNN training.\n",
    "\n",
    "    Args:\n",
    "        X_train_spikes (torch.Tensor): Spike-encoded training data\n",
    "        y_train (torch.Tensor): Training labels\n",
    "        X_test_spikes (torch.Tensor): Spike-encoded testing data\n",
    "        y_test (torch.Tensor): Testing labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset)\n",
    "    \"\"\"\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    if X_test is None:\n",
    "        print(\"dataset created successfully\")\n",
    "        return train_dataset\n",
    "    else:\n",
    "        test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af156a6e-9c64-4fc3-a31a-0ae61db9e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_tensor, y_train = load_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcd749-b60f-49de-a8bf-d2aa8d7a3819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e99a5c7-92f9-48b1-9911-eef93fb9a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qcsnn_model_4class(num_bits=8, input_size=180, stride4=1, kernel_size=3, \n",
    "                               dropout4=0.35, beta4=0.5, slope4=25, \n",
    "                               threshold4=0.5, learn_beta4=True, learn_threshold4=True):\n",
    "    \"\"\"Factory function for 4-class QCSNN with 3 conv blocks + 2 linear blocks\"\"\"\n",
    "    \n",
    "    spike_grad4 = snn.surrogate.fast_sigmoid(slope=slope4)\n",
    "    \n",
    "    # Calculate output sizes after 3 conv blocks\n",
    "    output_size1 = (input_size - kernel_size) // stride4 + 1\n",
    "    output_size1 = output_size1 // 2  # MaxPool\n",
    "    \n",
    "    output_size2 = (output_size1 - kernel_size) // stride4 + 1\n",
    "    output_size2 = output_size2 // 2  # MaxPool\n",
    "    \n",
    "    output_size3 = (output_size2 - kernel_size) // stride4 + 1\n",
    "    output_size3 = output_size3 // 2  # MaxPool\n",
    "    \n",
    "    flattened_size = output_size3 * 24  # 24 channels from 3rd conv block\n",
    "    \n",
    "    print(f\"Output sizes: Block1={output_size1}, Block2={output_size2}, Block3={output_size3}\")\n",
    "    print(f\"Flattened size: {flattened_size}\")\n",
    "    \n",
    "    model = torch.nn.Sequential(OrderedDict([\n",
    "        # ── Input quantiser ──────────────────────────────\n",
    "        (\"qcsnet4_cblk1_input\",\n",
    "         qnn.QuantIdentity(bit_width=num_bits, return_quant_tensor=True)),\n",
    "    \n",
    "        # ── First Conv block ─────────────────────────────\n",
    "        (\"qcsnet4_cblk1_qconv1d\",\n",
    "         qnn.QuantConv1d(\n",
    "             1, 16, 3, stride=stride4, bias=False,\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             return_quant_tensor=True)),\n",
    "    \n",
    "        (\"qcsnet4_cblk1_batch_norm\",\n",
    "         qnn.BatchNorm1dToQuantScaleBias(\n",
    "             16,\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             bias_quant=Int32Bias,\n",
    "             return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet4_cblk1_leaky\",\n",
    "         snn.Leaky(beta=beta4, learn_beta=learn_beta4,\n",
    "                   spike_grad=spike_grad4,\n",
    "                   threshold=threshold4, learn_threshold=learn_threshold4,\n",
    "                   init_hidden=True)),\n",
    "        \n",
    "        (\"qcsnet4_cblk1_max_pool\", torch.nn.MaxPool1d(2, 2)),\n",
    "    \n",
    "        # ── Second Conv block ────────────────────────────\n",
    "        (\"qcsnet4_cblk2_input\",\n",
    "         qnn.QuantIdentity(bit_width=num_bits, return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet4_cblk2_qconv1d\",\n",
    "         qnn.QuantConv1d(\n",
    "             16, 16, 3, stride=stride4, bias=False,  # 16→16 channels\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             return_quant_tensor=True)),\n",
    "    \n",
    "        (\"qcsnet4_cblk2_batch_norm\",\n",
    "         qnn.BatchNorm1dToQuantScaleBias(\n",
    "             16,  # 16 channels\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             bias_quant=Int32Bias,\n",
    "             return_quant_tensor=True)),\n",
    "    \n",
    "        (\"qcsnet4_cblk2_leaky\",\n",
    "         snn.Leaky(beta=beta4, learn_beta=learn_beta4,\n",
    "                   spike_grad=spike_grad4,\n",
    "                   threshold=threshold4, learn_threshold=learn_threshold4,\n",
    "                   init_hidden=True)),\n",
    "        \n",
    "        (\"qcsnet4_cblk2_max_pool\", torch.nn.MaxPool1d(2, 2)),\n",
    "    \n",
    "        # ── Third Conv block ────────────────────────────\n",
    "        (\"qcsnet4_cblk3_input\",\n",
    "         qnn.QuantIdentity(bit_width=num_bits, return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet4_cblk3_qconv1d\",\n",
    "         qnn.QuantConv1d(\n",
    "             16, 24, 3, stride=stride4, bias=False,  # 16→24 channels\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             return_quant_tensor=True)),\n",
    "    \n",
    "        (\"qcsnet4_cblk3_batch_norm\",\n",
    "         qnn.BatchNorm1dToQuantScaleBias(\n",
    "             24,  # 24 channels\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             bias_quant=Int32Bias,\n",
    "             return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet4_cblk3_leaky\",\n",
    "         snn.Leaky(beta=beta4, learn_beta=learn_beta4,\n",
    "                   spike_grad=spike_grad4,\n",
    "                   threshold=threshold4, learn_threshold=learn_threshold4,\n",
    "                   init_hidden=True)),\n",
    "        \n",
    "        (\"qcsnet4_cblk3_max_pool\", torch.nn.MaxPool1d(2, 2)),\n",
    "    \n",
    "        # ── Dense head (2 linear blocks) ──────────────────────────────\n",
    "        (\"qcsnet4_flatten\", torch.nn.Flatten()),\n",
    "        \n",
    "        # Linear block 1\n",
    "        (\"qcsnet4_lblk1_input\",\n",
    "         qnn.QuantIdentity(bit_width=num_bits, return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet4_lblk1_qlinear\",\n",
    "         qnn.QuantLinear(\n",
    "             flattened_size, 128, bias=False,  # Hidden layer with 128 units\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet4_lblk1_leaky\",\n",
    "         snn.Leaky(beta=beta4, learn_beta=learn_beta4,\n",
    "                   spike_grad=spike_grad4,\n",
    "                   threshold=threshold4, learn_threshold=learn_threshold4,\n",
    "                   init_hidden=True)),\n",
    "        \n",
    "        # Linear block 2\n",
    "        (\"qcsnet4_lblk2_input\",\n",
    "         qnn.QuantIdentity(bit_width=num_bits, return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet4_lblk2_qlinear\",\n",
    "         qnn.QuantLinear(\n",
    "             128, 4, bias=False,  # Output layer: 4 classes\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet4_lblk2_leaky\",\n",
    "         snn.Leaky(beta=beta4, learn_beta=learn_beta4,\n",
    "                   spike_grad=spike_grad4,\n",
    "                   threshold=threshold4, learn_threshold=learn_threshold4,\n",
    "                   init_hidden=True, output=True)),\n",
    "    ]))\n",
    "    \n",
    "    # CRITICAL FIX: Manually override runtime_shape for all 3 BatchNorm layers\n",
    "    model.qcsnet4_cblk1_batch_norm.runtime_shape = (1, -1, 1)\n",
    "    model.qcsnet4_cblk2_batch_norm.runtime_shape = (1, -1, 1)\n",
    "    model.qcsnet4_cblk3_batch_norm.runtime_shape = (1, -1, 1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def forward_pass(model, num_steps, data):\n",
    "    \"\"\"Run SNN for num_steps and collect spike/membrane recordings\"\"\"\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "    utils.reset(model)  # resets hidden states for all LIF neurons in net\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        spk_out, mem_out = model(data)\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "    \n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb764e-c818-4539-aa47-2c6eadcf235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdf029e-012b-483c-a693-ef4cddb9b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model4, dataloader, loss_func, optimizer4, device, num_steps):\n",
    "    train4_loss, train4_correct = 0.0, 0\n",
    "    model4.train()\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer4.zero_grad()\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets4 = targets.to(device, non_blocking=True).long()\n",
    "        \n",
    "        # Forward pass - returns [num_steps, batch, 4]\n",
    "        output4, _ = forward_pass(model4, num_steps, inputs) \n",
    "        \n",
    "        # TET: Compute loss at EACH timestep, then average\n",
    "        loss4 = torch.stack([loss_func(output4[t], targets4) for t in range(num_steps)]).mean()\n",
    "        \n",
    "        loss4.backward()\n",
    "        optimizer4.step()\n",
    "        \n",
    "        # For accuracy, use mean across timesteps (same as before)\n",
    "        output4_mean = output4.mean(0)\n",
    "        bs = targets4.size(0)\n",
    "        train4_loss += loss4.item() * bs\n",
    "        train4_correct += (output4_mean.argmax(1) == targets4).sum().item()\n",
    "    \n",
    "    return train4_loss, train4_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2c2e27-1807-458f-ac1e-8b554216bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation_epoch(model4, dataloader, loss_func, device, num_steps):\n",
    "    \"\"\"\n",
    "    Validate a 4-class model on the dataloader.\n",
    "    Returns dictionary with per-class metrics + macro averages.\n",
    "    \"\"\"\n",
    "    model4.eval()\n",
    "    \n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Metric containers\n",
    "    # ------------------------------------------------------------------ #\n",
    "    metrics4 = {\n",
    "        'loss': 0., 'acc': 0.,\n",
    "        'precision': [0., 0., 0., 0.],         # per-class lists\n",
    "        'recall':    [0., 0., 0., 0.],\n",
    "        'specificity': [0., 0., 0., 0.],\n",
    "        'f1-score': [0., 0., 0., 0.],\n",
    "    }\n",
    "   \n",
    "    valid4_loss = 0.0\n",
    "    valid4_correct = 0\n",
    " \n",
    "    # confusion-matrix elements - one array per metric and per class\n",
    "    tp4 = [0, 0, 0, 0]\n",
    "    fp4 = [0, 0, 0, 0]\n",
    "    tn4 = [0, 0, 0, 0]\n",
    "    fn4 = [0, 0, 0, 0]\n",
    "    n_seen = 0\n",
    "    \n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Validation loop\n",
    "    # ------------------------------------------------------------------ #\n",
    "    for x, y_multi in dataloader:\n",
    "        x = x.to(device, non_blocking=True) \n",
    "        y_multi = y_multi.to(device, non_blocking=True).long()\n",
    "      \n",
    "        # Forward pass - returns [num_steps, batch, 4]\n",
    "        out4, _ = forward_pass(model4, num_steps, x)\n",
    "        \n",
    "        # TET: Compute loss at each timestep, then average\n",
    "        loss4 = torch.stack([loss_func(out4[t], y_multi) for t in range(num_steps)]).mean()\n",
    "        \n",
    "        # For predictions, use average across timesteps\n",
    "        out4_mean = out4.mean(0)\n",
    "        pred4 = out4_mean.argmax(1)\n",
    "        \n",
    "        bs = y_multi.size(0)\n",
    "        valid4_loss += loss4.item() * bs\n",
    "        valid4_correct += (pred4 == y_multi).sum().item()\n",
    "        n_seen += bs\n",
    "        \n",
    "        # update multi-class confusion matrix\n",
    "        for i in (0, 1, 2, 3):\n",
    "            tp4[i] += ((pred4 == i) & (y_multi == i)).sum().item()\n",
    "            fp4[i] += ((pred4 == i) & (y_multi != i)).sum().item()\n",
    "            tn4[i] += ((pred4 != i) & (y_multi != i)).sum().item()\n",
    "            fn4[i] += ((pred4 != i) & (y_multi == i)).sum().item()\n",
    "    \n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Aggregate losses & accuracies (per-sample averages)\n",
    "    # ------------------------------------------------------------------ #\n",
    "    metrics4['loss'] = valid4_loss / max(n_seen, 1)\n",
    "    metrics4['acc']  = 100.0 * valid4_correct / max(n_seen, 1)\n",
    "    \n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Per-class metrics + macro averages\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def per_class_metrics(tp, fp, tn, fn):\n",
    "        EPS = 1e-8          # numerical safety\n",
    "        prec = [t / (t + f + EPS) for t, f in zip(tp, fp)]\n",
    "        rec  = [t / (t + f + EPS) for t, f in zip(tp, fn)]\n",
    "        spec = [t / (t + f + EPS) for t, f in zip(tn, fp)]\n",
    "        f1   = [2*p*r / (p + r + EPS) for p, r in zip(prec, rec)]\n",
    "        macro = {\n",
    "            'precision_macro': sum(prec)/len(prec),\n",
    "            'recall_macro':    sum(rec)/len(rec),\n",
    "            'specificity_macro': sum(spec)/len(spec),\n",
    "            'f1_macro':        sum(f1)/len(f1),\n",
    "        }\n",
    "        return prec, rec, spec, f1, macro\n",
    "    \n",
    "    # Calculate metrics\n",
    "    prec4, rec4, spec4, f14, macro4 = per_class_metrics(tp4, fp4, tn4, fn4)\n",
    "    metrics4['precision']   = prec4\n",
    "    metrics4['recall']      = rec4\n",
    "    metrics4['specificity'] = spec4\n",
    "    metrics4['f1-score']    = f14\n",
    "    metrics4.update(macro4)\n",
    "    \n",
    "    return metrics4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef763d7-586b-449b-91ed-c3760fb67541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b0159c0-b166-4197-ad29-5ee60993673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def _clear_cuda_cache():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# NOTE: keep your existing Brevitas warmup if you already have one.\n",
    "# This stub assumes forward_pass(model, num_steps, x) exists.\n",
    "@torch.no_grad()\n",
    "def _brevitas_warmup(model, dataset, device, num_steps=10, bs=8, n_steps=10):\n",
    "    loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=0,\n",
    "                        pin_memory=(device.type == \"cuda\"))\n",
    "    it = iter(loader)\n",
    "    for _ in range(n_steps):\n",
    "        try:\n",
    "            x, _ = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader)\n",
    "            x, _ = next(it)\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        out, _ = forward_pass(model, num_steps, x)  # (T,B,2) expected\n",
    "        _ = out.mean(0)  # just force execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cfff0d8-5003-48c1-a60e-ff990b783c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cv_4class(model_factory, epochs, dataset, device,\n",
    "                          loss_func, optimizer_class, optimizer_kwargs,\n",
    "                          num_steps=10, k_folds=6, batch_size=128,\n",
    "                          monitor=\"f1_macro\", mode=\"max\"):\n",
    "    \"\"\"\n",
    "    K-fold CV with fresh model per fold for 4-class classification.\n",
    "    Uses per-fold class-weighted CrossEntropyLoss.\n",
    "    Classes: 0=Normal, 1=SVEB, 2=VEB, 3=F\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    # ---- Brevitas warmup template ----\n",
    "    print(\"Creating template model for warmup...\")\n",
    "    model_template = model_factory().to(device)\n",
    "    model_template.train()\n",
    "\n",
    "    print(\"Running Brevitas warmup...\")\n",
    "    _brevitas_warmup(model_template, dataset, device, num_steps=num_steps, bs=8, n_steps=10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        warmup_state = {k: v.detach().cpu().clone()\n",
    "                        for k, v in model_template.state_dict().items()}\n",
    "\n",
    "    scaling_keys = sum('scaling_impl.value' in k for k in warmup_state.keys())\n",
    "    print(f\"Scaling keys after warmup: {scaling_keys}\")\n",
    "    if scaling_keys == 0:\n",
    "        print(\"WARNING: No scaling_impl.value keys found!\")\n",
    "\n",
    "    del model_template\n",
    "    _clear_cuda_cache()\n",
    "\n",
    "    # ---- Label helpers ----\n",
    "    def all_labels(ds):\n",
    "        if hasattr(ds, 'tensors') and len(ds.tensors) >= 2:\n",
    "            return ds.tensors[1].detach().cpu().long().numpy()\n",
    "        return np.asarray([ds[i][1] for i in range(len(ds))], dtype=np.int64)\n",
    "\n",
    "    def subset_labels(sub: Subset):\n",
    "        base, idx = sub.dataset, sub.indices\n",
    "        if hasattr(base, 'tensors') and len(base.tensors) >= 2:\n",
    "            return base.tensors[1][idx].detach().cpu().long().numpy()\n",
    "        return np.asarray([base[i][1] for i in idx], dtype=np.int64)\n",
    "\n",
    "    # ---- Monitor score helper (expanded for 4 classes) ----\n",
    "    per_class_map = {\n",
    "        # Normal (class 0)\n",
    "        \"recall_normal\": (\"recall\", 0),\n",
    "        \"precision_normal\": (\"precision\", 0),\n",
    "        \"f1_normal\": (\"f1-score\", 0),\n",
    "        \"specificity_normal\": (\"specificity\", 0),\n",
    "        \n",
    "        # SVEB (class 1)\n",
    "        \"recall_sveb\": (\"recall\", 1),\n",
    "        \"precision_sveb\": (\"precision\", 1),\n",
    "        \"f1_sveb\": (\"f1-score\", 1),\n",
    "        \"specificity_sveb\": (\"specificity\", 1),\n",
    "        \n",
    "        # VEB (class 2)\n",
    "        \"recall_veb\": (\"recall\", 2),\n",
    "        \"precision_veb\": (\"precision\", 2),\n",
    "        \"f1_veb\": (\"f1-score\", 2),\n",
    "        \"specificity_veb\": (\"specificity\", 2),\n",
    "        \n",
    "        # F (class 3)\n",
    "        \"recall_f\": (\"recall\", 3),\n",
    "        \"precision_f\": (\"precision\", 3),\n",
    "        \"f1_f\": (\"f1-score\", 3),\n",
    "        \"specificity_f\": (\"specificity\", 3),\n",
    "    }\n",
    "\n",
    "    def extract_score(metrics, monitor_key):\n",
    "        if monitor_key in per_class_map:\n",
    "            k, i = per_class_map[monitor_key]\n",
    "            return metrics[k][i]\n",
    "        return metrics[monitor_key]\n",
    "\n",
    "    # ---- CV setup ----\n",
    "    y_all = all_labels(dataset)\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_history = {}\n",
    "    fold_ckpts = {}\n",
    "\n",
    "    better = (lambda a, b: a > b) if mode == \"max\" else (lambda a, b: a < b)\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(np.arange(len(y_all)), y_all), 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold}/{k_folds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # FRESH model instance per fold\n",
    "        model_fold = model_factory()                 # CPU\n",
    "        model_fold.load_state_dict(warmup_state)     # CPU load\n",
    "        model_fold.to(device)\n",
    "        model_fold.train()\n",
    "\n",
    "        # Fresh optimizer\n",
    "        optimizer_fold = optimizer_class(model_fold.parameters(), **optimizer_kwargs)\n",
    "\n",
    "        # Subsets\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        valid_subset = Subset(dataset, valid_idx)\n",
    "\n",
    "        # ============ MODIFIED: Per-fold class weights for 4 classes ============\n",
    "        y_train = subset_labels(train_subset)\n",
    "        n0 = int((y_train == 0).sum())  # Normal\n",
    "        n1 = int((y_train == 1).sum())  # SVEB\n",
    "        n2 = int((y_train == 2).sum())  # VEB\n",
    "        n3 = int((y_train == 3).sum())  # F\n",
    "        \n",
    "        total = len(y_train)\n",
    "        # Inverse frequency weighting\n",
    "        w0 = total / (4 * max(n0, 1))\n",
    "        w1 = total / (4 * max(n1, 1))\n",
    "        w2 = total / (4 * max(n2, 1))\n",
    "        w3 = total / (4 * max(n3, 1))\n",
    "\n",
    "        print(f\"Train counts: Normal={n0}, SVEB={n1}, VEB={n2}, F={n3}\")\n",
    "        print(f\"Class weights: w0={w0:.3f}, w1={w1:.3f}, w2={w2:.3f}, w3={w3:.3f}\")\n",
    "\n",
    "        class_w = torch.tensor([w0, w1, w2, w3], dtype=torch.float32, device=device)\n",
    "        loss_func_fold = torch.nn.CrossEntropyLoss(weight=class_w)\n",
    "        # =========================================================================\n",
    "\n",
    "        # Loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=(device.type == 'cuda'),\n",
    "            num_workers=0\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=(device.type == 'cuda'),\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        # ============ MODIFIED: History tracking for 4 classes ============\n",
    "        epoch_history = {\n",
    "            'train4_loss': [], 'train4_acc': [],\n",
    "            'valid4_loss': [], 'valid4_acc': [],\n",
    "            'valid4_prec': [], 'valid4_rec': [], 'valid4_spec': [], 'valid4_f1': [],\n",
    "            'valid4_prec_macro': [], 'valid4_rec_macro': [],\n",
    "            'valid4_spec_macro': [], 'valid4_f1_macro': []\n",
    "        }\n",
    "        # ===================================================================\n",
    "\n",
    "        best_score = -float(\"inf\") if mode == \"max\" else float(\"inf\")\n",
    "        best_epoch = None\n",
    "        best_state_cpu = None\n",
    "        \n",
    "        patience = 5\n",
    "        no_improve = 0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            tr4_loss_sum, tr4_corr = train_epoch(\n",
    "                model_fold, train_loader, loss_func_fold, optimizer_fold, device, num_steps\n",
    "            )\n",
    "\n",
    "            v_metrics4 = validation_epoch(\n",
    "                model_fold, valid_loader, loss_func_fold, device, num_steps\n",
    "            )\n",
    "\n",
    "            n_train = len(train_subset)\n",
    "            tr4_loss = tr4_loss_sum / max(n_train, 1)\n",
    "            tr4_acc  = 100.0 * tr4_corr / max(n_train, 1)\n",
    "\n",
    "            # ============ MODIFIED: Print statement for 4 classes ============\n",
    "            print(\n",
    "                f\"Epoch {epoch:2d}/{epochs} | \"\n",
    "                f\"Train  L={tr4_loss:.4f}  A={tr4_acc:5.2f}% | \"\n",
    "                f\"Valid  L={v_metrics4['loss']:.4f}  A={v_metrics4['acc']:5.2f}% | \"\n",
    "                f\"Rec: N={v_metrics4['recall'][0]:.3f} \"\n",
    "                f\"S={v_metrics4['recall'][1]:.3f} \"\n",
    "                f\"V={v_metrics4['recall'][2]:.3f} \"\n",
    "                f\"F={v_metrics4['recall'][3]:.3f} | \"\n",
    "                f\"F1(M)={v_metrics4['f1_macro']:.4f}\"\n",
    "            )\n",
    "            # ==================================================================\n",
    "\n",
    "            epoch_history['train4_loss'].append(tr4_loss)\n",
    "            epoch_history['train4_acc'].append(tr4_acc)\n",
    "            epoch_history['valid4_loss'].append(v_metrics4['loss'])\n",
    "            epoch_history['valid4_acc'].append(v_metrics4['acc'])\n",
    "\n",
    "            epoch_history['valid4_prec'].append(v_metrics4['precision'])\n",
    "            epoch_history['valid4_rec'].append(v_metrics4['recall'])\n",
    "            epoch_history['valid4_spec'].append(v_metrics4['specificity'])\n",
    "            epoch_history['valid4_f1'].append(v_metrics4['f1-score'])\n",
    "\n",
    "            epoch_history['valid4_prec_macro'].append(v_metrics4['precision_macro'])\n",
    "            epoch_history['valid4_rec_macro'].append(v_metrics4['recall_macro'])\n",
    "            epoch_history['valid4_spec_macro'].append(v_metrics4['specificity_macro'])\n",
    "            epoch_history['valid4_f1_macro'].append(v_metrics4['f1_macro'])\n",
    "\n",
    "            score = extract_score(v_metrics4, monitor)\n",
    "\n",
    "            if better(score, best_score):\n",
    "                best_score = score\n",
    "                best_epoch = epoch\n",
    "                no_improve = 0\n",
    "                with torch.no_grad():\n",
    "                    best_state_cpu = {k: v.detach().cpu().clone()\n",
    "                                      for k, v in model_fold.state_dict().items()}\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"Early stopping: no improvement in {patience} epochs.\")\n",
    "                    break\n",
    "\n",
    "        fold_key = f\"fold{fold}\"\n",
    "        fold_history[fold_key] = epoch_history\n",
    "        fold_ckpts[fold_key] = {\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_score\": float(best_score),\n",
    "            \"monitor\": monitor,\n",
    "            \"state_dict_cpu\": best_state_cpu\n",
    "        }\n",
    "\n",
    "        print(f\"\\n[Fold {fold}] Best {monitor}={best_score:.4f} at epoch {best_epoch}\\n\")\n",
    "\n",
    "        del optimizer_fold, model_fold\n",
    "        del train_loader, valid_loader\n",
    "        _clear_cuda_cache()\n",
    "\n",
    "    return fold_history, fold_ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded7bc0-391d-4b1e-9dd7-d1f4298db76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd1e0742-fa5c-4e95-945e-d2e242234f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: MIT-BIH (80557, 180)\n",
      "Data range after z-score normalization: [-4.87, 6.05]\n",
      "Data mean: 0.0000, std: 1.0000\n",
      "Class distribution: Counter({0: 72073, 2: 5616, 1: 2224, 3: 644})\n",
      "X train shape: (80557, 1, 180)\n",
      "y_train shape: (80557,)\n",
      "Data tensor loaded successfully\n",
      "dataset created successfully\n",
      "Using device: cuda\n",
      "Creating template model for warmup...\n",
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "Running Brevitas warmup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/velox-217533/anaconda3/envs/fau_snn_torch-cuda/lib/python3.12/site-packages/torch/_tensor.py:1645: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1939.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling keys after warmup: 13\n",
      "\n",
      "============================================================\n",
      "Fold 1/6\n",
      "============================================================\n",
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "Train counts: Normal=60060, SVEB=1854, VEB=4680, F=536\n",
      "Class weights: w0=0.279, w1=9.052, w2=3.586, w3=31.311\n",
      "Epoch  1/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  2/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  3/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  4/80 | Train  L=1.3413  A=83.88% | Valid  L=1.2647  A=82.47% | Rec: N=0.830 S=0.608 V=0.851 F=0.722 | F1(M)=0.5360\n",
      "Epoch  5/80 | Train  L=1.1674  A=82.22% | Valid  L=1.0515  A=81.49% | Rec: N=0.817 S=0.646 V=0.862 F=0.704 | F1(M)=0.5226\n",
      "Epoch  6/80 | Train  L=0.9650  A=81.54% | Valid  L=0.9555  A=82.44% | Rec: N=0.827 S=0.651 V=0.862 F=0.750 | F1(M)=0.5411\n",
      "Epoch  7/80 | Train  L=0.9423  A=82.82% | Valid  L=0.9514  A=81.51% | Rec: N=0.816 S=0.659 V=0.872 F=0.778 | F1(M)=0.5332\n",
      "Epoch  8/80 | Train  L=0.9367  A=83.90% | Valid  L=0.9391  A=83.73% | Rec: N=0.841 S=0.668 V=0.864 F=0.769 | F1(M)=0.5478\n",
      "Epoch  9/80 | Train  L=0.9281  A=84.15% | Valid  L=0.9339  A=84.46% | Rec: N=0.849 S=0.670 V=0.870 F=0.778 | F1(M)=0.5532\n",
      "Epoch 10/80 | Train  L=0.9208  A=84.88% | Valid  L=0.9280  A=82.86% | Rec: N=0.830 S=0.705 V=0.869 F=0.806 | F1(M)=0.5400\n",
      "Epoch 11/80 | Train  L=0.9162  A=84.86% | Valid  L=0.9234  A=85.89% | Rec: N=0.863 S=0.678 V=0.891 F=0.778 | F1(M)=0.5702\n",
      "Epoch 12/80 | Train  L=0.9110  A=85.09% | Valid  L=0.9216  A=87.35% | Rec: N=0.878 S=0.678 V=0.901 F=0.759 | F1(M)=0.5884\n",
      "Epoch 13/80 | Train  L=0.9052  A=85.81% | Valid  L=0.9152  A=87.19% | Rec: N=0.877 S=0.695 V=0.891 F=0.759 | F1(M)=0.5896\n",
      "Epoch 14/80 | Train  L=0.9014  A=86.01% | Valid  L=0.9151  A=87.03% | Rec: N=0.874 S=0.697 V=0.895 F=0.787 | F1(M)=0.5897\n",
      "Epoch 15/80 | Train  L=0.8983  A=86.53% | Valid  L=0.9065  A=86.06% | Rec: N=0.862 S=0.727 V=0.895 F=0.806 | F1(M)=0.5771\n",
      "Epoch 16/80 | Train  L=0.8938  A=87.01% | Valid  L=0.9023  A=88.40% | Rec: N=0.887 S=0.716 V=0.919 F=0.806 | F1(M)=0.6093\n",
      "Epoch 17/80 | Train  L=0.8922  A=87.67% | Valid  L=0.9001  A=88.41% | Rec: N=0.888 S=0.719 V=0.904 F=0.796 | F1(M)=0.6090\n",
      "Epoch 18/80 | Train  L=0.8902  A=87.61% | Valid  L=0.8997  A=87.03% | Rec: N=0.872 S=0.730 V=0.918 F=0.796 | F1(M)=0.5911\n",
      "Epoch 19/80 | Train  L=0.8871  A=88.28% | Valid  L=0.8971  A=89.31% | Rec: N=0.897 S=0.727 V=0.923 F=0.787 | F1(M)=0.6248\n",
      "Epoch 20/80 | Train  L=0.8836  A=88.73% | Valid  L=0.8966  A=88.97% | Rec: N=0.891 S=0.738 V=0.943 F=0.778 | F1(M)=0.6226\n",
      "Epoch 21/80 | Train  L=0.8823  A=88.87% | Valid  L=0.8920  A=88.31% | Rec: N=0.886 S=0.749 V=0.903 F=0.833 | F1(M)=0.6105\n",
      "Epoch 22/80 | Train  L=0.8779  A=89.12% | Valid  L=0.8868  A=88.66% | Rec: N=0.889 S=0.754 V=0.923 F=0.815 | F1(M)=0.6175\n",
      "Epoch 23/80 | Train  L=0.8756  A=89.20% | Valid  L=0.8890  A=88.72% | Rec: N=0.889 S=0.732 V=0.935 F=0.824 | F1(M)=0.6238\n",
      "Epoch 24/80 | Train  L=0.8742  A=89.98% | Valid  L=0.8875  A=89.21% | Rec: N=0.896 S=0.741 V=0.915 F=0.824 | F1(M)=0.6273\n",
      "Epoch 25/80 | Train  L=0.8724  A=90.25% | Valid  L=0.8835  A=90.58% | Rec: N=0.909 S=0.746 V=0.931 F=0.843 | F1(M)=0.6571\n",
      "Epoch 26/80 | Train  L=0.8693  A=90.46% | Valid  L=0.8843  A=90.70% | Rec: N=0.910 S=0.738 V=0.947 F=0.824 | F1(M)=0.6629\n",
      "Epoch 27/80 | Train  L=0.8665  A=90.80% | Valid  L=0.8790  A=91.25% | Rec: N=0.917 S=0.730 V=0.929 F=0.852 | F1(M)=0.6724\n",
      "Epoch 28/80 | Train  L=0.8623  A=91.14% | Valid  L=0.8809  A=91.23% | Rec: N=0.916 S=0.738 V=0.942 F=0.852 | F1(M)=0.6753\n",
      "Epoch 29/80 | Train  L=0.8617  A=91.03% | Valid  L=0.8758  A=90.59% | Rec: N=0.908 S=0.751 V=0.952 F=0.852 | F1(M)=0.6597\n",
      "Epoch 30/80 | Train  L=0.8619  A=91.11% | Valid  L=0.8824  A=90.71% | Rec: N=0.910 S=0.727 V=0.953 F=0.824 | F1(M)=0.6622\n",
      "Epoch 31/80 | Train  L=0.8601  A=91.03% | Valid  L=0.8701  A=91.06% | Rec: N=0.913 S=0.757 V=0.946 F=0.870 | F1(M)=0.6715\n",
      "Epoch 32/80 | Train  L=0.8557  A=91.40% | Valid  L=0.8786  A=91.73% | Rec: N=0.921 S=0.757 V=0.953 F=0.796 | F1(M)=0.6886\n",
      "Epoch 33/80 | Train  L=0.8562  A=91.81% | Valid  L=0.8664  A=91.21% | Rec: N=0.915 S=0.770 V=0.937 F=0.889 | F1(M)=0.6743\n",
      "Epoch 34/80 | Train  L=0.8555  A=91.52% | Valid  L=0.8696  A=92.37% | Rec: N=0.928 S=0.743 V=0.941 F=0.880 | F1(M)=0.7047\n",
      "Epoch 35/80 | Train  L=0.8532  A=91.77% | Valid  L=0.8664  A=90.15% | Rec: N=0.903 S=0.757 V=0.948 F=0.880 | F1(M)=0.6521\n",
      "Epoch 36/80 | Train  L=0.8520  A=91.76% | Valid  L=0.8656  A=91.77% | Rec: N=0.923 S=0.746 V=0.922 F=0.917 | F1(M)=0.6829\n",
      "Epoch 37/80 | Train  L=0.8539  A=91.66% | Valid  L=0.8700  A=92.34% | Rec: N=0.928 S=0.743 V=0.935 F=0.889 | F1(M)=0.7024\n",
      "Epoch 38/80 | Train  L=0.8521  A=91.84% | Valid  L=0.8672  A=92.41% | Rec: N=0.929 S=0.743 V=0.936 F=0.880 | F1(M)=0.7009\n",
      "Epoch 39/80 | Train  L=0.8476  A=92.16% | Valid  L=0.8682  A=92.66% | Rec: N=0.931 S=0.746 V=0.946 F=0.843 | F1(M)=0.7134\n",
      "Epoch 40/80 | Train  L=0.8461  A=92.02% | Valid  L=0.8642  A=91.80% | Rec: N=0.921 S=0.751 V=0.949 F=0.880 | F1(M)=0.6887\n",
      "Epoch 41/80 | Train  L=0.8465  A=91.99% | Valid  L=0.8649  A=91.73% | Rec: N=0.921 S=0.746 V=0.939 F=0.889 | F1(M)=0.6833\n",
      "Epoch 42/80 | Train  L=0.8455  A=92.56% | Valid  L=0.8662  A=91.64% | Rec: N=0.919 S=0.757 V=0.950 F=0.861 | F1(M)=0.6864\n",
      "Epoch 43/80 | Train  L=0.8452  A=92.31% | Valid  L=0.8619  A=92.69% | Rec: N=0.932 S=0.749 V=0.946 F=0.852 | F1(M)=0.7126\n",
      "Epoch 44/80 | Train  L=0.8447  A=92.64% | Valid  L=0.8631  A=92.62% | Rec: N=0.931 S=0.743 V=0.941 F=0.870 | F1(M)=0.7079\n",
      "Early stopping: no improvement in 5 epochs.\n",
      "\n",
      "[Fold 1] Best f1_macro=0.7134 at epoch 39\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 2/6\n",
      "============================================================\n",
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "Train counts: Normal=60061, SVEB=1853, VEB=4680, F=537\n",
      "Class weights: w0=0.279, w1=9.057, w2=3.586, w3=31.253\n",
      "Epoch  1/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  2/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  3/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  4/80 | Train  L=1.3423  A=84.25% | Valid  L=1.2711  A=79.07% | Rec: N=0.794 S=0.625 V=0.827 F=0.710 | F1(M)=0.5098\n",
      "Epoch  5/80 | Train  L=1.1675  A=81.35% | Valid  L=1.0600  A=79.97% | Rec: N=0.803 S=0.660 V=0.828 F=0.682 | F1(M)=0.5051\n",
      "Epoch  6/80 | Train  L=0.9647  A=81.69% | Valid  L=0.9626  A=84.22% | Rec: N=0.849 S=0.666 V=0.857 F=0.607 | F1(M)=0.5395\n",
      "Epoch  7/80 | Train  L=0.9407  A=83.80% | Valid  L=0.9514  A=83.56% | Rec: N=0.841 S=0.671 V=0.844 F=0.692 | F1(M)=0.5364\n",
      "Epoch  8/80 | Train  L=0.9322  A=84.55% | Valid  L=0.9479  A=85.28% | Rec: N=0.862 S=0.658 V=0.837 F=0.692 | F1(M)=0.5530\n",
      "Epoch  9/80 | Train  L=0.9271  A=85.56% | Valid  L=0.9395  A=85.07% | Rec: N=0.858 S=0.668 V=0.841 F=0.757 | F1(M)=0.5554\n",
      "Epoch 10/80 | Train  L=0.9203  A=85.16% | Valid  L=0.9347  A=85.62% | Rec: N=0.862 S=0.679 V=0.866 F=0.692 | F1(M)=0.5643\n",
      "Epoch 11/80 | Train  L=0.9170  A=86.13% | Valid  L=0.9291  A=86.59% | Rec: N=0.875 S=0.663 V=0.847 F=0.748 | F1(M)=0.5756\n",
      "Epoch 12/80 | Train  L=0.9113  A=86.41% | Valid  L=0.9303  A=87.70% | Rec: N=0.886 S=0.679 V=0.863 F=0.682 | F1(M)=0.5848\n",
      "Epoch 13/80 | Train  L=0.9080  A=86.92% | Valid  L=0.9250  A=86.85% | Rec: N=0.877 S=0.690 V=0.846 F=0.738 | F1(M)=0.5781\n",
      "Epoch 14/80 | Train  L=0.9037  A=86.93% | Valid  L=0.9219  A=86.87% | Rec: N=0.876 S=0.687 V=0.869 F=0.729 | F1(M)=0.5807\n",
      "Epoch 15/80 | Train  L=0.9000  A=87.52% | Valid  L=0.9190  A=87.90% | Rec: N=0.886 S=0.712 V=0.870 F=0.729 | F1(M)=0.5966\n",
      "Epoch 16/80 | Train  L=0.8958  A=88.09% | Valid  L=0.9176  A=88.43% | Rec: N=0.891 S=0.728 V=0.874 F=0.738 | F1(M)=0.6025\n",
      "Epoch 17/80 | Train  L=0.8924  A=88.02% | Valid  L=0.9120  A=87.54% | Rec: N=0.879 S=0.725 V=0.896 F=0.766 | F1(M)=0.5973\n",
      "Epoch 18/80 | Train  L=0.8901  A=88.70% | Valid  L=0.9059  A=87.17% | Rec: N=0.875 S=0.747 V=0.893 F=0.776 | F1(M)=0.5922\n",
      "Epoch 19/80 | Train  L=0.8872  A=88.43% | Valid  L=0.9027  A=87.59% | Rec: N=0.879 S=0.747 V=0.902 F=0.785 | F1(M)=0.6007\n",
      "Epoch 20/80 | Train  L=0.8845  A=88.93% | Valid  L=0.9015  A=88.46% | Rec: N=0.888 S=0.747 V=0.904 F=0.766 | F1(M)=0.6090\n",
      "Epoch 21/80 | Train  L=0.8808  A=89.07% | Valid  L=0.9026  A=89.89% | Rec: N=0.904 S=0.736 V=0.915 F=0.748 | F1(M)=0.6326\n",
      "Epoch 22/80 | Train  L=0.8778  A=89.29% | Valid  L=0.8958  A=89.74% | Rec: N=0.902 S=0.741 V=0.907 F=0.785 | F1(M)=0.6310\n",
      "Epoch 23/80 | Train  L=0.8755  A=89.58% | Valid  L=0.8935  A=89.77% | Rec: N=0.903 S=0.733 V=0.910 F=0.785 | F1(M)=0.6318\n",
      "Epoch 24/80 | Train  L=0.8730  A=89.84% | Valid  L=0.8897  A=89.36% | Rec: N=0.897 S=0.752 V=0.919 F=0.785 | F1(M)=0.6252\n",
      "Epoch 25/80 | Train  L=0.8716  A=89.96% | Valid  L=0.8934  A=89.60% | Rec: N=0.900 S=0.744 V=0.920 F=0.776 | F1(M)=0.6294\n",
      "Epoch 26/80 | Train  L=0.8679  A=90.23% | Valid  L=0.8904  A=88.60% | Rec: N=0.889 S=0.744 V=0.907 F=0.804 | F1(M)=0.6165\n",
      "Early stopping: no improvement in 5 epochs.\n",
      "\n",
      "[Fold 2] Best f1_macro=0.6326 at epoch 21\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 3/6\n",
      "============================================================\n",
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "Train counts: Normal=60061, SVEB=1853, VEB=4680, F=537\n",
      "Class weights: w0=0.279, w1=9.057, w2=3.586, w3=31.253\n",
      "Epoch  1/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  2/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  3/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  4/80 | Train  L=1.3424  A=84.52% | Valid  L=1.2629  A=83.26% | Rec: N=0.839 S=0.617 V=0.847 F=0.729 | F1(M)=0.5463\n",
      "Epoch  5/80 | Train  L=1.1713  A=81.95% | Valid  L=1.0502  A=79.42% | Rec: N=0.794 S=0.644 V=0.854 F=0.794 | F1(M)=0.5157\n",
      "Epoch  6/80 | Train  L=0.9710  A=81.34% | Valid  L=0.9535  A=80.75% | Rec: N=0.809 S=0.658 V=0.850 F=0.766 | F1(M)=0.5332\n",
      "Epoch  7/80 | Train  L=0.9464  A=82.81% | Valid  L=0.9458  A=83.29% | Rec: N=0.837 S=0.666 V=0.855 F=0.766 | F1(M)=0.5490\n",
      "Epoch  8/80 | Train  L=0.9386  A=83.94% | Valid  L=0.9419  A=79.86% | Rec: N=0.797 S=0.690 V=0.863 F=0.785 | F1(M)=0.5234\n",
      "Epoch  9/80 | Train  L=0.9331  A=84.09% | Valid  L=0.9368  A=83.99% | Rec: N=0.842 S=0.674 V=0.884 F=0.785 | F1(M)=0.5574\n",
      "Epoch 10/80 | Train  L=0.9293  A=85.15% | Valid  L=0.9347  A=86.43% | Rec: N=0.868 S=0.687 V=0.892 F=0.776 | F1(M)=0.5777\n",
      "Epoch 11/80 | Train  L=0.9234  A=85.88% | Valid  L=0.9268  A=87.46% | Rec: N=0.882 S=0.685 V=0.863 F=0.757 | F1(M)=0.5890\n",
      "Epoch 12/80 | Train  L=0.9187  A=85.87% | Valid  L=0.9222  A=86.58% | Rec: N=0.871 S=0.685 V=0.886 F=0.785 | F1(M)=0.5805\n",
      "Epoch 13/80 | Train  L=0.9138  A=86.47% | Valid  L=0.9178  A=85.54% | Rec: N=0.858 S=0.701 V=0.888 F=0.804 | F1(M)=0.5727\n",
      "Epoch 14/80 | Train  L=0.9127  A=86.56% | Valid  L=0.9191  A=86.62% | Rec: N=0.870 S=0.690 V=0.892 F=0.804 | F1(M)=0.5799\n",
      "Epoch 15/80 | Train  L=0.9081  A=86.45% | Valid  L=0.9160  A=88.26% | Rec: N=0.889 S=0.712 V=0.885 F=0.766 | F1(M)=0.5980\n",
      "Epoch 16/80 | Train  L=0.9054  A=86.85% | Valid  L=0.9114  A=87.29% | Rec: N=0.877 S=0.720 V=0.892 F=0.794 | F1(M)=0.5887\n",
      "Epoch 17/80 | Train  L=0.9012  A=86.97% | Valid  L=0.9088  A=86.33% | Rec: N=0.865 S=0.722 V=0.903 F=0.813 | F1(M)=0.5790\n",
      "Epoch 18/80 | Train  L=0.8980  A=87.41% | Valid  L=0.9074  A=87.35% | Rec: N=0.875 S=0.722 V=0.918 F=0.804 | F1(M)=0.5931\n",
      "Epoch 19/80 | Train  L=0.8934  A=87.97% | Valid  L=0.9024  A=86.49% | Rec: N=0.866 S=0.725 V=0.909 F=0.813 | F1(M)=0.5805\n",
      "Epoch 20/80 | Train  L=0.8912  A=87.81% | Valid  L=0.9019  A=88.78% | Rec: N=0.893 S=0.714 V=0.901 F=0.813 | F1(M)=0.6109\n",
      "Epoch 21/80 | Train  L=0.8881  A=88.74% | Valid  L=0.8965  A=89.26% | Rec: N=0.898 S=0.722 V=0.904 F=0.813 | F1(M)=0.6199\n",
      "Epoch 22/80 | Train  L=0.8860  A=88.77% | Valid  L=0.8932  A=89.31% | Rec: N=0.898 S=0.725 V=0.909 F=0.794 | F1(M)=0.6208\n",
      "Epoch 23/80 | Train  L=0.8851  A=88.60% | Valid  L=0.8952  A=88.80% | Rec: N=0.892 S=0.739 V=0.908 F=0.813 | F1(M)=0.6147\n",
      "Epoch 24/80 | Train  L=0.8835  A=88.90% | Valid  L=0.8921  A=90.09% | Rec: N=0.906 S=0.741 V=0.907 F=0.813 | F1(M)=0.6362\n",
      "Epoch 25/80 | Train  L=0.8794  A=89.78% | Valid  L=0.8919  A=91.05% | Rec: N=0.915 S=0.741 V=0.928 F=0.804 | F1(M)=0.6591\n",
      "Epoch 26/80 | Train  L=0.8805  A=89.65% | Valid  L=0.8903  A=88.99% | Rec: N=0.894 S=0.730 V=0.909 F=0.832 | F1(M)=0.6210\n",
      "Epoch 27/80 | Train  L=0.8815  A=89.32% | Valid  L=0.8895  A=89.54% | Rec: N=0.898 S=0.747 V=0.931 F=0.822 | F1(M)=0.6336\n",
      "Epoch 28/80 | Train  L=0.8778  A=89.60% | Valid  L=0.8848  A=89.55% | Rec: N=0.898 S=0.763 V=0.919 F=0.832 | F1(M)=0.6304\n",
      "Epoch 29/80 | Train  L=0.8753  A=89.77% | Valid  L=0.8902  A=88.40% | Rec: N=0.885 S=0.749 V=0.921 F=0.860 | F1(M)=0.6209\n",
      "Epoch 30/80 | Train  L=0.8743  A=89.63% | Valid  L=0.8882  A=90.08% | Rec: N=0.904 S=0.747 V=0.923 F=0.841 | F1(M)=0.6442\n",
      "Early stopping: no improvement in 5 epochs.\n",
      "\n",
      "[Fold 3] Best f1_macro=0.6591 at epoch 25\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 4/6\n",
      "============================================================\n",
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "Train counts: Normal=60061, SVEB=1853, VEB=4680, F=537\n",
      "Class weights: w0=0.279, w1=9.057, w2=3.586, w3=31.253\n",
      "Epoch  1/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  2/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  3/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  4/80 | Train  L=1.3422  A=84.90% | Valid  L=1.2603  A=83.66% | Rec: N=0.844 S=0.620 V=0.829 F=0.766 | F1(M)=0.5467\n",
      "Epoch  5/80 | Train  L=1.1703  A=81.97% | Valid  L=1.0437  A=81.83% | Rec: N=0.822 S=0.650 V=0.847 F=0.766 | F1(M)=0.5263\n",
      "Epoch  6/80 | Train  L=0.9655  A=82.08% | Valid  L=0.9432  A=83.41% | Rec: N=0.839 S=0.663 V=0.843 F=0.776 | F1(M)=0.5456\n",
      "Epoch  7/80 | Train  L=0.9400  A=83.88% | Valid  L=0.9352  A=83.78% | Rec: N=0.841 S=0.682 V=0.859 F=0.776 | F1(M)=0.5499\n",
      "Epoch  8/80 | Train  L=0.9338  A=84.69% | Valid  L=0.9320  A=84.86% | Rec: N=0.852 S=0.671 V=0.879 F=0.776 | F1(M)=0.5604\n",
      "Epoch  9/80 | Train  L=0.9272  A=85.39% | Valid  L=0.9286  A=82.73% | Rec: N=0.831 S=0.677 V=0.849 F=0.794 | F1(M)=0.5432\n",
      "Epoch 10/80 | Train  L=0.9204  A=84.73% | Valid  L=0.9204  A=85.95% | Rec: N=0.865 S=0.693 V=0.863 F=0.757 | F1(M)=0.5705\n",
      "Epoch 11/80 | Train  L=0.9174  A=85.54% | Valid  L=0.9171  A=84.98% | Rec: N=0.854 S=0.709 V=0.865 F=0.776 | F1(M)=0.5605\n",
      "Epoch 12/80 | Train  L=0.9113  A=86.14% | Valid  L=0.9139  A=85.67% | Rec: N=0.860 S=0.698 V=0.884 F=0.804 | F1(M)=0.5749\n",
      "Epoch 13/80 | Train  L=0.9066  A=86.19% | Valid  L=0.9111  A=86.05% | Rec: N=0.864 S=0.706 V=0.889 F=0.794 | F1(M)=0.5763\n",
      "Epoch 14/80 | Train  L=0.9029  A=86.44% | Valid  L=0.9090  A=86.79% | Rec: N=0.873 S=0.725 V=0.865 F=0.794 | F1(M)=0.5818\n",
      "Epoch 15/80 | Train  L=0.8980  A=86.91% | Valid  L=0.9019  A=87.52% | Rec: N=0.878 S=0.741 V=0.897 F=0.785 | F1(M)=0.5957\n",
      "Epoch 16/80 | Train  L=0.8934  A=87.14% | Valid  L=0.9009  A=88.57% | Rec: N=0.890 S=0.747 V=0.900 F=0.748 | F1(M)=0.6069\n",
      "Epoch 17/80 | Train  L=0.8895  A=87.39% | Valid  L=0.8961  A=86.34% | Rec: N=0.864 S=0.757 V=0.899 F=0.822 | F1(M)=0.5828\n",
      "Epoch 18/80 | Train  L=0.8850  A=87.42% | Valid  L=0.8938  A=86.00% | Rec: N=0.859 S=0.765 V=0.916 F=0.822 | F1(M)=0.5824\n",
      "Epoch 19/80 | Train  L=0.8817  A=87.71% | Valid  L=0.8891  A=87.93% | Rec: N=0.882 S=0.744 V=0.907 F=0.841 | F1(M)=0.6036\n",
      "Epoch 20/80 | Train  L=0.8766  A=88.45% | Valid  L=0.8878  A=87.90% | Rec: N=0.879 S=0.755 V=0.929 F=0.850 | F1(M)=0.6086\n",
      "Epoch 21/80 | Train  L=0.8756  A=88.63% | Valid  L=0.8833  A=87.13% | Rec: N=0.871 S=0.782 V=0.912 F=0.860 | F1(M)=0.5965\n",
      "Epoch 22/80 | Train  L=0.8725  A=89.05% | Valid  L=0.8883  A=88.22% | Rec: N=0.885 S=0.774 V=0.891 F=0.850 | F1(M)=0.6078\n",
      "Epoch 23/80 | Train  L=0.8732  A=89.17% | Valid  L=0.8849  A=88.98% | Rec: N=0.893 S=0.774 V=0.903 F=0.841 | F1(M)=0.6206\n",
      "Epoch 24/80 | Train  L=0.8696  A=89.56% | Valid  L=0.8808  A=89.70% | Rec: N=0.899 S=0.768 V=0.928 F=0.850 | F1(M)=0.6388\n",
      "Epoch 25/80 | Train  L=0.8682  A=89.76% | Valid  L=0.8797  A=89.77% | Rec: N=0.900 S=0.757 V=0.924 F=0.860 | F1(M)=0.6416\n",
      "Epoch 26/80 | Train  L=0.8658  A=90.16% | Valid  L=0.8751  A=90.55% | Rec: N=0.907 S=0.779 V=0.938 F=0.850 | F1(M)=0.6583\n",
      "Epoch 27/80 | Train  L=0.8623  A=90.03% | Valid  L=0.8756  A=91.03% | Rec: N=0.913 S=0.765 V=0.941 F=0.822 | F1(M)=0.6680\n",
      "Epoch 28/80 | Train  L=0.8638  A=90.39% | Valid  L=0.8746  A=90.91% | Rec: N=0.913 S=0.779 V=0.924 F=0.841 | F1(M)=0.6623\n",
      "Epoch 29/80 | Train  L=0.8592  A=90.73% | Valid  L=0.8724  A=90.92% | Rec: N=0.912 S=0.779 V=0.932 F=0.860 | F1(M)=0.6668\n",
      "Epoch 30/80 | Train  L=0.8584  A=90.84% | Valid  L=0.8747  A=90.35% | Rec: N=0.906 S=0.776 V=0.934 F=0.813 | F1(M)=0.6507\n",
      "Epoch 31/80 | Train  L=0.8589  A=90.70% | Valid  L=0.8768  A=90.06% | Rec: N=0.903 S=0.784 V=0.922 F=0.841 | F1(M)=0.6484\n",
      "Epoch 32/80 | Train  L=0.8581  A=90.82% | Valid  L=0.8748  A=91.44% | Rec: N=0.917 S=0.784 V=0.941 F=0.813 | F1(M)=0.6774\n",
      "Epoch 33/80 | Train  L=0.8592  A=90.87% | Valid  L=0.8695  A=90.44% | Rec: N=0.906 S=0.792 V=0.939 F=0.841 | F1(M)=0.6556\n",
      "Epoch 34/80 | Train  L=0.8557  A=90.71% | Valid  L=0.8707  A=90.41% | Rec: N=0.905 S=0.787 V=0.941 F=0.832 | F1(M)=0.6546\n",
      "Epoch 35/80 | Train  L=0.8551  A=91.48% | Valid  L=0.8689  A=90.59% | Rec: N=0.907 S=0.798 V=0.941 F=0.841 | F1(M)=0.6587\n",
      "Epoch 36/80 | Train  L=0.8541  A=91.11% | Valid  L=0.8688  A=90.82% | Rec: N=0.910 S=0.790 V=0.940 F=0.869 | F1(M)=0.6667\n",
      "Epoch 37/80 | Train  L=0.8529  A=91.20% | Valid  L=0.8699  A=90.44% | Rec: N=0.907 S=0.814 V=0.916 F=0.860 | F1(M)=0.6526\n",
      "Early stopping: no improvement in 5 epochs.\n",
      "\n",
      "[Fold 4] Best f1_macro=0.6774 at epoch 32\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 5/6\n",
      "============================================================\n",
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "Train counts: Normal=60061, SVEB=1853, VEB=4680, F=537\n",
      "Class weights: w0=0.279, w1=9.057, w2=3.586, w3=31.253\n",
      "Epoch  1/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  2/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  3/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  4/80 | Train  L=1.3405  A=84.81% | Valid  L=1.2610  A=82.06% | Rec: N=0.825 S=0.666 V=0.831 F=0.748 | F1(M)=0.5308\n",
      "Epoch  5/80 | Train  L=1.1687  A=81.21% | Valid  L=1.0488  A=81.81% | Rec: N=0.823 S=0.671 V=0.819 F=0.766 | F1(M)=0.5225\n",
      "Epoch  6/80 | Train  L=0.9681  A=81.21% | Valid  L=0.9519  A=80.49% | Rec: N=0.806 S=0.671 V=0.842 F=0.776 | F1(M)=0.5168\n",
      "Epoch  7/80 | Train  L=0.9466  A=82.20% | Valid  L=0.9445  A=82.75% | Rec: N=0.831 S=0.698 V=0.837 F=0.757 | F1(M)=0.5321\n",
      "Epoch  8/80 | Train  L=0.9404  A=83.24% | Valid  L=0.9399  A=81.85% | Rec: N=0.821 S=0.693 V=0.833 F=0.794 | F1(M)=0.5358\n",
      "Epoch  9/80 | Train  L=0.9346  A=83.73% | Valid  L=0.9334  A=84.02% | Rec: N=0.846 S=0.693 V=0.830 F=0.794 | F1(M)=0.5492\n",
      "Epoch 10/80 | Train  L=0.9280  A=84.91% | Valid  L=0.9275  A=83.10% | Rec: N=0.833 S=0.704 V=0.860 F=0.813 | F1(M)=0.5408\n",
      "Epoch 11/80 | Train  L=0.9195  A=85.01% | Valid  L=0.9233  A=84.54% | Rec: N=0.849 S=0.704 V=0.855 F=0.804 | F1(M)=0.5545\n",
      "Epoch 12/80 | Train  L=0.9163  A=85.61% | Valid  L=0.9210  A=85.68% | Rec: N=0.863 S=0.709 V=0.838 F=0.785 | F1(M)=0.5664\n",
      "Epoch 13/80 | Train  L=0.9114  A=85.84% | Valid  L=0.9168  A=86.56% | Rec: N=0.872 S=0.709 V=0.860 F=0.794 | F1(M)=0.5810\n",
      "Epoch 14/80 | Train  L=0.9060  A=86.13% | Valid  L=0.9099  A=86.70% | Rec: N=0.872 S=0.730 V=0.858 F=0.813 | F1(M)=0.5815\n",
      "Epoch 15/80 | Train  L=0.9024  A=85.95% | Valid  L=0.9098  A=86.49% | Rec: N=0.869 S=0.722 V=0.873 F=0.822 | F1(M)=0.5855\n",
      "Epoch 16/80 | Train  L=0.8962  A=86.60% | Valid  L=0.9061  A=87.29% | Rec: N=0.876 S=0.717 V=0.895 F=0.822 | F1(M)=0.5963\n",
      "Epoch 17/80 | Train  L=0.8935  A=87.03% | Valid  L=0.9022  A=88.19% | Rec: N=0.889 S=0.725 V=0.863 F=0.822 | F1(M)=0.6055\n",
      "Epoch 18/80 | Train  L=0.8913  A=87.34% | Valid  L=0.8994  A=87.04% | Rec: N=0.874 S=0.749 V=0.873 F=0.869 | F1(M)=0.5919\n",
      "Epoch 19/80 | Train  L=0.8870  A=87.48% | Valid  L=0.8951  A=86.68% | Rec: N=0.869 S=0.744 V=0.885 F=0.869 | F1(M)=0.5887\n",
      "Epoch 20/80 | Train  L=0.8855  A=88.09% | Valid  L=0.8962  A=86.62% | Rec: N=0.870 S=0.747 V=0.868 F=0.888 | F1(M)=0.5881\n",
      "Epoch 21/80 | Train  L=0.8829  A=88.40% | Valid  L=0.8960  A=88.54% | Rec: N=0.889 S=0.752 V=0.889 F=0.879 | F1(M)=0.6156\n",
      "Epoch 22/80 | Train  L=0.8792  A=88.88% | Valid  L=0.8914  A=88.18% | Rec: N=0.884 S=0.760 V=0.897 F=0.879 | F1(M)=0.6120\n",
      "Epoch 23/80 | Train  L=0.8752  A=89.35% | Valid  L=0.8903  A=89.12% | Rec: N=0.894 S=0.744 V=0.924 F=0.841 | F1(M)=0.6279\n",
      "Epoch 24/80 | Train  L=0.8735  A=89.32% | Valid  L=0.8857  A=88.61% | Rec: N=0.890 S=0.749 V=0.895 F=0.888 | F1(M)=0.6163\n",
      "Epoch 25/80 | Train  L=0.8703  A=89.83% | Valid  L=0.8857  A=89.43% | Rec: N=0.900 S=0.755 V=0.878 F=0.869 | F1(M)=0.6284\n",
      "Epoch 26/80 | Train  L=0.8700  A=89.63% | Valid  L=0.8843  A=89.01% | Rec: N=0.893 S=0.774 V=0.895 F=0.888 | F1(M)=0.6238\n",
      "Epoch 27/80 | Train  L=0.8676  A=89.67% | Valid  L=0.8802  A=89.58% | Rec: N=0.898 S=0.771 V=0.924 F=0.850 | F1(M)=0.6353\n",
      "Epoch 28/80 | Train  L=0.8647  A=89.67% | Valid  L=0.8785  A=89.27% | Rec: N=0.894 S=0.768 V=0.928 F=0.860 | F1(M)=0.6310\n",
      "Epoch 29/80 | Train  L=0.8655  A=90.26% | Valid  L=0.8788  A=88.60% | Rec: N=0.887 S=0.779 V=0.925 F=0.850 | F1(M)=0.6195\n",
      "Epoch 30/80 | Train  L=0.8618  A=90.16% | Valid  L=0.8789  A=89.45% | Rec: N=0.897 S=0.760 V=0.917 F=0.869 | F1(M)=0.6366\n",
      "Epoch 31/80 | Train  L=0.8626  A=89.94% | Valid  L=0.8779  A=90.68% | Rec: N=0.910 S=0.768 V=0.921 F=0.860 | F1(M)=0.6567\n",
      "Epoch 32/80 | Train  L=0.8594  A=90.60% | Valid  L=0.8755  A=89.13% | Rec: N=0.893 S=0.784 V=0.912 F=0.879 | F1(M)=0.6286\n",
      "Epoch 33/80 | Train  L=0.8594  A=90.89% | Valid  L=0.8791  A=90.68% | Rec: N=0.912 S=0.782 V=0.901 F=0.841 | F1(M)=0.6530\n",
      "Epoch 34/80 | Train  L=0.8612  A=90.49% | Valid  L=0.8786  A=89.66% | Rec: N=0.900 S=0.792 V=0.893 F=0.869 | F1(M)=0.6348\n",
      "Epoch 35/80 | Train  L=0.8595  A=90.81% | Valid  L=0.8741  A=90.67% | Rec: N=0.909 S=0.782 V=0.934 F=0.832 | F1(M)=0.6567\n",
      "Epoch 36/80 | Train  L=0.8557  A=91.57% | Valid  L=0.8709  A=91.28% | Rec: N=0.915 S=0.760 V=0.946 F=0.860 | F1(M)=0.6739\n",
      "Epoch 37/80 | Train  L=0.8558  A=91.51% | Valid  L=0.8719  A=91.26% | Rec: N=0.917 S=0.771 V=0.919 F=0.860 | F1(M)=0.6695\n",
      "Epoch 38/80 | Train  L=0.8562  A=91.50% | Valid  L=0.8706  A=90.98% | Rec: N=0.913 S=0.776 V=0.926 F=0.869 | F1(M)=0.6643\n",
      "Epoch 39/80 | Train  L=0.8547  A=91.54% | Valid  L=0.8715  A=91.35% | Rec: N=0.916 S=0.782 V=0.937 F=0.860 | F1(M)=0.6761\n",
      "Epoch 40/80 | Train  L=0.8531  A=91.67% | Valid  L=0.8745  A=91.14% | Rec: N=0.916 S=0.782 V=0.902 F=0.879 | F1(M)=0.6647\n",
      "Epoch 41/80 | Train  L=0.8536  A=91.50% | Valid  L=0.8699  A=91.45% | Rec: N=0.918 S=0.771 V=0.938 F=0.841 | F1(M)=0.6756\n",
      "Epoch 42/80 | Train  L=0.8574  A=91.16% | Valid  L=0.8713  A=91.76% | Rec: N=0.922 S=0.774 V=0.929 F=0.860 | F1(M)=0.6854\n",
      "Epoch 43/80 | Train  L=0.8528  A=91.45% | Valid  L=0.8706  A=91.57% | Rec: N=0.920 S=0.795 V=0.917 F=0.869 | F1(M)=0.6775\n",
      "Epoch 44/80 | Train  L=0.8502  A=91.72% | Valid  L=0.8712  A=90.06% | Rec: N=0.903 S=0.806 V=0.916 F=0.879 | F1(M)=0.6461\n",
      "Epoch 45/80 | Train  L=0.8517  A=91.75% | Valid  L=0.8701  A=91.90% | Rec: N=0.923 S=0.792 V=0.929 F=0.841 | F1(M)=0.6901\n",
      "Epoch 46/80 | Train  L=0.8482  A=91.71% | Valid  L=0.8667  A=91.75% | Rec: N=0.921 S=0.790 V=0.923 F=0.888 | F1(M)=0.6832\n",
      "Epoch 47/80 | Train  L=0.8454  A=92.14% | Valid  L=0.8645  A=91.69% | Rec: N=0.920 S=0.784 V=0.933 F=0.879 | F1(M)=0.6853\n",
      "Epoch 48/80 | Train  L=0.8507  A=91.38% | Valid  L=0.8659  A=90.18% | Rec: N=0.903 S=0.811 V=0.928 F=0.888 | F1(M)=0.6513\n",
      "Epoch 49/80 | Train  L=0.8461  A=91.93% | Valid  L=0.8637  A=92.16% | Rec: N=0.924 S=0.790 V=0.944 F=0.869 | F1(M)=0.7002\n",
      "Epoch 50/80 | Train  L=0.8426  A=92.22% | Valid  L=0.8644  A=91.70% | Rec: N=0.921 S=0.795 V=0.913 F=0.888 | F1(M)=0.6813\n",
      "Epoch 51/80 | Train  L=0.8440  A=92.24% | Valid  L=0.8618  A=91.22% | Rec: N=0.915 S=0.787 V=0.934 F=0.879 | F1(M)=0.6711\n",
      "Epoch 52/80 | Train  L=0.8475  A=91.98% | Valid  L=0.8650  A=91.87% | Rec: N=0.923 S=0.782 V=0.928 F=0.860 | F1(M)=0.6866\n",
      "Epoch 53/80 | Train  L=0.8469  A=92.06% | Valid  L=0.8646  A=91.64% | Rec: N=0.921 S=0.779 V=0.923 F=0.869 | F1(M)=0.6776\n",
      "Epoch 54/80 | Train  L=0.8465  A=91.85% | Valid  L=0.8664  A=92.05% | Rec: N=0.926 S=0.787 V=0.912 F=0.888 | F1(M)=0.6879\n",
      "Early stopping: no improvement in 5 epochs.\n",
      "\n",
      "[Fold 5] Best f1_macro=0.7002 at epoch 49\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 6/6\n",
      "============================================================\n",
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "Train counts: Normal=60061, SVEB=1854, VEB=4680, F=536\n",
      "Class weights: w0=0.279, w1=9.052, w2=3.586, w3=31.311\n",
      "Epoch  1/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  2/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  3/80 | Train  L=1.3863  A=89.47% | Valid  L=1.3863  A=89.47% | Rec: N=1.000 S=0.000 V=0.000 F=0.000 | F1(M)=0.2361\n",
      "Epoch  4/80 | Train  L=1.3415  A=84.55% | Valid  L=1.2617  A=79.76% | Rec: N=0.799 S=0.646 V=0.842 F=0.750 | F1(M)=0.5097\n",
      "Epoch  5/80 | Train  L=1.1711  A=81.35% | Valid  L=1.0488  A=81.02% | Rec: N=0.816 S=0.651 V=0.812 F=0.713 | F1(M)=0.5112\n",
      "Epoch  6/80 | Train  L=0.9680  A=80.94% | Valid  L=0.9510  A=82.07% | Rec: N=0.826 S=0.662 V=0.826 F=0.722 | F1(M)=0.5290\n",
      "Epoch  7/80 | Train  L=0.9431  A=82.83% | Valid  L=0.9387  A=83.34% | Rec: N=0.838 S=0.673 V=0.853 F=0.741 | F1(M)=0.5373\n",
      "Epoch  8/80 | Train  L=0.9351  A=84.00% | Valid  L=0.9333  A=84.27% | Rec: N=0.848 S=0.670 V=0.850 F=0.759 | F1(M)=0.5465\n",
      "Epoch  9/80 | Train  L=0.9304  A=84.48% | Valid  L=0.9306  A=84.42% | Rec: N=0.851 S=0.686 V=0.835 F=0.750 | F1(M)=0.5472\n",
      "Epoch 10/80 | Train  L=0.9228  A=85.12% | Valid  L=0.9232  A=86.33% | Rec: N=0.870 S=0.692 V=0.857 F=0.741 | F1(M)=0.5700\n",
      "Epoch 11/80 | Train  L=0.9177  A=85.93% | Valid  L=0.9187  A=86.71% | Rec: N=0.873 S=0.695 V=0.880 F=0.731 | F1(M)=0.5765\n",
      "Epoch 12/80 | Train  L=0.9121  A=86.17% | Valid  L=0.9177  A=86.65% | Rec: N=0.873 S=0.697 V=0.868 F=0.731 | F1(M)=0.5760\n",
      "Epoch 13/80 | Train  L=0.9089  A=86.49% | Valid  L=0.9108  A=87.30% | Rec: N=0.877 S=0.716 V=0.900 F=0.731 | F1(M)=0.5896\n",
      "Epoch 14/80 | Train  L=0.9046  A=86.46% | Valid  L=0.9082  A=86.11% | Rec: N=0.865 S=0.716 V=0.879 F=0.787 | F1(M)=0.5762\n",
      "Epoch 15/80 | Train  L=0.8993  A=86.73% | Valid  L=0.9049  A=87.54% | Rec: N=0.880 S=0.719 V=0.899 F=0.750 | F1(M)=0.5919\n",
      "Epoch 16/80 | Train  L=0.8954  A=87.14% | Valid  L=0.9055  A=85.96% | Rec: N=0.862 S=0.730 V=0.887 F=0.796 | F1(M)=0.5781\n",
      "Epoch 17/80 | Train  L=0.8934  A=87.63% | Valid  L=0.9020  A=86.82% | Rec: N=0.872 S=0.732 V=0.886 F=0.796 | F1(M)=0.5853\n",
      "Epoch 18/80 | Train  L=0.8900  A=88.04% | Valid  L=0.8964  A=87.93% | Rec: N=0.883 S=0.732 V=0.901 F=0.787 | F1(M)=0.6000\n",
      "Epoch 19/80 | Train  L=0.8862  A=88.46% | Valid  L=0.8919  A=88.41% | Rec: N=0.888 S=0.743 V=0.901 F=0.806 | F1(M)=0.6102\n",
      "Epoch 20/80 | Train  L=0.8822  A=88.65% | Valid  L=0.8898  A=89.04% | Rec: N=0.895 S=0.730 V=0.900 F=0.806 | F1(M)=0.6178\n",
      "Epoch 21/80 | Train  L=0.8795  A=88.95% | Valid  L=0.8863  A=90.37% | Rec: N=0.909 S=0.743 V=0.916 F=0.759 | F1(M)=0.6391\n",
      "Epoch 22/80 | Train  L=0.8775  A=89.37% | Valid  L=0.8841  A=89.11% | Rec: N=0.895 S=0.738 V=0.917 F=0.787 | F1(M)=0.6203\n",
      "Epoch 23/80 | Train  L=0.8752  A=88.94% | Valid  L=0.8813  A=89.82% | Rec: N=0.902 S=0.746 V=0.927 F=0.796 | F1(M)=0.6322\n",
      "Epoch 24/80 | Train  L=0.8737  A=89.34% | Valid  L=0.8816  A=90.18% | Rec: N=0.907 S=0.746 V=0.906 F=0.806 | F1(M)=0.6366\n",
      "Epoch 25/80 | Train  L=0.8720  A=89.41% | Valid  L=0.8795  A=89.50% | Rec: N=0.897 S=0.751 V=0.928 F=0.824 | F1(M)=0.6311\n",
      "Epoch 26/80 | Train  L=0.8683  A=89.80% | Valid  L=0.8777  A=88.05% | Rec: N=0.880 S=0.776 V=0.924 F=0.870 | F1(M)=0.6110\n",
      "Early stopping: no improvement in 5 epochs.\n",
      "\n",
      "[Fold 6] Best f1_macro=0.6391 at epoch 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_steps = 10\n",
    "epochs = 15\n",
    "k_folds=6\n",
    "\n",
    "# Step 1: Load Data\n",
    "train_data, train_targets = load_train_test_data()\n",
    "\n",
    "# Step 2: Convert Data to Tensors\n",
    "dataset = create_csnn_datasets(train_data, train_targets) \n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ❌ REMOVE THESE LINES - Not needed anymore!\n",
    "# csnet2.to(device)\n",
    "# optimizer2 = torch.optim.Adam(csnet2.parameters(), lr=0.0001)\n",
    "\n",
    "# loss_func = torch.nn.CrossEntropyLoss()   # template (used)\n",
    "# loss_func = FocalLoss(alpha=[0.25, 0.75], gamma=2.0)  # favors abnormal a bit\n",
    "\n",
    "# Run CV - this creates fresh models per fold\n",
    "fold_history, fold_ckpts = train_model_cv_4class(\n",
    "    model_factory=create_qcsnn_model_4class,\n",
    "    epochs=80,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    loss_func=None,  # Created per-fold\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 0.0001},\n",
    "    num_steps=10,\n",
    "    k_folds=6,\n",
    "    batch_size=256,\n",
    "    monitor=\"f1_macro\",  # Or \"recall_veb\" for VEB-specific\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37027f7d-7fc3-4a35-a610-116a0712a375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f83681-7b52-4e85-b104-9943940d4ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2956a952-45a5-4ac1-9c24-c247cb5754be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: MIT-BIH (20161, 180)\n",
      "Data range after z-score normalization: [-5.26, 6.12]\n",
      "Data mean: 0.0000, std: 1.0000\n",
      "Class distribution: Counter({0: 18052, 2: 1393, 1: 557, 3: 159})\n",
      "X train shape: (20161, 1, 180)\n",
      "y_train shape: (20161,)\n",
      "Data tensor loaded successfully\n",
      "dataset created successfully\n",
      "Test set size: 20161\n",
      "Test normal samples: 18052\n",
      "Test sveb samples: 557\n",
      "Test veb samples: 1393\n",
      "Test f samples: 159\n"
     ]
    }
   ],
   "source": [
    "# Load test data (adjust function name if different)\n",
    "mitbih_test_path='/.../data/mitbih_processed_intra_patient_4class_180_center90_filtered/test'\n",
    "\n",
    "test_data, test_targets = load_train_test_data(mitbih_test_path)\n",
    "test_dataset = create_csnn_datasets(test_data, test_targets)\n",
    "\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "print(f\"Test normal samples: {(test_targets == 0).sum()}\")\n",
    "print(f\"Test sveb samples: {(test_targets == 1).sum()}\")\n",
    "print(f\"Test veb samples: {(test_targets == 2).sum()}\")\n",
    "print(f\"Test f samples: {(test_targets == 3).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd6701-a6b6-4e31-939a-7b3da8b889bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9b1bf9e-7feb-4135-8765-660b88d91352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "import snntorch as snn\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "\n",
    "def _ensure_dir(d):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def _to_one(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.numel() == 1:\n",
    "            return float(x.detach().cpu().item())\n",
    "        return x.detach().cpu().numpy()\n",
    "    if isinstance(x, (float, int)):\n",
    "        return x\n",
    "    return x\n",
    "\n",
    "def _get_bit_scale_zp_from_quant(q):\n",
    "    \"\"\"Return (bit_width, scale, zero_point, signed) from a brevitas quantizer-like object.\"\"\"\n",
    "    bit_width = None\n",
    "    scale = None\n",
    "    zero_point = None\n",
    "    signed = True\n",
    "    # bit width\n",
    "    for k in ('bit_width', 'bit_width_impl', 'bit_width_f'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            bit_width = int(_to_one(v))\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # scale\n",
    "    for k in ('scale', 'tensor_scale', 'scale_impl', 'act_scale', 'weight_scale'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            v = _to_one(v)\n",
    "            if isinstance(v, (float, int)):\n",
    "                scale = float(v)\n",
    "                break\n",
    "            if isinstance(v, np.ndarray) and v.size == 1:\n",
    "                scale = float(v.item())\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # zero-point\n",
    "    for k in ('zero_point', 'zero_point_impl', 'zp'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            zero_point = int(round(_to_one(v)))\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # signed\n",
    "    sattr = getattr(q, 'signed', None)\n",
    "    if isinstance(sattr, bool):\n",
    "        signed = sattr\n",
    "    elif zero_point is None:\n",
    "        signed = True\n",
    "    # defaults\n",
    "    if bit_width is None: bit_width = 8\n",
    "    if scale is None:     scale = 1.0\n",
    "    if zero_point is None: zero_point = 0\n",
    "    return bit_width, float(scale), int(zero_point), bool(signed)\n",
    "\n",
    "def _quantize_multiplier(real_multiplier: float):\n",
    "    \"\"\"TFLite-style integer multiplier/shift approximation for a positive real multiplier.\"\"\"\n",
    "    if real_multiplier <= 0.0:\n",
    "        return 0, 0\n",
    "    mantissa, exponent = math.frexp(real_multiplier)  # real = mantissa * 2^exponent, mantissa in [0.5,1)\n",
    "    q = int(round(mantissa * (1 << 31)))\n",
    "    if q == (1 << 31):\n",
    "        q //= 2\n",
    "        exponent += 1\n",
    "    shift = 31 - exponent\n",
    "    if shift < 0:\n",
    "        q <<= (-shift)\n",
    "        shift = 0\n",
    "    return int(q), int(shift)\n",
    "\n",
    "# helper types used in headers (names only; arrays use ap_int in C++)\n",
    "def _sum_weights_per_out(W_int8: np.ndarray) -> np.ndarray:\n",
    "    # W_int8 shape: [OUT_CH, IN_CH, K]\n",
    "    # returns int32 sums per OUT_CH\n",
    "    return W_int8.reshape(W_int8.shape[0], -1).sum(axis=1).astype(np.int32)\n",
    "\n",
    "def _bias_int32_vector(b_f: np.ndarray, s_in: float, s_w: float, out_ch: int) -> np.ndarray:\n",
    "    # Quantize bias or return zeros if no bias provided\n",
    "    if b_f is None:\n",
    "        return np.zeros(out_ch, dtype=np.int32)\n",
    "    s_bias = s_in * s_w if (s_in and s_w) else 1.0\n",
    "    return np.round(b_f / s_bias).astype(np.int32)\n",
    "\n",
    "\n",
    "def _qt_weight_int8_per_tensor(W_f: np.ndarray, scale: float, zero_point: int = 0):\n",
    "    Wq = np.round(W_f / scale) + zero_point\n",
    "    return np.clip(Wq, -128, 127).astype(np.int8)\n",
    "\n",
    "def _bias_int32_from_float(b_f: np.ndarray, s_in: float, s_w: float):\n",
    "    s_bias = s_in * s_w\n",
    "    if s_bias == 0.0: s_bias = 1.0\n",
    "    bq = np.round(b_f / s_bias).astype(np.int32)\n",
    "    return bq\n",
    "\n",
    "def _as1(x):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return int(x[0])\n",
    "    return int(x)\n",
    "\n",
    "def _guard_out_scale(name: str, out_scale: float):\n",
    "    if out_scale is None or not np.isfinite(out_scale) or out_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid out_scale={out_scale}\")\n",
    "\n",
    "def _guard_in_scale(name: str, in_scale: float):\n",
    "    if in_scale is None or not np.isfinite(in_scale) or in_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid in_scale={in_scale}\")\n",
    "\n",
    "def _guard_weight_scale(name: str, w_scale: float):\n",
    "    if w_scale is None or not np.isfinite(w_scale) or w_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid weight_scale={w_scale}\")\n",
    "\n",
    "def _id_guard_macro(base: str):\n",
    "    return base.upper().replace('/', '_').replace('.', '_')\n",
    "\n",
    "def _sym(name: str):\n",
    "    \"\"\"C identifier from layer name (keep as-is but safe for C).\"\"\"\n",
    "    return name.replace('/', '_').replace('.', '_')\n",
    "\n",
    "def _fmt_int_list(vals, per_line=16):\n",
    "    out = []\n",
    "    line = []\n",
    "    for i, v in enumerate(vals):\n",
    "        line.append(str(int(v)))\n",
    "        if (i + 1) % per_line == 0:\n",
    "            out.append(\", \".join(line))\n",
    "            line = []\n",
    "    if line:\n",
    "        out.append(\", \".join(line))\n",
    "    return \",\\n    \".join(out)\n",
    "\n",
    "def _fmt_array_2d(arr2d):\n",
    "    rows = []\n",
    "    for r in arr2d:\n",
    "        rows.append(\"{ \" + _fmt_int_list(r) + \" }\")\n",
    "    return \"{\\n  \" + \",\\n  \".join(rows) + \"\\n}\"\n",
    "\n",
    "def _fmt_array_3d(arr3d):\n",
    "    blocks = []\n",
    "    for b in arr3d:\n",
    "        blocks.append(_fmt_array_2d(b))\n",
    "    return \"{\\n\" + \",\\n\".join(blocks) + \"\\n}\"\n",
    "\n",
    "# -----------------------\n",
    "# Emitters\n",
    "# -----------------------\n",
    "\n",
    "def _emit_header_open(fp, guard, ns=\"hls4csnn1d_cblk_sd\"):\n",
    "    fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "    fp.write(\"#include <ap_int.h>\\n\\n\")\n",
    "    fp.write(f\"namespace {ns} {{\\n\\n\")\n",
    "\n",
    "def _emit_header_close(fp, guard, ns=\"hls4csnn1d_cblk_sd\"):\n",
    "    fp.write(f\"}} // namespace\\n#endif // {guard}\\n\")\n",
    "\n",
    "def _emit_conv1d_header(path, lname, W_int8, rq_mult, rq_shift,\n",
    "                        bias_int32_vec, input_zp, weight_sum_vec, m):\n",
    "    # Guard: QCSNET2_CBLK1_QCONV1D_WEIGHTS_H style\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_WEIGHTS_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        _emit_header_open(fp, guard)  # writes includes + namespace line\n",
    "\n",
    "        sym   = _sym(lname)\n",
    "        OC, IC, K = W_int8.shape\n",
    "        stride = _as1(m.stride)\n",
    "\n",
    "        # Structural constants (optional but handy for TBs)\n",
    "        fp.write(f\"const int {sym}_OUT_CH = {OC};\\n\")\n",
    "        fp.write(f\"const int {sym}_IN_CH  = {IC};\\n\")\n",
    "        fp.write(f\"const int {sym}_KERNEL_SIZE = {K};\\n\")\n",
    "        fp.write(f\"const int {sym}_STRIDE = {stride};\\n\\n\")\n",
    "\n",
    "        # Input ZP (INT8)\n",
    "        fp.write(f\"const ap_int<8> {sym}_input_zero_point = {int(input_zp)};\\n\\n\")\n",
    "\n",
    "        # Requantization arrays (duplicated per OUT_CH to match your API)\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_mult]*OC))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const int {sym}_right_shift[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_shift]*OC))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Bias (INT32)\n",
    "        fp.write(f\"const acc32_t {sym}_bias[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(bias_int32_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weight sums for asymmetric correction (INT32)\n",
    "        fp.write(f\"const acc32_t {sym}_weight_sum[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(weight_sum_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weights (INT8): [OUT_CH][IN_CH][K]\n",
    "        fp.write(f\"const ap_int<8> {sym}_weights[{OC}][{IC}][{K}] = \")\n",
    "        fp.write(_fmt_array_3d(W_int8))\n",
    "        fp.write(\";\\n\\n\")\n",
    "\n",
    "        _emit_header_close(fp, guard)  # closes namespace + guard\n",
    "\n",
    "\n",
    "def _emit_linear_header(path, lname,\n",
    "                           W_int8,                 # [OUT][IN] int8\n",
    "                           rq_mult, rq_shift,      # per-tensor constants, repeated per OUT\n",
    "                           bias_int32_vec,         # [OUT] int32\n",
    "                           input_zp,               # int (will be placed as ap_int<8>)\n",
    "                           weight_sum_vec):        # [OUT] int32\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_WEIGHTS_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "\n",
    "        sym = _sym(lname)\n",
    "        OUT, IN = W_int8.shape\n",
    "\n",
    "        # Structural (optional helpers)\n",
    "        fp.write(f\"const int {sym}_OUTPUT_SIZE = {OUT};\\n\")\n",
    "        fp.write(f\"const int {sym}_INPUT_SIZE  = {IN};\\n\\n\")\n",
    "\n",
    "        # Input zero-point (matches template type)\n",
    "        fp.write(f\"const ap_int<8> {sym}_input_zero_point = {int(input_zp)};\\n\\n\")\n",
    "\n",
    "        # Requant arrays (match template types; repeat the per-tensor constants)\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_mult] * OUT))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const int {sym}_right_shift[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_shift] * OUT))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Bias and weight_sum (acc domain)\n",
    "        fp.write(f\"using acc32_t = ap_int<32>;\\n\")\n",
    "        fp.write(f\"const acc32_t {sym}_bias[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(bias_int32_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const acc32_t {sym}_weight_sum[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(weight_sum_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weights (use ap_int8_c to match your template)\n",
    "        fp.write(f\"const ap_int8_c {sym}_weights[{OUT}][{IN}] = \")\n",
    "        fp.write(_fmt_array_2d(W_int8))\n",
    "        fp.write(\";\\n\\n\")\n",
    "\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def _emit_bn_header(path, lname, w_q, b32, mult_arr, shift_arr):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_BN_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        sym = _sym(lname); C = len(w_q)\n",
    "\n",
    "        fp.write(f\"const int {sym}_C = {C};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const ap_int8_c {sym}_weight[{C}] = {{\\n  {_fmt_int_list(w_q)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<32> {sym}_bias[{C}] = {{\\n  {_fmt_int_list(b32)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{C}] = {{\\n  {_fmt_int_list(mult_arr)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const int {sym}_right_shift[{C}] = {{\\n  {_fmt_int_list(shift_arr)}\\n}};\\n\\n\")\n",
    "\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def _emit_lif_header_scalar_sd(path, lname, beta_q, theta_q, scale_q, frac_bits):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_LIF_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        sym = _sym(lname)\n",
    "        fp.write(f\"enum {{ {sym}_FRAC_BITS = {int(frac_bits)} }};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_beta_int   = {int(beta_q)};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_theta_int  = {int(theta_q)};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_scale_int  = {int(scale_q)};\\n\\n\")\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "def _emit_lif_header_vector_sd(path, lname, beta_arr_q, theta_arr_q, scale_q, frac_bits):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_LIF_H\")\n",
    "    sym   = _sym(lname)\n",
    "    N     = len(beta_arr_q)\n",
    "    if len(theta_arr_q) != N:\n",
    "        raise ValueError(\"beta/theta array lengths must match\")\n",
    "\n",
    "    def _fmt_list(vals, per_line=16):\n",
    "        rows = []\n",
    "        for i in range(0, len(vals), per_line):\n",
    "            chunk = \", \".join(str(int(v)) for v in vals[i:i+per_line])\n",
    "            rows.append(\"    \" + chunk)\n",
    "        return \"{\\n\" + \",\\n\".join(rows) + \"\\n}\"\n",
    "\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        fp.write(f\"enum {{ {sym}_FRAC_BITS = {int(frac_bits)}, {sym}_OUT_CH = {int(N)} }};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_scale_int = {int(scale_q)};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_beta_int[{sym}_OUT_CH] = \"  + _fmt_list(beta_arr_q)  + \";\\n\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_theta_int[{sym}_OUT_CH] = \" + _fmt_list(theta_arr_q) + \";\\n\\n\")\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "INT16_MIN, INT16_MAX = -32768, 32767\n",
    "\n",
    "def _to_q_i16(x: float, Q: int) -> int:\n",
    "    return int(np.clip(round(float(x) * Q), INT16_MIN, INT16_MAX))\n",
    "\n",
    "def _tensor_to_q_i16_list(t: torch.Tensor, Q: int):\n",
    "    flat = t.detach().float().reshape(-1).cpu().tolist()\n",
    "    return [_to_q_i16(v, Q) for v in flat]\n",
    "\n",
    "\n",
    "_Q_SCALE = 1 << 12   # e.g., FRAC_BITS = 12\n",
    "\n",
    "def _emit_qparams_header(path, lname, bit_w, scale, zp):\n",
    "    guard = _id_guard_macro(f\"QPARAMS_{_sym(lname)}_H\")\n",
    "    sym = _sym(lname)\n",
    "    sym_base = sym  # no renaming, no suffix stripping\n",
    "\n",
    "    # Q-encode the activation scale for HLS QuantIdentity (uses _Q_SCALE; not emitted)\n",
    "    act_scale_int = _to_q_i16(float(scale), _Q_SCALE)\n",
    "\n",
    "    with open(path, \"w\") as fp:\n",
    "        _emit_header_open(fp, guard)  # must include <ap_int.h> and open your namespace\n",
    "\n",
    "        fp.write(\"// Activation quantization parameters (optional for kernels)\\n\")\n",
    "        fp.write(f\"const int   {sym}_bit_width = {int(bit_w)};\\n\")\n",
    "        fp.write(f\"// const float {sym}_scale     = {float(scale):.10g};  // kept for reference only\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym_base}_act_scale_int = {int(act_scale_int)};\\n\")\n",
    "        fp.write(f\"const int   {sym}_zero_point= {int(zp)};\\n\\n\")\n",
    "\n",
    "        _emit_header_close(fp, guard)  # close namespace and guard\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Orchestrator\n",
    "# -----------------------\n",
    "\n",
    "def emit_headers_for_model(model: torch.nn.Module,\n",
    "                           example_input: torch.Tensor,\n",
    "                           out_dir: str = \"headers_int\",\n",
    "                           lif_frac_bits: int = 12):\n",
    "    model.eval()\n",
    "    _ensure_dir(out_dir)\n",
    "\n",
    "    # Run one dry forward to initialize any lazy buffers (ignore output tuples)\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            _ = model(example_input)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Track current activation qparams (propagated as in your graph)\n",
    "    current_act = {\"bit_width\": 8, \"scale\": 1.0, \"zero_point\": 0}\n",
    "    \n",
    "    last_out_ch = None  # initialize outside the loop\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        if m is model:\n",
    "            continue\n",
    "\n",
    "        # QuantIdentity (export activation qparams as optional header)\n",
    "        if isinstance(m, qnn.QuantIdentity):\n",
    "            aq = getattr(m, 'act_quant', getattr(m, 'output_quant', None))\n",
    "            bit_w, s, zp, _ = _get_bit_scale_zp_from_quant(aq)\n",
    "            _emit_qparams_header(os.path.join(out_dir, f\"qparams_{name}.h\"), name, bit_w, s, zp)\n",
    "            current_act = {\"bit_width\": bit_w, \"scale\": s, \"zero_point\": zp}\n",
    "            continue\n",
    "\n",
    "\n",
    "        if isinstance(m, qnn.QuantConv1d):\n",
    "            # Float → INT8 weights\n",
    "            Wf = m.weight.detach().cpu().numpy()            # [OUT, IN, K]\n",
    "            wq = getattr(m, 'weight_quant', None)\n",
    "            wb, s_w, z_w, _ = _get_bit_scale_zp_from_quant(wq)\n",
    "        \n",
    "            _guard_in_scale(name, current_act['scale'])\n",
    "            _guard_weight_scale(name, s_w)\n",
    "        \n",
    "            W_int8 = _qt_weight_int8_per_tensor(Wf, s_w, z_w)\n",
    "        \n",
    "            # Output activation qparams (for requant)\n",
    "            oq = getattr(m, 'output_quant', None)\n",
    "            ob, s_out, z_out, _ = _get_bit_scale_zp_from_quant(oq)\n",
    "            _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Integer requant constants (per-tensor → repeat per OUT_CH)\n",
    "            M = (current_act['scale'] * s_w) / s_out\n",
    "            rq_mult, rq_shift = _quantize_multiplier(M)\n",
    "        \n",
    "            # Bias (if present) → INT32 vector (length OUT_CH); else zeros\n",
    "            b_f = m.bias.detach().cpu().numpy() if (hasattr(m, 'bias') and m.bias is not None) else None\n",
    "            bias_int32_vec = _bias_int32_vector(b_f, current_act['scale'], s_w, W_int8.shape[0])\n",
    "        \n",
    "            # Weight sums per output channel (for asymmetric correction)\n",
    "            weight_sum_vec = _sum_weights_per_out(W_int8)\n",
    "        \n",
    "            # Input zero point (INT8) for asymmetric correction\n",
    "            input_zp = current_act['zero_point']\n",
    "        \n",
    "            # Emit header that matches your Conv1D_SD::forward signature\n",
    "            _emit_conv1d_header(\n",
    "                os.path.join(out_dir, f\"{name}_weights.h\"),\n",
    "                name,\n",
    "                W_int8,\n",
    "                rq_mult, rq_shift,\n",
    "                bias_int32_vec,\n",
    "                input_zp,\n",
    "                weight_sum_vec,\n",
    "                m\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams for the next layer\n",
    "            current_act = {\"bit_width\": ob, \"scale\": s_out, \"zero_point\": z_out}\n",
    "            continue\n",
    "\n",
    "\n",
    "        # BN -> ScaleBias (for BatchNorm1dToQuantScaleBias)\n",
    "        if hasattr(qnn, 'BatchNorm1dToQuantScaleBias') and isinstance(m, qnn.BatchNorm1dToQuantScaleBias):\n",
    "            # gamma, beta (float, per channel)\n",
    "            gamma = _to_one(getattr(m, 'weight', None)) if getattr(m, 'weight', None) is not None else _to_one(getattr(m, 'scale', None))\n",
    "            beta  = _to_one(getattr(m, 'bias',   None)) if getattr(m, 'bias',   None) is not None else _to_one(getattr(m, 'beta',  None))\n",
    "            if gamma is None: gamma = 1.0\n",
    "            if beta  is None: beta  = 0.0\n",
    "            gamma = np.array(gamma, dtype=np.float32).reshape(-1)\n",
    "            beta  = np.array(beta,  dtype=np.float32).reshape(-1)\n",
    "            C = gamma.shape[0]\n",
    "        \n",
    "            # Input/output quant\n",
    "            s_in = float(current_act['scale']);  z_in = int(current_act['zero_point']);  _guard_in_scale(name, s_in)\n",
    "            oq   = getattr(m, 'output_quant', None)\n",
    "            _, s_out, _, _ = _get_bit_scale_zp_from_quant(oq);  _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Brevitas BN quantized weight\n",
    "            qW = m.quant_weight()  # IntQuantTensor\n",
    "            w_q = qW.int().detach().cpu().numpy().astype(np.int8)    # [C]\n",
    "            s_w = float(_to_one(qW.scale))                            # scalar\n",
    "        \n",
    "            # Shared requant scale S and its integer pair\n",
    "            S = s_w * (s_in / s_out)\n",
    "            mult_S, shift_S = _quantize_multiplier(S)\n",
    "        \n",
    "            # Int32 bias per channel (no int8 clipping)\n",
    "            # M_c = (s_in/s_out) * gamma_c  = S * w_q[c]  (approximately)\n",
    "            M = gamma * (s_in / s_out)                          # [C]\n",
    "            b32 = np.round((beta / s_out - M * z_in) / S).astype(np.int32)  # [C]\n",
    "        \n",
    "            _emit_bn_header(\n",
    "                os.path.join(out_dir, f\"{name}_bn.h\"),\n",
    "                name,\n",
    "                w_q.tolist(),\n",
    "                b32.tolist(),\n",
    "                [mult_S] * C,\n",
    "                [shift_S] * C\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams\n",
    "            current_act = {\"bit_width\": current_act['bit_width'], \"scale\": s_out, \"zero_point\": 0}\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # QuantLinear\n",
    "        if isinstance(m, qnn.QuantLinear):\n",
    "            # Float → INT8 weights\n",
    "            Wf = m.weight.detach().cpu().numpy()             # [OUT, IN]\n",
    "            wq = getattr(m, 'weight_quant', None)\n",
    "            wb, s_w, z_w, _ = _get_bit_scale_zp_from_quant(wq)\n",
    "        \n",
    "            _guard_in_scale(name, current_act['scale'])\n",
    "            _guard_weight_scale(name, s_w)\n",
    "        \n",
    "            W_int8 = _qt_weight_int8_per_tensor(Wf, s_w, z_w)\n",
    "        \n",
    "            # Output activation qparams (for requant)\n",
    "            oq = getattr(m, 'output_quant', None)\n",
    "            ob, s_out, z_out, _ = _get_bit_scale_zp_from_quant(oq)\n",
    "            _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Requant: M = (s_in * s_w) / s_out\n",
    "            M = (current_act['scale'] * s_w) / s_out\n",
    "            rq_mult, rq_shift = _quantize_multiplier(M)\n",
    "        \n",
    "            # Bias (if present) → INT32; else zeros\n",
    "            b_f = m.bias.detach().cpu().numpy() if (hasattr(m, 'bias') and m.bias is not None) else None\n",
    "            bias_int32_vec = _bias_int32_vector(b_f, current_act['scale'], s_w, W_int8.shape[0])\n",
    "        \n",
    "            # Weight sums for asymmetric correction: sum over input dim\n",
    "            weight_sum_vec = W_int8.sum(axis=1).astype(np.int32)\n",
    "        \n",
    "            # Input zero-point for asymmetric correction\n",
    "            input_zp = current_act['zero_point']\n",
    "        \n",
    "            # Emit header that matches Linear1D_SD::forward\n",
    "            _emit_linear_header(\n",
    "                os.path.join(out_dir, f\"{name}_weights.h\"),\n",
    "                name,\n",
    "                W_int8,\n",
    "                rq_mult, rq_shift,\n",
    "                bias_int32_vec,\n",
    "                input_zp,\n",
    "                weight_sum_vec\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams\n",
    "            current_act = {\"bit_width\": ob, \"scale\": s_out, \"zero_point\": z_out}\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if isinstance(m, snn.Leaky):\n",
    "            scale_in = float(current_act['scale'])\n",
    "            Q = 1 << lif_frac_bits\n",
    "        \n",
    "            # Grab beta/threshold as tensors (handles both scalar and vector)\n",
    "            beta_t  = m.beta if isinstance(m.beta, torch.Tensor) else torch.as_tensor(m.beta)\n",
    "            thr_t   = m.threshold if isinstance(m.threshold, torch.Tensor) else torch.as_tensor(m.threshold)\n",
    "        \n",
    "            beta_t  = beta_t.detach().float()\n",
    "            print(\"beta: \", beta_t)\n",
    "            thr_t   = thr_t.detach().float()\n",
    "        \n",
    "            # Number of “neurons” (channels) from parameter size\n",
    "            n_beta  = int(beta_t.numel())\n",
    "            n_thr   = int(thr_t.numel())\n",
    "            if n_beta != n_thr and n_beta != 1 and n_thr != 1:\n",
    "                raise ValueError(f\"LIF param size mismatch: beta has {n_beta}, threshold has {n_thr}\")\n",
    "        \n",
    "            # Broadcast if one is scalar\n",
    "            out_ch = max(n_beta, n_thr)\n",
    "            if n_beta == 1 and out_ch > 1:\n",
    "                beta_t = beta_t.expand(out_ch)\n",
    "            if n_thr == 1 and out_ch > 1:\n",
    "                thr_t = thr_t.expand(out_ch)\n",
    "        \n",
    "            # Quantize\n",
    "            beta_arr_q  = _tensor_to_q_i16_list(beta_t, Q)\n",
    "            theta_arr_q = _tensor_to_q_i16_list(thr_t, Q)\n",
    "            scale_q     = _to_q_i16(scale_in, Q)   # keep scale scalar for now\n",
    "        \n",
    "            # Emit vector or scalar header depending on out_ch\n",
    "            out_path = os.path.join(out_dir, f\"{name}_lif.h\")\n",
    "            if out_ch > 1:\n",
    "                _emit_lif_header_vector_sd(\n",
    "                    out_path, name, beta_arr_q, theta_arr_q, scale_q, lif_frac_bits\n",
    "                )\n",
    "            else:\n",
    "                _emit_lif_header_scalar_sd(\n",
    "                    out_path, name, beta_arr_q[0], theta_arr_q[0], scale_q, lif_frac_bits\n",
    "                )\n",
    "        \n",
    "            # LIF outputs binary spikes {0,1} → treat next op input scale as 1.0\n",
    "            current_act = {\"bit_width\": 8, \"scale\": 1.0, \"zero_point\": 0}\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"[emit] C++ headers written to: {os.path.abspath(out_dir)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0c180-d340-4b2b-ada7-f3a86e20d7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19c1c471-e9ff-48b5-9a3b-82326c9c7eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "beta:  tensor(0.4599)\n",
      "beta:  tensor(0.5905)\n",
      "beta:  tensor(0.7678)\n",
      "beta:  tensor(0.8207)\n",
      "beta:  tensor(1.0003)\n",
      "[emit] C++ headers written to: /home/velox-217533/Projects/fau_projects/research/snn_quant/model4/intra_patient_models/weights_sd/intra_patient/headers_stage2\n"
     ]
    }
   ],
   "source": [
    "# Extract weights from Fold 5 (4-class model)\n",
    "model_stage1 = create_qcsnn_model_4class()\n",
    "model_stage1.load_state_dict(fold_ckpts['fold5']['state_dict_cpu'])\n",
    "model_stage1.eval()\n",
    "\n",
    "# Create example input\n",
    "example_input = torch.randn(1, 1, 180)\n",
    "\n",
    "# Extract to C++ headers\n",
    "emit_headers_for_model(\n",
    "    model=model_stage1,\n",
    "    example_input=example_input,\n",
    "    out_dir=\"weights_sd/intra_patient/headers_stage2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c55f8a59-254b-4a3e-8af2-f0c92136f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sizes: Block1=89, Block2=43, Block3=20\n",
      "Flattened size: 480\n",
      "Saved: fold5_4class_fpga_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# 2. 4-class model (Fold 5)\n",
    "model_4class = create_qcsnn_model_4class()\n",
    "model_4class.load_state_dict(fold_ckpts['fold5']['state_dict_cpu'])\n",
    "torch.save(model_4class.state_dict(), 'weights_sd/fold5_4class_fpga_weights.pth')\n",
    "print(\"Saved: fold5_4class_fpga_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f53c1-af80-4080-86d1-f6ef42e1b4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde33d3a-f13a-4da8-a3da-f073727c3f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae213b-4b02-421a-9924-f50d517acabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "import snntorch as snn\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "\n",
    "def _ensure_dir(d):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def _to_one(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.numel() == 1:\n",
    "            return float(x.detach().cpu().item())\n",
    "        return x.detach().cpu().numpy()\n",
    "    if isinstance(x, (float, int)):\n",
    "        return x\n",
    "    return x\n",
    "\n",
    "def _get_bit_scale_zp_from_quant(q):\n",
    "    \"\"\"Return (bit_width, scale, zero_point, signed) from a brevitas quantizer-like object.\"\"\"\n",
    "    bit_width = None\n",
    "    scale = None\n",
    "    zero_point = None\n",
    "    signed = True\n",
    "    # bit width\n",
    "    for k in ('bit_width', 'bit_width_impl', 'bit_width_f'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            bit_width = int(_to_one(v))\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # scale\n",
    "    for k in ('scale', 'tensor_scale', 'scale_impl', 'act_scale', 'weight_scale'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            v = _to_one(v)\n",
    "            if isinstance(v, (float, int)):\n",
    "                scale = float(v)\n",
    "                break\n",
    "            if isinstance(v, np.ndarray) and v.size == 1:\n",
    "                scale = float(v.item())\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # zero-point\n",
    "    for k in ('zero_point', 'zero_point_impl', 'zp'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            zero_point = int(round(_to_one(v)))\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # signed\n",
    "    sattr = getattr(q, 'signed', None)\n",
    "    if isinstance(sattr, bool):\n",
    "        signed = sattr\n",
    "    elif zero_point is None:\n",
    "        signed = True\n",
    "    # defaults\n",
    "    if bit_width is None: bit_width = 8\n",
    "    if scale is None:     scale = 1.0\n",
    "    if zero_point is None: zero_point = 0\n",
    "    return bit_width, float(scale), int(zero_point), bool(signed)\n",
    "\n",
    "def _quantize_multiplier(real_multiplier: float):\n",
    "    \"\"\"TFLite-style integer multiplier/shift approximation for a positive real multiplier.\"\"\"\n",
    "    if real_multiplier <= 0.0:\n",
    "        return 0, 0\n",
    "    mantissa, exponent = math.frexp(real_multiplier)  # real = mantissa * 2^exponent, mantissa in [0.5,1)\n",
    "    q = int(round(mantissa * (1 << 31)))\n",
    "    if q == (1 << 31):\n",
    "        q //= 2\n",
    "        exponent += 1\n",
    "    shift = 31 - exponent\n",
    "    if shift < 0:\n",
    "        q <<= (-shift)\n",
    "        shift = 0\n",
    "    return int(q), int(shift)\n",
    "\n",
    "# helper types used in headers (names only; arrays use ap_int in C++)\n",
    "def _sum_weights_per_out(W_int8: np.ndarray) -> np.ndarray:\n",
    "    # W_int8 shape: [OUT_CH, IN_CH, K]\n",
    "    # returns int32 sums per OUT_CH\n",
    "    return W_int8.reshape(W_int8.shape[0], -1).sum(axis=1).astype(np.int32)\n",
    "\n",
    "def _bias_int32_vector(b_f: np.ndarray, s_in: float, s_w: float, out_ch: int) -> np.ndarray:\n",
    "    # Quantize bias or return zeros if no bias provided\n",
    "    if b_f is None:\n",
    "        return np.zeros(out_ch, dtype=np.int32)\n",
    "    s_bias = s_in * s_w if (s_in and s_w) else 1.0\n",
    "    return np.round(b_f / s_bias).astype(np.int32)\n",
    "\n",
    "\n",
    "def _qt_weight_int8_per_tensor(W_f: np.ndarray, scale: float, zero_point: int = 0):\n",
    "    Wq = np.round(W_f / scale) + zero_point\n",
    "    return np.clip(Wq, -128, 127).astype(np.int8)\n",
    "\n",
    "def _bias_int32_from_float(b_f: np.ndarray, s_in: float, s_w: float):\n",
    "    s_bias = s_in * s_w\n",
    "    if s_bias == 0.0: s_bias = 1.0\n",
    "    bq = np.round(b_f / s_bias).astype(np.int32)\n",
    "    return bq\n",
    "\n",
    "def _as1(x):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return int(x[0])\n",
    "    return int(x)\n",
    "\n",
    "def _guard_out_scale(name: str, out_scale: float):\n",
    "    if out_scale is None or not np.isfinite(out_scale) or out_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid out_scale={out_scale}\")\n",
    "\n",
    "def _guard_in_scale(name: str, in_scale: float):\n",
    "    if in_scale is None or not np.isfinite(in_scale) or in_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid in_scale={in_scale}\")\n",
    "\n",
    "def _guard_weight_scale(name: str, w_scale: float):\n",
    "    if w_scale is None or not np.isfinite(w_scale) or w_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid weight_scale={w_scale}\")\n",
    "\n",
    "def _id_guard_macro(base: str):\n",
    "    return base.upper().replace('/', '_').replace('.', '_')\n",
    "\n",
    "def _sym(name: str):\n",
    "    \"\"\"C identifier from layer name (keep as-is but safe for C).\"\"\"\n",
    "    return name.replace('/', '_').replace('.', '_')\n",
    "\n",
    "def _fmt_int_list(vals, per_line=16):\n",
    "    out = []\n",
    "    line = []\n",
    "    for i, v in enumerate(vals):\n",
    "        line.append(str(int(v)))\n",
    "        if (i + 1) % per_line == 0:\n",
    "            out.append(\", \".join(line))\n",
    "            line = []\n",
    "    if line:\n",
    "        out.append(\", \".join(line))\n",
    "    return \",\\n    \".join(out)\n",
    "\n",
    "def _fmt_array_2d(arr2d):\n",
    "    rows = []\n",
    "    for r in arr2d:\n",
    "        rows.append(\"{ \" + _fmt_int_list(r) + \" }\")\n",
    "    return \"{\\n  \" + \",\\n  \".join(rows) + \"\\n}\"\n",
    "\n",
    "def _fmt_array_3d(arr3d):\n",
    "    blocks = []\n",
    "    for b in arr3d:\n",
    "        blocks.append(_fmt_array_2d(b))\n",
    "    return \"{\\n\" + \",\\n\".join(blocks) + \"\\n}\"\n",
    "\n",
    "# -----------------------\n",
    "# Emitters\n",
    "# -----------------------\n",
    "\n",
    "def _emit_header_open(fp, guard, ns=\"hls4csnn1d_cblk_sd\"):\n",
    "    fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "    fp.write(\"#include <ap_int.h>\\n\\n\")\n",
    "    fp.write(f\"namespace {ns} {{\\n\\n\")\n",
    "\n",
    "def _emit_header_close(fp, guard, ns=\"hls4csnn1d_cblk_sd\"):\n",
    "    fp.write(f\"}} // namespace\\n#endif // {guard}\\n\")\n",
    "\n",
    "def _emit_conv1d_header(path, lname, W_int8, rq_mult, rq_shift,\n",
    "                        bias_int32_vec, input_zp, weight_sum_vec, m):\n",
    "    # Guard: QCSNET2_CBLK1_QCONV1D_WEIGHTS_H style\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_WEIGHTS_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        _emit_header_open(fp, guard)  # writes includes + namespace line\n",
    "\n",
    "        sym   = _sym(lname)\n",
    "        OC, IC, K = W_int8.shape\n",
    "        stride = _as1(m.stride)\n",
    "\n",
    "        # Structural constants (optional but handy for TBs)\n",
    "        fp.write(f\"const int {sym}_OUT_CH = {OC};\\n\")\n",
    "        fp.write(f\"const int {sym}_IN_CH  = {IC};\\n\")\n",
    "        fp.write(f\"const int {sym}_KERNEL_SIZE = {K};\\n\")\n",
    "        fp.write(f\"const int {sym}_STRIDE = {stride};\\n\\n\")\n",
    "\n",
    "        # Input ZP (INT8)\n",
    "        fp.write(f\"const ap_int<8> {sym}_input_zero_point = {int(input_zp)};\\n\\n\")\n",
    "\n",
    "        # Requantization arrays (duplicated per OUT_CH to match your API)\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_mult]*OC))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const int {sym}_right_shift[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_shift]*OC))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Bias (INT32)\n",
    "        fp.write(f\"const acc32_t {sym}_bias[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(bias_int32_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weight sums for asymmetric correction (INT32)\n",
    "        fp.write(f\"const acc32_t {sym}_weight_sum[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(weight_sum_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weights (INT8): [OUT_CH][IN_CH][K]\n",
    "        fp.write(f\"const ap_int<8> {sym}_weights[{OC}][{IC}][{K}] = \")\n",
    "        fp.write(_fmt_array_3d(W_int8))\n",
    "        fp.write(\";\\n\\n\")\n",
    "\n",
    "        _emit_header_close(fp, guard)  # closes namespace + guard\n",
    "\n",
    "\n",
    "def _emit_linear_header(path, lname,\n",
    "                           W_int8,                 # [OUT][IN] int8\n",
    "                           rq_mult, rq_shift,      # per-tensor constants, repeated per OUT\n",
    "                           bias_int32_vec,         # [OUT] int32\n",
    "                           input_zp,               # int (will be placed as ap_int<8>)\n",
    "                           weight_sum_vec):        # [OUT] int32\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_WEIGHTS_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "\n",
    "        sym = _sym(lname)\n",
    "        OUT, IN = W_int8.shape\n",
    "\n",
    "        # Structural (optional helpers)\n",
    "        fp.write(f\"const int {sym}_OUTPUT_SIZE = {OUT};\\n\")\n",
    "        fp.write(f\"const int {sym}_INPUT_SIZE  = {IN};\\n\\n\")\n",
    "\n",
    "        # Input zero-point (matches template type)\n",
    "        fp.write(f\"const ap_int<8> {sym}_input_zero_point = {int(input_zp)};\\n\\n\")\n",
    "\n",
    "        # Requant arrays (match template types; repeat the per-tensor constants)\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_mult] * OUT))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const int {sym}_right_shift[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_shift] * OUT))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Bias and weight_sum (acc domain)\n",
    "        fp.write(f\"using acc32_t = ap_int<32>;\\n\")\n",
    "        fp.write(f\"const acc32_t {sym}_bias[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(bias_int32_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const acc32_t {sym}_weight_sum[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(weight_sum_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weights (use ap_int8_c to match your template)\n",
    "        fp.write(f\"const ap_int8_c {sym}_weights[{OUT}][{IN}] = \")\n",
    "        fp.write(_fmt_array_2d(W_int8))\n",
    "        fp.write(\";\\n\\n\")\n",
    "\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def _emit_bn_header(path, lname, w_q, b32, mult_arr, shift_arr):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_BN_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        sym = _sym(lname); C = len(w_q)\n",
    "\n",
    "        fp.write(f\"const int {sym}_C = {C};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const ap_int8_c {sym}_weight[{C}] = {{\\n  {_fmt_int_list(w_q)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<32> {sym}_bias[{C}] = {{\\n  {_fmt_int_list(b32)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{C}] = {{\\n  {_fmt_int_list(mult_arr)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const int {sym}_right_shift[{C}] = {{\\n  {_fmt_int_list(shift_arr)}\\n}};\\n\\n\")\n",
    "\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def _emit_lif_header_scalar_sd(path, lname, beta_q, theta_q, scale_q, frac_bits):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_LIF_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        sym = _sym(lname)\n",
    "        fp.write(f\"enum {{ {sym}_FRAC_BITS = {int(frac_bits)} }};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_beta_int   = {int(beta_q)};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_theta_int  = {int(theta_q)};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_scale_int  = {int(scale_q)};\\n\\n\")\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "def _emit_lif_header_vector_sd(path, lname, beta_arr_q, theta_arr_q, scale_q, frac_bits):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_LIF_H\")\n",
    "    sym   = _sym(lname)\n",
    "    N     = len(beta_arr_q)\n",
    "    if len(theta_arr_q) != N:\n",
    "        raise ValueError(\"beta/theta array lengths must match\")\n",
    "\n",
    "    def _fmt_list(vals, per_line=16):\n",
    "        rows = []\n",
    "        for i in range(0, len(vals), per_line):\n",
    "            chunk = \", \".join(str(int(v)) for v in vals[i:i+per_line])\n",
    "            rows.append(\"    \" + chunk)\n",
    "        return \"{\\n\" + \",\\n\".join(rows) + \"\\n}\"\n",
    "\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        fp.write(f\"enum {{ {sym}_FRAC_BITS = {int(frac_bits)}, {sym}_OUT_CH = {int(N)} }};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_scale_int = {int(scale_q)};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_beta_int[{sym}_OUT_CH] = \"  + _fmt_list(beta_arr_q)  + \";\\n\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_theta_int[{sym}_OUT_CH] = \" + _fmt_list(theta_arr_q) + \";\\n\\n\")\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "INT16_MIN, INT16_MAX = -32768, 32767\n",
    "\n",
    "def _to_q_i16(x: float, Q: int) -> int:\n",
    "    return int(np.clip(round(float(x) * Q), INT16_MIN, INT16_MAX))\n",
    "\n",
    "def _tensor_to_q_i16_list(t: torch.Tensor, Q: int):\n",
    "    flat = t.detach().float().reshape(-1).cpu().tolist()\n",
    "    return [_to_q_i16(v, Q) for v in flat]\n",
    "\n",
    "\n",
    "_Q_SCALE = 1 << 12   # e.g., FRAC_BITS = 12\n",
    "\n",
    "def _emit_qparams_header(path, lname, bit_w, scale, zp):\n",
    "    guard = _id_guard_macro(f\"QPARAMS_{_sym(lname)}_H\")\n",
    "    sym = _sym(lname)\n",
    "    sym_base = sym  # no renaming, no suffix stripping\n",
    "\n",
    "    # Q-encode the activation scale for HLS QuantIdentity (uses _Q_SCALE; not emitted)\n",
    "    act_scale_int = _to_q_i16(float(scale), _Q_SCALE)\n",
    "\n",
    "    with open(path, \"w\") as fp:\n",
    "        _emit_header_open(fp, guard)  # must include <ap_int.h> and open your namespace\n",
    "\n",
    "        fp.write(\"// Activation quantization parameters (optional for kernels)\\n\")\n",
    "        fp.write(f\"const int   {sym}_bit_width = {int(bit_w)};\\n\")\n",
    "        fp.write(f\"// const float {sym}_scale     = {float(scale):.10g};  // kept for reference only\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym_base}_act_scale_int = {int(act_scale_int)};\\n\")\n",
    "        fp.write(f\"const int   {sym}_zero_point= {int(zp)};\\n\\n\")\n",
    "\n",
    "        _emit_header_close(fp, guard)  # close namespace and guard\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Orchestrator\n",
    "# -----------------------\n",
    "\n",
    "def emit_headers_for_model(model: torch.nn.Module,\n",
    "                           example_input: torch.Tensor,\n",
    "                           out_dir: str = \"headers_int\",\n",
    "                           lif_frac_bits: int = 12):\n",
    "    model.eval()\n",
    "    _ensure_dir(out_dir)\n",
    "\n",
    "    # Run one dry forward to initialize any lazy buffers (ignore output tuples)\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            _ = model(example_input)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Track current activation qparams (propagated as in your graph)\n",
    "    current_act = {\"bit_width\": 8, \"scale\": 1.0, \"zero_point\": 0}\n",
    "    \n",
    "    last_out_ch = None  # initialize outside the loop\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        if m is model:\n",
    "            continue\n",
    "\n",
    "        # QuantIdentity (export activation qparams as optional header)\n",
    "        if isinstance(m, qnn.QuantIdentity):\n",
    "            aq = getattr(m, 'act_quant', getattr(m, 'output_quant', None))\n",
    "            bit_w, s, zp, _ = _get_bit_scale_zp_from_quant(aq)\n",
    "            _emit_qparams_header(os.path.join(out_dir, f\"qparams_{name}.h\"), name, bit_w, s, zp)\n",
    "            current_act = {\"bit_width\": bit_w, \"scale\": s, \"zero_point\": zp}\n",
    "            continue\n",
    "\n",
    "\n",
    "        if isinstance(m, qnn.QuantConv1d):\n",
    "            # Float → INT8 weights\n",
    "            Wf = m.weight.detach().cpu().numpy()            # [OUT, IN, K]\n",
    "            wq = getattr(m, 'weight_quant', None)\n",
    "            wb, s_w, z_w, _ = _get_bit_scale_zp_from_quant(wq)\n",
    "        \n",
    "            _guard_in_scale(name, current_act['scale'])\n",
    "            _guard_weight_scale(name, s_w)\n",
    "        \n",
    "            W_int8 = _qt_weight_int8_per_tensor(Wf, s_w, z_w)\n",
    "        \n",
    "            # Output activation qparams (for requant)\n",
    "            oq = getattr(m, 'output_quant', None)\n",
    "            ob, s_out, z_out, _ = _get_bit_scale_zp_from_quant(oq)\n",
    "            _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Integer requant constants (per-tensor → repeat per OUT_CH)\n",
    "            M = (current_act['scale'] * s_w) / s_out\n",
    "            rq_mult, rq_shift = _quantize_multiplier(M)\n",
    "        \n",
    "            # Bias (if present) → INT32 vector (length OUT_CH); else zeros\n",
    "            b_f = m.bias.detach().cpu().numpy() if (hasattr(m, 'bias') and m.bias is not None) else None\n",
    "            bias_int32_vec = _bias_int32_vector(b_f, current_act['scale'], s_w, W_int8.shape[0])\n",
    "        \n",
    "            # Weight sums per output channel (for asymmetric correction)\n",
    "            weight_sum_vec = _sum_weights_per_out(W_int8)\n",
    "        \n",
    "            # Input zero point (INT8) for asymmetric correction\n",
    "            input_zp = current_act['zero_point']\n",
    "        \n",
    "            # Emit header that matches your Conv1D_SD::forward signature\n",
    "            _emit_conv1d_header(\n",
    "                os.path.join(out_dir, f\"{name}_weights.h\"),\n",
    "                name,\n",
    "                W_int8,\n",
    "                rq_mult, rq_shift,\n",
    "                bias_int32_vec,\n",
    "                input_zp,\n",
    "                weight_sum_vec,\n",
    "                m\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams for the next layer\n",
    "            current_act = {\"bit_width\": ob, \"scale\": s_out, \"zero_point\": z_out}\n",
    "            continue\n",
    "\n",
    "\n",
    "        # BN -> ScaleBias (for BatchNorm1dToQuantScaleBias)\n",
    "        if hasattr(qnn, 'BatchNorm1dToQuantScaleBias') and isinstance(m, qnn.BatchNorm1dToQuantScaleBias):\n",
    "            # gamma, beta (float, per channel)\n",
    "            gamma = _to_one(getattr(m, 'weight', None)) if getattr(m, 'weight', None) is not None else _to_one(getattr(m, 'scale', None))\n",
    "            beta  = _to_one(getattr(m, 'bias',   None)) if getattr(m, 'bias',   None) is not None else _to_one(getattr(m, 'beta',  None))\n",
    "            if gamma is None: gamma = 1.0\n",
    "            if beta  is None: beta  = 0.0\n",
    "            gamma = np.array(gamma, dtype=np.float32).reshape(-1)\n",
    "            beta  = np.array(beta,  dtype=np.float32).reshape(-1)\n",
    "            C = gamma.shape[0]\n",
    "        \n",
    "            # Input/output quant\n",
    "            s_in = float(current_act['scale']);  z_in = int(current_act['zero_point']);  _guard_in_scale(name, s_in)\n",
    "            oq   = getattr(m, 'output_quant', None)\n",
    "            _, s_out, _, _ = _get_bit_scale_zp_from_quant(oq);  _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Brevitas BN quantized weight\n",
    "            qW = m.quant_weight()  # IntQuantTensor\n",
    "            w_q = qW.int().detach().cpu().numpy().astype(np.int8)    # [C]\n",
    "            s_w = float(_to_one(qW.scale))                            # scalar\n",
    "        \n",
    "            # Shared requant scale S and its integer pair\n",
    "            S = s_w * (s_in / s_out)\n",
    "            mult_S, shift_S = _quantize_multiplier(S)\n",
    "        \n",
    "            # Int32 bias per channel (no int8 clipping)\n",
    "            # M_c = (s_in/s_out) * gamma_c  = S * w_q[c]  (approximately)\n",
    "            M = gamma * (s_in / s_out)                          # [C]\n",
    "            b32 = np.round((beta / s_out - M * z_in) / S).astype(np.int32)  # [C]\n",
    "        \n",
    "            _emit_bn_header(\n",
    "                os.path.join(out_dir, f\"{name}_bn.h\"),\n",
    "                name,\n",
    "                w_q.tolist(),\n",
    "                b32.tolist(),\n",
    "                [mult_S] * C,\n",
    "                [shift_S] * C\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams\n",
    "            current_act = {\"bit_width\": current_act['bit_width'], \"scale\": s_out, \"zero_point\": 0}\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # QuantLinear\n",
    "        if isinstance(m, qnn.QuantLinear):\n",
    "            # Float → INT8 weights\n",
    "            Wf = m.weight.detach().cpu().numpy()             # [OUT, IN]\n",
    "            wq = getattr(m, 'weight_quant', None)\n",
    "            wb, s_w, z_w, _ = _get_bit_scale_zp_from_quant(wq)\n",
    "        \n",
    "            _guard_in_scale(name, current_act['scale'])\n",
    "            _guard_weight_scale(name, s_w)\n",
    "        \n",
    "            W_int8 = _qt_weight_int8_per_tensor(Wf, s_w, z_w)\n",
    "        \n",
    "            # Output activation qparams (for requant)\n",
    "            oq = getattr(m, 'output_quant', None)\n",
    "            ob, s_out, z_out, _ = _get_bit_scale_zp_from_quant(oq)\n",
    "            _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Requant: M = (s_in * s_w) / s_out\n",
    "            M = (current_act['scale'] * s_w) / s_out\n",
    "            rq_mult, rq_shift = _quantize_multiplier(M)\n",
    "        \n",
    "            # Bias (if present) → INT32; else zeros\n",
    "            b_f = m.bias.detach().cpu().numpy() if (hasattr(m, 'bias') and m.bias is not None) else None\n",
    "            bias_int32_vec = _bias_int32_vector(b_f, current_act['scale'], s_w, W_int8.shape[0])\n",
    "        \n",
    "            # Weight sums for asymmetric correction: sum over input dim\n",
    "            weight_sum_vec = W_int8.sum(axis=1).astype(np.int32)\n",
    "        \n",
    "            # Input zero-point for asymmetric correction\n",
    "            input_zp = current_act['zero_point']\n",
    "        \n",
    "            # Emit header that matches Linear1D_SD::forward\n",
    "            _emit_linear_header(\n",
    "                os.path.join(out_dir, f\"{name}_weights.h\"),\n",
    "                name,\n",
    "                W_int8,\n",
    "                rq_mult, rq_shift,\n",
    "                bias_int32_vec,\n",
    "                input_zp,\n",
    "                weight_sum_vec\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams\n",
    "            current_act = {\"bit_width\": ob, \"scale\": s_out, \"zero_point\": z_out}\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if isinstance(m, snn.Leaky):\n",
    "            scale_in = float(current_act['scale'])\n",
    "            Q = 1 << lif_frac_bits\n",
    "        \n",
    "            # Grab beta/threshold as tensors (handles both scalar and vector)\n",
    "            beta_t  = m.beta if isinstance(m.beta, torch.Tensor) else torch.as_tensor(m.beta)\n",
    "            thr_t   = m.threshold if isinstance(m.threshold, torch.Tensor) else torch.as_tensor(m.threshold)\n",
    "        \n",
    "            beta_t  = beta_t.detach().float()\n",
    "            print(\"beta: \", beta_t)\n",
    "            thr_t   = thr_t.detach().float()\n",
    "        \n",
    "            # Number of “neurons” (channels) from parameter size\n",
    "            n_beta  = int(beta_t.numel())\n",
    "            n_thr   = int(thr_t.numel())\n",
    "            if n_beta != n_thr and n_beta != 1 and n_thr != 1:\n",
    "                raise ValueError(f\"LIF param size mismatch: beta has {n_beta}, threshold has {n_thr}\")\n",
    "        \n",
    "            # Broadcast if one is scalar\n",
    "            out_ch = max(n_beta, n_thr)\n",
    "            if n_beta == 1 and out_ch > 1:\n",
    "                beta_t = beta_t.expand(out_ch)\n",
    "            if n_thr == 1 and out_ch > 1:\n",
    "                thr_t = thr_t.expand(out_ch)\n",
    "        \n",
    "            # Quantize\n",
    "            beta_arr_q  = _tensor_to_q_i16_list(beta_t, Q)\n",
    "            theta_arr_q = _tensor_to_q_i16_list(thr_t, Q)\n",
    "            scale_q     = _to_q_i16(scale_in, Q)   # keep scale scalar for now\n",
    "        \n",
    "            # Emit vector or scalar header depending on out_ch\n",
    "            out_path = os.path.join(out_dir, f\"{name}_lif.h\")\n",
    "            if out_ch > 1:\n",
    "                _emit_lif_header_vector_sd(\n",
    "                    out_path, name, beta_arr_q, theta_arr_q, scale_q, lif_frac_bits\n",
    "                )\n",
    "            else:\n",
    "                _emit_lif_header_scalar_sd(\n",
    "                    out_path, name, beta_arr_q[0], theta_arr_q[0], scale_q, lif_frac_bits\n",
    "                )\n",
    "        \n",
    "            # LIF outputs binary spikes {0,1} → treat next op input scale as 1.0\n",
    "            current_act = {\"bit_width\": 8, \"scale\": 1.0, \"zero_point\": 0}\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"[emit] C++ headers written to: {os.path.abspath(out_dir)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c136c7-1dd4-46f3-985d-4ad6344dbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emit_headers_for_model(qcsnet2_eval, torch.randn(1,1,180), out_dir=\"weights_sd/headers_int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccebaf99-dd35-4527-8e6d-4c64af3888ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from Fold 5\n",
    "model_stage1 = create_qcsnn_model()\n",
    "model_stage1.load_state_dict(fold_ckpts['fold5']['state_dict_cpu'])\n",
    "model_stage1.eval()\n",
    "\n",
    "# Create example input\n",
    "example_input = torch.randn(1, 1, 180)\n",
    "\n",
    "# Extract to C++ headers\n",
    "emit_headers_for_model(\n",
    "    model=model_stage1,\n",
    "    example_input=example_input,\n",
    "    out_dir=\"weights_sd/intra_patient/headers_stage1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff2c21-896a-45a1-b6fb-e2f11765f180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb656a3d-92f4-4ac3-ad68-2d2fcd86fad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91234745-fe31-4f3f-a7cd-23dd6beb9a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153ec2c-48b2-4bae-8ec4-6a0de83c62ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74b699-ebc8-4266-b545-88ccbf9fc566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033e5c4-fe10-4d69-8e8b-b9c6f354ab84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9b663-dd75-43c8-a5de-fe304b6bac1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c0600-325a-4097-8317-a9a3a1f156f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
