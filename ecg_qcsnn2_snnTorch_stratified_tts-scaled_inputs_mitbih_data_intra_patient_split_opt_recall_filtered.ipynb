{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2415796d-09d6-4732-80d6-b592d076cc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/velox-217533/anaconda3/envs/fau_snn_torch-cuda/lib/python3.12/site-packages/brevitas/graph/equalize.py:69: UserWarning: fast_hadamard_transform package not found, using standard pytorch kernels\n",
      "  warnings.warn(\"fast_hadamard_transform package not found, using standard pytorch kernels\")\n",
      "/home/velox-217533/anaconda3/envs/fau_snn_torch-cuda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen, utils, surrogate\n",
    "from snntorch.functional import quant\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "# from brevitas.nn import QuantConv1d, QuantIdentity, QuantLinear, BatchNorm1dToQuantScaleBias\n",
    "from brevitas.quant import Int8WeightPerTensorFloat, Int8ActPerTensorFloat, Int32Bias\n",
    "from brevitas.export import export_qonnx\n",
    "from brevitas.core.scaling      import ScalingImplType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.quant        import QuantType\n",
    "\n",
    "\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "#from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv, copy, glob\n",
    "import imblearn, imblearn.over_sampling\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize, normalize\n",
    "from scipy.signal import butter, lfilter, freqz\n",
    "\n",
    "import os, sys, time, datetime, argparse, json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8f3951-b67e-455c-87c6-3fe3354ab33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # for multi-GPU setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018f70e-05a4-45b2-a8c4-b80d1a89cc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e300c1f1-eb95-44ad-9447-7be1b8a18ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_data(\n",
    "    mitbih_train_path='/..../data/mitbih_processed_intra_patient_4class_180_center90_filtered/train',\n",
    "    folders=None, random_state=42,\n",
    "    max_files_per_folder=None\n",
    "):\n",
    "    if folders is None:\n",
    "        folders = ['normal', 'sveb', 'veb', 'f']\n",
    "    label_mapping = {'normal': 0, 'sveb': 1, 'veb': 1, 'f': 1}\n",
    "    \n",
    "    def scan_folder(base_path):\n",
    "        X_parts, y_parts = [], []\n",
    "        expected_cols = None\n",
    "    \n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            csvs = sorted(\n",
    "                glob.glob(os.path.join(folder_path, '*.csv')) +\n",
    "                glob.glob(os.path.join(folder_path, '*.CSV'))\n",
    "            )\n",
    "            if not csvs:\n",
    "                print(f'Warning: no CSVs found in: {folder_path}')\n",
    "                continue\n",
    "    \n",
    "            for fpath in csvs:\n",
    "                df = pd.read_csv(\n",
    "                    fpath, dtype=np.float32, engine='c',\n",
    "                    usecols=lambda c: c != 'Unnamed: 0'\n",
    "                )\n",
    "                if df.shape[0] == 0:\n",
    "                    df = pd.read_csv(fpath, header=None, dtype=np.float32, engine='c')\n",
    "    \n",
    "                df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "                arr = df.to_numpy(copy=False)\n",
    "                if expected_cols is None:\n",
    "                    expected_cols = arr.shape[1]\n",
    "                elif arr.shape[1] != expected_cols:\n",
    "                    raise ValueError(\n",
    "                        f'Inconsistent column count: {fpath} has {arr.shape[1]}, expected {expected_cols}'\n",
    "                    )\n",
    "    \n",
    "                X_parts.append(arr)\n",
    "                y_parts.extend([label_mapping[folder]] * arr.shape[0])\n",
    "    \n",
    "        if not X_parts:\n",
    "            raise FileNotFoundError(f'No usable CSV rows under {base_path} (folders={folders})')\n",
    "    \n",
    "        X = np.vstack(X_parts).astype(np.float32, copy=False)\n",
    "        y = np.asarray(y_parts, dtype=np.int64)\n",
    "        return X, y\n",
    "    \n",
    "    # Load and shuffle\n",
    "    X_mit, y_mit = scan_folder(mitbih_train_path)\n",
    "    print(f'Loaded: MIT-BIH {X_mit.shape}')\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    perm = rng.permutation(len(y_mit))\n",
    "    X = X_mit[perm]\n",
    "    y = y_mit[perm]\n",
    "    \n",
    "    # OPTION 3: NO SCALING - data already z-score normalized from preprocessing\n",
    "    # Data will have mean≈0, std≈1, range≈[-3, 3]\n",
    "    print(f\"Data range after z-score normalization: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "    print(f\"Data mean: {X.mean():.4f}, std: {X.std():.4f}\")\n",
    "    \n",
    "    # Handle any NaNs just in case\n",
    "    X = np.nan_to_num(X, nan=0.0).astype(np.float32, copy=False)\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    X_train_tensor = torch.from_numpy(X).unsqueeze(1)   # (N, 1, L)\n",
    "    y_train = torch.from_numpy(y)                       # (N,)\n",
    "    \n",
    "    print(\"Class distribution:\", Counter(y_train.tolist()))\n",
    "    print(\"X train shape:\", tuple(X_train_tensor.shape))\n",
    "    print(\"y_train shape:\", tuple(y_train.shape))\n",
    "    print(\"Data tensor loaded successfully\")\n",
    "    \n",
    "    return X_train_tensor, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154e7548-ff76-4f5d-bfa0-c4ac03c31ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csnn_datasets(X_train, y_train, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Create TensorDataset objects for SNN training.\n",
    "\n",
    "    Args:\n",
    "        X_train_spikes (torch.Tensor): Spike-encoded training data\n",
    "        y_train (torch.Tensor): Training labels\n",
    "        X_test_spikes (torch.Tensor): Spike-encoded testing data\n",
    "        y_test (torch.Tensor): Testing labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset)\n",
    "    \"\"\"\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    if X_test is None:\n",
    "        print(\"dataset created successfully\")\n",
    "        return train_dataset\n",
    "    else:\n",
    "        test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcd749-b60f-49de-a8bf-d2aa8d7a3819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e99a5c7-92f9-48b1-9911-eef93fb9a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qcsnn_model(num_bits=8, input_size=180, stride2=1, kernel_size=3, \n",
    "                       dropout2=0.35, beta2=0.5, slope2=25, \n",
    "                       threshold2=0.5, learn_beta2=True, learn_threshold2=True):\n",
    "    \"\"\"Factory function to create a fresh QCSNN instance\"\"\"\n",
    "    \n",
    "    spike_grad2 = snn.surrogate.fast_sigmoid(slope=slope2)\n",
    "    \n",
    "    # Calculate output sizes\n",
    "    output_size1 = (input_size - kernel_size) // stride2 + 1\n",
    "    output_size1 = output_size1 // 2\n",
    "    output_size2 = (output_size1 - kernel_size) // stride2 + 1\n",
    "    output_size2 = output_size2 // 2\n",
    "    flattened_size = output_size2 * 24\n",
    "    \n",
    "    model = torch.nn.Sequential(OrderedDict([\n",
    "        # ── Input quantiser ──────────────────────────────\n",
    "        (\"qcsnet2_cblk1_input\",\n",
    "         qnn.QuantIdentity(bit_width=num_bits, return_quant_tensor=True)),\n",
    "\n",
    "        # ── First Conv block ─────────────────────────────\n",
    "        (\"qcsnet2_cblk1_qconv1d\",\n",
    "         qnn.QuantConv1d(\n",
    "             1, 16, 3, stride=stride2, bias=False,\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             return_quant_tensor=True)),\n",
    "\n",
    "        (\"qcsnet2_cblk1_batch_norm\",\n",
    "         qnn.BatchNorm1dToQuantScaleBias(\n",
    "             16,\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             bias_quant=Int32Bias,\n",
    "             return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet2_cblk1_leaky\",\n",
    "         snn.Leaky(beta=beta2, learn_beta=learn_beta2,\n",
    "                   spike_grad=spike_grad2,\n",
    "                   threshold=threshold2, learn_threshold=learn_threshold2,\n",
    "                   init_hidden=True)),\n",
    "        \n",
    "        (\"qcsnet2_cblk1_max_pool\", torch.nn.MaxPool1d(2, 2)),\n",
    "\n",
    "        # ── Second Conv block ────────────────────────────\n",
    "        (\"qcsnet2_cblk2_input\",\n",
    "         qnn.QuantIdentity(bit_width=num_bits, return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet2_cblk2_qconv1d\",\n",
    "         qnn.QuantConv1d(\n",
    "             16, 24, 3, stride=stride2, bias=False,\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             return_quant_tensor=True)),\n",
    "\n",
    "        (\"qcsnet2_cblk2_batch_norm\",\n",
    "         qnn.BatchNorm1dToQuantScaleBias(\n",
    "             24,\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             bias_quant=Int32Bias,\n",
    "             return_quant_tensor=True)),\n",
    "\n",
    "        (\"qcsnet2_cblk2_drp\", torch.nn.Dropout(dropout2)),\n",
    "        \n",
    "        (\"qcsnet2_cblk2_leaky\",\n",
    "         snn.Leaky(beta=beta2, learn_beta=learn_beta2,\n",
    "                   spike_grad=spike_grad2,\n",
    "                   threshold=threshold2, learn_threshold=learn_threshold2,\n",
    "                   init_hidden=True)),\n",
    "        \n",
    "        (\"qcsnet2_cblk2_max_pool\", torch.nn.MaxPool1d(2, 2)),\n",
    "\n",
    "        # ── Dense head ──────────────────────────────────\n",
    "        (\"qcsnet2_flatten\", torch.nn.Flatten()),\n",
    "        \n",
    "        (\"qcsnet2_lblk1_input\",\n",
    "         qnn.QuantIdentity(bit_width=num_bits, return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet2_lblk1_qlinear\",\n",
    "         qnn.QuantLinear(\n",
    "             flattened_size, 2, bias=False,\n",
    "             weight_bit_width=num_bits,\n",
    "             weight_quant=Int8WeightPerTensorFloat,\n",
    "             output_quant=Int8ActPerTensorFloat,\n",
    "             return_quant_tensor=True)),\n",
    "        \n",
    "        (\"qcsnet2_lblk1_leaky\",\n",
    "         snn.Leaky(beta=beta2, learn_beta=learn_beta2,\n",
    "                   spike_grad=spike_grad2,\n",
    "                   threshold=threshold2, learn_threshold=learn_threshold2,\n",
    "                   init_hidden=True, output=True)),\n",
    "    ]))\n",
    "    \n",
    "    # CRITICAL FIX: Manually override runtime_shape\n",
    "    model.qcsnet2_cblk1_batch_norm.runtime_shape = (1, -1, 1)\n",
    "    model.qcsnet2_cblk2_batch_norm.runtime_shape = (1, -1, 1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass(model, num_steps, data):\n",
    "    \"\"\"Run SNN for num_steps and collect spike/membrane recordings\"\"\"\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "    utils.reset(model)  # resets hidden states for all LIF neurons in net\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        spk_out, mem_out = model(data)\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "    \n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "663adae8-5f9f-485b-8f0e-f86fb8d9eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csnet2.qcsnet2_cblk1_batch_norm.quant_weight().int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "289b7438-186d-46ef-a5fd-d464a3185c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csnet2.qcsnet2_cblk1_qconv1d.quant_weight().int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c62f4-d598-4b9e-bdf8-22c006431ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb764e-c818-4539-aa47-2c6eadcf235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdf029e-012b-483c-a693-ef4cddb9b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# train_epoch (your version; only enforces .long())\n",
    "# -------------------------\n",
    "def train_epoch(model2, dataloader, loss_func, optimizer2, device, num_steps):\n",
    "    train2_loss, train2_correct = 0.0, 0\n",
    "    model2.train()\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer2.zero_grad()\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets2 = targets.to(device, non_blocking=True).long()\n",
    "\n",
    "        out2, _ = forward_pass(model2, num_steps, inputs)  # (T,B,2)\n",
    "        out2 = out2.mean(0)                                # (B,2)\n",
    "\n",
    "        loss2 = loss_func(out2, targets2)\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        bs = targets2.size(0)\n",
    "        train2_loss += loss2.item() * bs\n",
    "        train2_correct += (out2.argmax(1) == targets2).sum().item()\n",
    "\n",
    "    return train2_loss, train2_correct\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2c2e27-1807-458f-ac1e-8b554216bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# validation_epoch (your version; only enforces .long())\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def validation_epoch(model2, dataloader, loss_func, device, num_steps):\n",
    "    model2.eval()\n",
    "    metrics2 = {\n",
    "        'loss': 0., 'acc': 0.,\n",
    "        'precision': [0., 0.], 'recall': [0., 0.],\n",
    "        'specificity': [0., 0.], 'f1-score': [0., 0.],\n",
    "    }\n",
    "\n",
    "    valid2_loss = 0.0\n",
    "    valid2_correct = 0\n",
    "    tp2 = [0, 0]; fp2 = [0, 0]; tn2 = [0, 0]; fn2 = [0, 0]\n",
    "    n_seen = 0\n",
    "\n",
    "    for x, y_bin in dataloader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y_bin = y_bin.to(device, non_blocking=True).long()\n",
    "\n",
    "        out2, _ = forward_pass(model2, num_steps, x)  # (T,B,2)\n",
    "        out2 = out2.mean(0)                           # (B,2)\n",
    "\n",
    "        loss2 = loss_func(out2, y_bin)\n",
    "        bs = y_bin.size(0)\n",
    "        valid2_loss += loss2.item() * bs\n",
    "        valid2_correct += (out2.argmax(1) == y_bin).sum().item()\n",
    "        n_seen += bs\n",
    "\n",
    "        pred2 = out2.argmax(1)\n",
    "        for i in (0, 1):\n",
    "            tp2[i] += ((pred2 == i) & (y_bin == i)).sum().item()\n",
    "            fp2[i] += ((pred2 == i) & (y_bin != i)).sum().item()\n",
    "            tn2[i] += ((pred2 != i) & (y_bin != i)).sum().item()\n",
    "            fn2[i] += ((pred2 != i) & (y_bin == i)).sum().item()\n",
    "\n",
    "    metrics2['loss'] = valid2_loss / max(n_seen, 1)\n",
    "    metrics2['acc']  = 100.0 * valid2_correct / max(n_seen, 1)\n",
    "\n",
    "    def per_class_metrics(tp, fp, tn, fn):\n",
    "        EPS = 1e-8\n",
    "        prec = [t / (t + f + EPS) for t, f in zip(tp, fp)]\n",
    "        rec  = [t / (t + f + EPS) for t, f in zip(tp, fn)]\n",
    "        spec = [t / (t + f + EPS) for t, f in zip(tn, fp)]\n",
    "        f1   = [2*p*r / (p + r + EPS) for p, r in zip(prec, rec)]\n",
    "        macro = {\n",
    "            'precision_macro': sum(prec)/2,\n",
    "            'recall_macro': sum(rec)/2,\n",
    "            'specificity_macro': sum(spec)/2,\n",
    "            'f1_macro': sum(f1)/2,\n",
    "        }\n",
    "        return prec, rec, spec, f1, macro\n",
    "\n",
    "    prec2, rec2, spec2, f12, macro2 = per_class_metrics(tp2, fp2, tn2, fn2)\n",
    "    metrics2['precision'] = prec2\n",
    "    metrics2['recall'] = rec2\n",
    "    metrics2['specificity'] = spec2\n",
    "    metrics2['f1-score'] = f12\n",
    "    metrics2.update(macro2)\n",
    "    return metrics2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef763d7-586b-449b-91ed-c3760fb67541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b0159c0-b166-4197-ad29-5ee60993673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def _clear_cuda_cache():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# NOTE: keep your existing Brevitas warmup if you already have one.\n",
    "# This stub assumes forward_pass(model, num_steps, x) exists.\n",
    "@torch.no_grad()\n",
    "def _brevitas_warmup(model, dataset, device, num_steps=10, bs=8, n_steps=10):\n",
    "    loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=0,\n",
    "                        pin_memory=(device.type == \"cuda\"))\n",
    "    it = iter(loader)\n",
    "    for _ in range(n_steps):\n",
    "        try:\n",
    "            x, _ = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader)\n",
    "            x, _ = next(it)\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        out, _ = forward_pass(model, num_steps, x)  # (T,B,2) expected\n",
    "        _ = out.mean(0)  # just force execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cfff0d8-5003-48c1-a60e-ff990b783c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cv(model_factory, epochs, dataset, device,\n",
    "                   loss_func, optimizer_class, optimizer_kwargs,\n",
    "                   num_steps=10, k_folds=6, batch_size=128,\n",
    "                   monitor=\"f1_macro\", mode=\"max\"):\n",
    "    \"\"\"\n",
    "    K-fold CV with fresh model per fold.\n",
    "    PATCH: NO SMOTE. Uses per-fold class-weighted CrossEntropyLoss.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    # ---- Brevitas warmup template ----\n",
    "    print(\"Creating template model for warmup...\")\n",
    "    model_template = model_factory().to(device)\n",
    "    model_template.train()\n",
    "\n",
    "    print(\"Running Brevitas warmup...\")\n",
    "    _brevitas_warmup(model_template, dataset, device, num_steps=num_steps, bs=8, n_steps=10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        warmup_state = {k: v.detach().cpu().clone()\n",
    "                        for k, v in model_template.state_dict().items()}\n",
    "\n",
    "    scaling_keys = sum('scaling_impl.value' in k for k in warmup_state.keys())\n",
    "    print(f\"Scaling keys after warmup: {scaling_keys}\")\n",
    "    if scaling_keys == 0:\n",
    "        print(\"WARNING: No scaling_impl.value keys found!\")\n",
    "\n",
    "    del model_template\n",
    "    _clear_cuda_cache()\n",
    "\n",
    "    # ---- Label helpers ----\n",
    "    def all_labels(ds):\n",
    "        if hasattr(ds, 'tensors') and len(ds.tensors) >= 2:\n",
    "            return ds.tensors[1].detach().cpu().long().numpy()\n",
    "        return np.asarray([ds[i][1] for i in range(len(ds))], dtype=np.int64)\n",
    "\n",
    "    def subset_labels(sub: Subset):\n",
    "        base, idx = sub.dataset, sub.indices\n",
    "        if hasattr(base, 'tensors') and len(base.tensors) >= 2:\n",
    "            return base.tensors[1][idx].detach().cpu().long().numpy()\n",
    "        return np.asarray([base[i][1] for i in idx], dtype=np.int64)\n",
    "\n",
    "    # ---- Monitor score helper ----\n",
    "    per_class_map = {\n",
    "        \"recall_abnormal\": (\"recall\", 1),\n",
    "        \"precision_abnormal\": (\"precision\", 1),\n",
    "        \"f1_abnormal\": (\"f1-score\", 1),\n",
    "        \"specificity_abnormal\": (\"specificity\", 1),\n",
    "\n",
    "        \"recall_normal\": (\"recall\", 0),\n",
    "        \"precision_normal\": (\"precision\", 0),\n",
    "        \"f1_normal\": (\"f1-score\", 0),\n",
    "        \"specificity_normal\": (\"specificity\", 0),\n",
    "    }\n",
    "\n",
    "    def extract_score(metrics, monitor_key):\n",
    "        if monitor_key in per_class_map:\n",
    "            k, i = per_class_map[monitor_key]\n",
    "            return metrics[k][i]\n",
    "        return metrics[monitor_key]\n",
    "\n",
    "    # ---- CV setup ----\n",
    "    y_all = all_labels(dataset)\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_history = {}\n",
    "    fold_ckpts = {}\n",
    "\n",
    "    better = (lambda a, b: a > b) if mode == \"max\" else (lambda a, b: a < b)\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(np.arange(len(y_all)), y_all), 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold}/{k_folds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # FRESH model instance per fold\n",
    "        model_fold = model_factory()                 # CPU\n",
    "        model_fold.load_state_dict(warmup_state)     # CPU load\n",
    "        model_fold.to(device)\n",
    "        model_fold.train()\n",
    "\n",
    "        # Fresh optimizer\n",
    "        optimizer_fold = optimizer_class(model_fold.parameters(), **optimizer_kwargs)\n",
    "\n",
    "        # Subsets (NO SMOTE)\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        valid_subset = Subset(dataset, valid_idx)\n",
    "\n",
    "        # Per-fold class weights from training split\n",
    "        y_train = subset_labels(train_subset)\n",
    "        n0 = int((y_train == 0).sum())\n",
    "        n1 = int((y_train == 1).sum())\n",
    "        w0 = 1.0\n",
    "        w1 = n0 / max(n1, 1)\n",
    "\n",
    "        print(f\"Train counts: Normal={n0}, Abnormal={n1} | weights: w0={w0:.3f}, w1={w1:.3f}\")\n",
    "\n",
    "        class_w = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
    "        loss_func_fold = torch.nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "        # Loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=(device.type == 'cuda'),\n",
    "            num_workers=0\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=(device.type == 'cuda'),\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        epoch_history = {\n",
    "            'train2_loss': [], 'train2_acc': [],\n",
    "            'valid2_loss': [], 'valid2_acc': [],\n",
    "            'valid2_prec': [], 'valid2_rec': [], 'valid2_spec': [], 'valid2_f1': [],\n",
    "            'valid2_prec_macro': [], 'valid2_rec_macro': [],\n",
    "            'valid2_spec_macro': [], 'valid2_f1_macro': []\n",
    "        }\n",
    "\n",
    "        best_score = -float(\"inf\") if mode == \"max\" else float(\"inf\")\n",
    "        best_epoch = None\n",
    "        best_state_cpu = None\n",
    "        \n",
    "        patience = 10\n",
    "        no_improve = 0\n",
    "\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            tr2_loss_sum, tr2_corr = train_epoch(model_fold, train_loader, loss_func_fold, optimizer_fold, device, num_steps)\n",
    "\n",
    "            v_metrics2 = validation_epoch(model_fold, valid_loader, loss_func_fold, device, num_steps)\n",
    "\n",
    "            n_train = len(train_subset)\n",
    "            tr2_loss = tr2_loss_sum / max(n_train, 1)\n",
    "            tr2_acc  = 100.0 * tr2_corr / max(n_train, 1)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch:2d}/{epochs} | \"\n",
    "                f\"Train  L={tr2_loss:.4f}  A={tr2_acc:5.2f}% | \"\n",
    "                f\"Valid  L={v_metrics2['loss']:.4f}  A={v_metrics2['acc']:5.2f}% \"\n",
    "                f\"Rec1={v_metrics2['recall'][1]:.4f} \"\n",
    "                f\"Prec1={v_metrics2['precision'][1]:.4f} \"\n",
    "                f\"Rec(M)={v_metrics2['recall_macro']:.4f} \"\n",
    "                f\"F1(M)={v_metrics2['f1_macro']:.4f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "            epoch_history['train2_loss'].append(tr2_loss)\n",
    "            epoch_history['train2_acc'].append(tr2_acc)\n",
    "            epoch_history['valid2_loss'].append(v_metrics2['loss'])\n",
    "            epoch_history['valid2_acc'].append(v_metrics2['acc'])\n",
    "\n",
    "            epoch_history['valid2_prec'].append(v_metrics2['precision'])\n",
    "            epoch_history['valid2_rec'].append(v_metrics2['recall'])\n",
    "            epoch_history['valid2_spec'].append(v_metrics2['specificity'])\n",
    "            epoch_history['valid2_f1'].append(v_metrics2['f1-score'])\n",
    "\n",
    "            epoch_history['valid2_prec_macro'].append(v_metrics2['precision_macro'])\n",
    "            epoch_history['valid2_rec_macro'].append(v_metrics2['recall_macro'])\n",
    "            epoch_history['valid2_spec_macro'].append(v_metrics2['specificity_macro'])\n",
    "            epoch_history['valid2_f1_macro'].append(v_metrics2['f1_macro'])\n",
    "\n",
    "            score = extract_score(v_metrics2, monitor)\n",
    "\n",
    "            if better(score, best_score):\n",
    "                best_score = score\n",
    "                best_epoch = epoch\n",
    "                no_improve = 0\n",
    "                with torch.no_grad():\n",
    "                    best_state_cpu = {k: v.detach().cpu().clone()\n",
    "                                      for k, v in model_fold.state_dict().items()}\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"Early stopping: no improvement in {patience} epochs.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        fold_key = f\"fold{fold}\"\n",
    "        fold_history[fold_key] = epoch_history\n",
    "        fold_ckpts[fold_key] = {\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_score\": float(best_score),\n",
    "            \"monitor\": monitor,\n",
    "            \"state_dict_cpu\": best_state_cpu\n",
    "        }\n",
    "\n",
    "        print(f\"\\n[Fold {fold}] Best {monitor}={best_score:.4f} at epoch {best_epoch}\\n\")\n",
    "\n",
    "        del optimizer_fold, model_fold\n",
    "        del train_loader, valid_loader\n",
    "        _clear_cuda_cache()\n",
    "\n",
    "    return fold_history, fold_ckpts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded7bc0-391d-4b1e-9dd7-d1f4298db76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd1e0742-fa5c-4e95-945e-d2e242234f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: MIT-BIH (80557, 180)\n",
      "Data range after z-score normalization: [-4.87, 6.05]\n",
      "Data mean: 0.0000, std: 1.0000\n",
      "Class distribution: Counter({0: 72073, 1: 8484})\n",
      "X train shape: (80557, 1, 180)\n",
      "y_train shape: (80557,)\n",
      "Data tensor loaded successfully\n",
      "dataset created successfully\n",
      "Using device: cuda\n",
      "Creating template model for warmup...\n",
      "Running Brevitas warmup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/velox-217533/anaconda3/envs/fau_snn_torch-cuda/lib/python3.12/site-packages/torch/_tensor.py:1645: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1939.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling keys after warmup: 8\n",
      "\n",
      "============================================================\n",
      "Fold 1/6\n",
      "============================================================\n",
      "Train counts: Normal=60060, Abnormal=7070 | weights: w0=1.000, w1=8.495\n",
      "Epoch  1/80 | Train  L=0.6614  A=89.30% | Valid  L=0.6322  A=89.13% Rec1=0.7638 Prec1=0.4898 Rec(M)=0.8351 F1(M)=0.7670\n",
      "Epoch  2/80 | Train  L=0.6139  A=90.24% | Valid  L=0.5979  A=89.26% Rec1=0.8218 Prec1=0.4940 Rec(M)=0.8614 F1(M)=0.7773\n",
      "Epoch  3/80 | Train  L=0.5935  A=90.43% | Valid  L=0.5721  A=89.92% Rec1=0.8246 Prec1=0.5132 Rec(M)=0.8663 F1(M)=0.7871\n",
      "Epoch  4/80 | Train  L=0.5494  A=90.60% | Valid  L=0.5425  A=89.86% Rec1=0.8416 Prec1=0.5112 Rec(M)=0.8734 F1(M)=0.7885\n",
      "Epoch  5/80 | Train  L=0.5373  A=90.54% | Valid  L=0.5393  A=90.18% Rec1=0.8444 Prec1=0.5209 Rec(M)=0.8765 F1(M)=0.7937\n",
      "Epoch  6/80 | Train  L=0.5201  A=90.81% | Valid  L=0.4943  A=90.23% Rec1=0.8529 Prec1=0.5221 Rec(M)=0.8805 F1(M)=0.7955\n",
      "Epoch  7/80 | Train  L=0.4687  A=90.84% | Valid  L=0.4352  A=90.20% Rec1=0.8536 Prec1=0.5212 Rec(M)=0.8806 F1(M)=0.7951\n",
      "Epoch  8/80 | Train  L=0.4238  A=90.99% | Valid  L=0.4292  A=91.01% Rec1=0.8479 Prec1=0.5472 Rec(M)=0.8827 F1(M)=0.8066\n",
      "Epoch  9/80 | Train  L=0.4212  A=91.18% | Valid  L=0.4253  A=91.04% Rec1=0.8564 Prec1=0.5477 Rec(M)=0.8866 F1(M)=0.8082\n",
      "Epoch 10/80 | Train  L=0.4176  A=91.31% | Valid  L=0.4239  A=91.63% Rec1=0.8494 Prec1=0.5687 Rec(M)=0.8868 F1(M)=0.8165\n",
      "Epoch 11/80 | Train  L=0.4152  A=91.36% | Valid  L=0.4187  A=91.47% Rec1=0.8614 Prec1=0.5621 Rec(M)=0.8912 F1(M)=0.8155\n",
      "Epoch 12/80 | Train  L=0.4133  A=91.50% | Valid  L=0.4179  A=91.42% Rec1=0.8586 Prec1=0.5605 Rec(M)=0.8897 F1(M)=0.8144\n",
      "Epoch 13/80 | Train  L=0.4115  A=91.67% | Valid  L=0.4150  A=91.93% Rec1=0.8614 Prec1=0.5783 Rec(M)=0.8937 F1(M)=0.8228\n",
      "Epoch 14/80 | Train  L=0.4106  A=91.73% | Valid  L=0.4132  A=92.17% Rec1=0.8635 Prec1=0.5873 Rec(M)=0.8960 F1(M)=0.8271\n",
      "Epoch 15/80 | Train  L=0.4089  A=91.94% | Valid  L=0.4133  A=92.07% Rec1=0.8656 Prec1=0.5831 Rec(M)=0.8964 F1(M)=0.8256\n",
      "Epoch 16/80 | Train  L=0.4074  A=91.97% | Valid  L=0.4119  A=92.06% Rec1=0.8713 Prec1=0.5822 Rec(M)=0.8989 F1(M)=0.8262\n",
      "Epoch 17/80 | Train  L=0.4059  A=92.18% | Valid  L=0.4110  A=92.78% Rec1=0.8685 Prec1=0.6103 Rec(M)=0.9016 F1(M)=0.8377\n",
      "Epoch 18/80 | Train  L=0.4044  A=92.44% | Valid  L=0.4092  A=92.12% Rec1=0.8720 Prec1=0.5844 Rec(M)=0.8995 F1(M)=0.8272\n",
      "Epoch 19/80 | Train  L=0.4035  A=92.54% | Valid  L=0.4093  A=92.40% Rec1=0.8692 Prec1=0.5954 Rec(M)=0.8998 F1(M)=0.8315\n",
      "Epoch 20/80 | Train  L=0.4024  A=92.56% | Valid  L=0.4082  A=92.81% Rec1=0.8727 Prec1=0.6109 Rec(M)=0.9036 F1(M)=0.8387\n",
      "Epoch 21/80 | Train  L=0.4011  A=92.86% | Valid  L=0.4070  A=92.84% Rec1=0.8685 Prec1=0.6131 Rec(M)=0.9020 F1(M)=0.8389\n",
      "Epoch 22/80 | Train  L=0.4002  A=93.04% | Valid  L=0.4056  A=92.40% Rec1=0.8777 Prec1=0.5941 Rec(M)=0.9035 F1(M)=0.8324\n",
      "Epoch 23/80 | Train  L=0.3997  A=93.14% | Valid  L=0.4055  A=91.79% Rec1=0.8755 Prec1=0.5721 Rec(M)=0.8992 F1(M)=0.8223\n",
      "Epoch 24/80 | Train  L=0.3982  A=93.09% | Valid  L=0.4057  A=92.27% Rec1=0.8699 Prec1=0.5902 Rec(M)=0.8994 F1(M)=0.8294\n",
      "Epoch 25/80 | Train  L=0.3976  A=93.08% | Valid  L=0.4047  A=92.76% Rec1=0.8734 Prec1=0.6090 Rec(M)=0.9037 F1(M)=0.8380\n",
      "Epoch 26/80 | Train  L=0.3975  A=93.10% | Valid  L=0.4042  A=92.40% Rec1=0.8784 Prec1=0.5943 Rec(M)=0.9039 F1(M)=0.8326\n",
      "Epoch 27/80 | Train  L=0.3969  A=93.14% | Valid  L=0.4038  A=92.09% Rec1=0.8805 Prec1=0.5823 Rec(M)=0.9031 F1(M)=0.8277\n",
      "Epoch 28/80 | Train  L=0.3966  A=93.06% | Valid  L=0.4052  A=92.96% Rec1=0.8699 Prec1=0.6178 Rec(M)=0.9033 F1(M)=0.8411\n",
      "Epoch 29/80 | Train  L=0.3958  A=93.06% | Valid  L=0.4029  A=92.41% Rec1=0.8798 Prec1=0.5944 Rec(M)=0.9046 F1(M)=0.8329\n",
      "Epoch 30/80 | Train  L=0.3951  A=93.27% | Valid  L=0.4040  A=92.97% Rec1=0.8755 Prec1=0.6171 Rec(M)=0.9058 F1(M)=0.8418\n",
      "Epoch 31/80 | Train  L=0.3950  A=93.39% | Valid  L=0.4027  A=92.94% Rec1=0.8812 Prec1=0.6150 Rec(M)=0.9081 F1(M)=0.8420\n",
      "Epoch 32/80 | Train  L=0.3944  A=93.27% | Valid  L=0.4028  A=92.11% Rec1=0.8826 Prec1=0.5826 Rec(M)=0.9041 F1(M)=0.8282\n",
      "Epoch 33/80 | Train  L=0.3939  A=93.28% | Valid  L=0.4021  A=92.20% Rec1=0.8868 Prec1=0.5857 Rec(M)=0.9065 F1(M)=0.8303\n",
      "Epoch 34/80 | Train  L=0.3938  A=93.37% | Valid  L=0.4007  A=92.61% Rec1=0.8876 Prec1=0.6011 Rec(M)=0.9091 F1(M)=0.8371\n",
      "Epoch 35/80 | Train  L=0.3931  A=93.31% | Valid  L=0.4000  A=92.20% Rec1=0.8897 Prec1=0.5854 Rec(M)=0.9078 F1(M)=0.8306\n",
      "Epoch 36/80 | Train  L=0.3930  A=93.33% | Valid  L=0.4005  A=92.45% Rec1=0.8861 Prec1=0.5950 Rec(M)=0.9076 F1(M)=0.8342\n",
      "Epoch 37/80 | Train  L=0.3929  A=93.53% | Valid  L=0.4022  A=93.44% Rec1=0.8727 Prec1=0.6377 Rec(M)=0.9072 F1(M)=0.8497\n",
      "Epoch 38/80 | Train  L=0.3931  A=93.52% | Valid  L=0.3997  A=92.60% Rec1=0.8876 Prec1=0.6005 Rec(M)=0.9090 F1(M)=0.8369\n",
      "Epoch 39/80 | Train  L=0.3919  A=93.55% | Valid  L=0.3989  A=92.73% Rec1=0.8861 Prec1=0.6059 Rec(M)=0.9091 F1(M)=0.8390\n",
      "Epoch 40/80 | Train  L=0.3919  A=93.59% | Valid  L=0.3990  A=92.69% Rec1=0.8890 Prec1=0.6037 Rec(M)=0.9101 F1(M)=0.8385\n",
      "Epoch 41/80 | Train  L=0.3918  A=93.62% | Valid  L=0.3990  A=92.79% Rec1=0.8861 Prec1=0.6083 Rec(M)=0.9095 F1(M)=0.8400\n",
      "Epoch 42/80 | Train  L=0.3912  A=93.79% | Valid  L=0.3989  A=92.41% Rec1=0.8918 Prec1=0.5929 Rec(M)=0.9099 F1(M)=0.8343\n",
      "Epoch 43/80 | Train  L=0.3914  A=93.61% | Valid  L=0.3991  A=92.62% Rec1=0.8876 Prec1=0.6013 Rec(M)=0.9091 F1(M)=0.8373\n",
      "Epoch 44/80 | Train  L=0.3911  A=93.73% | Valid  L=0.3983  A=92.69% Rec1=0.8904 Prec1=0.6038 Rec(M)=0.9108 F1(M)=0.8388\n",
      "Epoch 45/80 | Train  L=0.3910  A=93.63% | Valid  L=0.4010  A=92.90% Rec1=0.8791 Prec1=0.6138 Rec(M)=0.9070 F1(M)=0.8411\n",
      "Epoch 46/80 | Train  L=0.3904  A=93.75% | Valid  L=0.3983  A=92.84% Rec1=0.8833 Prec1=0.6105 Rec(M)=0.9085 F1(M)=0.8404\n",
      "Epoch 47/80 | Train  L=0.3906  A=93.76% | Valid  L=0.3991  A=92.46% Rec1=0.8826 Prec1=0.5957 Rec(M)=0.9060 F1(M)=0.8340\n",
      "Epoch 48/80 | Train  L=0.3910  A=93.27% | Valid  L=0.3983  A=92.75% Rec1=0.8897 Prec1=0.6063 Rec(M)=0.9108 F1(M)=0.8397\n",
      "Epoch 49/80 | Train  L=0.3902  A=93.42% | Valid  L=0.3983  A=92.79% Rec1=0.8868 Prec1=0.6081 Rec(M)=0.9098 F1(M)=0.8401\n",
      "Epoch 50/80 | Train  L=0.3909  A=93.18% | Valid  L=0.3991  A=91.81% Rec1=0.8890 Prec1=0.5714 Rec(M)=0.9052 F1(M)=0.8241\n",
      "Epoch 51/80 | Train  L=0.3905  A=93.21% | Valid  L=0.3983  A=91.99% Rec1=0.8897 Prec1=0.5779 Rec(M)=0.9066 F1(M)=0.8272\n",
      "Epoch 52/80 | Train  L=0.3899  A=93.33% | Valid  L=0.3996  A=91.47% Rec1=0.8904 Prec1=0.5598 Rec(M)=0.9040 F1(M)=0.8190\n",
      "Early stopping: no improvement in 10 epochs.\n",
      "\n",
      "[Fold 1] Best recall_abnormal=0.8918 at epoch 42\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 2/6\n",
      "============================================================\n",
      "Train counts: Normal=60061, Abnormal=7070 | weights: w0=1.000, w1=8.495\n",
      "Epoch  1/80 | Train  L=0.6612  A=89.27% | Valid  L=0.6304  A=87.77% Rec1=0.8182 Prec1=0.4552 Rec(M)=0.8515 F1(M)=0.7566\n",
      "Epoch  2/80 | Train  L=0.6143  A=89.98% | Valid  L=0.5961  A=89.19% Rec1=0.8345 Prec1=0.4923 Rec(M)=0.8666 F1(M)=0.7781\n",
      "Epoch  3/80 | Train  L=0.5941  A=90.49% | Valid  L=0.5702  A=90.17% Rec1=0.8331 Prec1=0.5208 Rec(M)=0.8714 F1(M)=0.7920\n",
      "Epoch  4/80 | Train  L=0.5502  A=90.66% | Valid  L=0.5407  A=90.47% Rec1=0.8416 Prec1=0.5298 Rec(M)=0.8768 F1(M)=0.7975\n",
      "Epoch  5/80 | Train  L=0.5382  A=90.75% | Valid  L=0.5366  A=90.97% Rec1=0.8416 Prec1=0.5461 Rec(M)=0.8796 F1(M)=0.8051\n",
      "Epoch  6/80 | Train  L=0.5222  A=90.74% | Valid  L=0.4915  A=91.03% Rec1=0.8451 Prec1=0.5482 Rec(M)=0.8816 F1(M)=0.8066\n",
      "Epoch  7/80 | Train  L=0.4706  A=90.69% | Valid  L=0.4283  A=90.93% Rec1=0.8579 Prec1=0.5439 Rec(M)=0.8866 F1(M)=0.8066\n",
      "Epoch  8/80 | Train  L=0.4261  A=90.86% | Valid  L=0.4238  A=91.32% Rec1=0.8628 Prec1=0.5568 Rec(M)=0.8910 F1(M)=0.8134\n",
      "Epoch  9/80 | Train  L=0.4216  A=91.20% | Valid  L=0.4194  A=91.10% Rec1=0.8642 Prec1=0.5492 Rec(M)=0.8904 F1(M)=0.8101\n",
      "Epoch 10/80 | Train  L=0.4182  A=91.30% | Valid  L=0.4168  A=91.05% Rec1=0.8678 Prec1=0.5473 Rec(M)=0.8916 F1(M)=0.8097\n",
      "Epoch 11/80 | Train  L=0.4154  A=91.45% | Valid  L=0.4154  A=92.08% Rec1=0.8685 Prec1=0.5831 Rec(M)=0.8977 F1(M)=0.8261\n",
      "Epoch 12/80 | Train  L=0.4128  A=91.83% | Valid  L=0.4127  A=91.75% Rec1=0.8769 Prec1=0.5704 Rec(M)=0.8996 F1(M)=0.8218\n",
      "Epoch 13/80 | Train  L=0.4107  A=92.08% | Valid  L=0.4124  A=93.33% Rec1=0.8685 Prec1=0.6340 Rec(M)=0.9047 F1(M)=0.8474\n",
      "Epoch 14/80 | Train  L=0.4088  A=92.41% | Valid  L=0.4105  A=93.04% Rec1=0.8713 Prec1=0.6210 Rec(M)=0.9043 F1(M)=0.8427\n",
      "Epoch 15/80 | Train  L=0.4075  A=92.27% | Valid  L=0.4090  A=91.86% Rec1=0.8762 Prec1=0.5744 Rec(M)=0.8999 F1(M)=0.8235\n",
      "Epoch 16/80 | Train  L=0.4057  A=92.18% | Valid  L=0.4079  A=91.70% Rec1=0.8812 Prec1=0.5682 Rec(M)=0.9012 F1(M)=0.8215\n",
      "Epoch 17/80 | Train  L=0.4046  A=92.26% | Valid  L=0.4072  A=91.93% Rec1=0.8791 Prec1=0.5765 Rec(M)=0.9015 F1(M)=0.8249\n",
      "Epoch 18/80 | Train  L=0.4042  A=92.46% | Valid  L=0.4062  A=92.10% Rec1=0.8819 Prec1=0.5827 Rec(M)=0.9038 F1(M)=0.8281\n",
      "Epoch 19/80 | Train  L=0.4028  A=92.67% | Valid  L=0.4056  A=92.25% Rec1=0.8833 Prec1=0.5880 Rec(M)=0.9052 F1(M)=0.8307\n",
      "Epoch 20/80 | Train  L=0.4014  A=92.83% | Valid  L=0.4055  A=92.56% Rec1=0.8819 Prec1=0.5998 Rec(M)=0.9063 F1(M)=0.8356\n",
      "Epoch 21/80 | Train  L=0.4010  A=93.02% | Valid  L=0.4050  A=93.03% Rec1=0.8826 Prec1=0.6184 Rec(M)=0.9092 F1(M)=0.8437\n",
      "Epoch 22/80 | Train  L=0.4004  A=93.20% | Valid  L=0.4039  A=92.72% Rec1=0.8833 Prec1=0.6057 Rec(M)=0.9078 F1(M)=0.8384\n",
      "Epoch 23/80 | Train  L=0.3995  A=93.26% | Valid  L=0.4031  A=92.75% Rec1=0.8868 Prec1=0.6064 Rec(M)=0.9095 F1(M)=0.8393\n",
      "Epoch 24/80 | Train  L=0.3984  A=93.22% | Valid  L=0.4021  A=92.77% Rec1=0.8890 Prec1=0.6070 Rec(M)=0.9106 F1(M)=0.8399\n",
      "Epoch 25/80 | Train  L=0.3984  A=93.19% | Valid  L=0.4017  A=92.75% Rec1=0.8911 Prec1=0.6061 Rec(M)=0.9115 F1(M)=0.8399\n",
      "Epoch 26/80 | Train  L=0.3977  A=93.29% | Valid  L=0.4014  A=92.56% Rec1=0.8897 Prec1=0.5988 Rec(M)=0.9097 F1(M)=0.8365\n",
      "Epoch 27/80 | Train  L=0.3969  A=93.35% | Valid  L=0.4006  A=92.85% Rec1=0.8876 Prec1=0.6104 Rec(M)=0.9104 F1(M)=0.8411\n",
      "Epoch 28/80 | Train  L=0.3965  A=93.56% | Valid  L=0.3999  A=92.19% Rec1=0.8967 Prec1=0.5843 Rec(M)=0.9108 F1(M)=0.8313\n",
      "Epoch 29/80 | Train  L=0.3963  A=93.52% | Valid  L=0.3989  A=92.61% Rec1=0.8946 Prec1=0.6001 Rec(M)=0.9122 F1(M)=0.8379\n",
      "Epoch 30/80 | Train  L=0.3952  A=93.63% | Valid  L=0.3987  A=92.54% Rec1=0.8996 Prec1=0.5966 Rec(M)=0.9140 F1(M)=0.8372\n",
      "Epoch 31/80 | Train  L=0.3950  A=93.48% | Valid  L=0.3995  A=93.42% Rec1=0.8868 Prec1=0.6340 Rec(M)=0.9133 F1(M)=0.8509\n",
      "Epoch 32/80 | Train  L=0.3941  A=93.51% | Valid  L=0.3983  A=93.27% Rec1=0.8925 Prec1=0.6266 Rec(M)=0.9149 F1(M)=0.8488\n",
      "Epoch 33/80 | Train  L=0.3935  A=93.62% | Valid  L=0.3982  A=92.73% Rec1=0.8975 Prec1=0.6043 Rec(M)=0.9141 F1(M)=0.8402\n",
      "Epoch 34/80 | Train  L=0.3933  A=93.65% | Valid  L=0.3967  A=93.17% Rec1=0.8918 Prec1=0.6227 Rec(M)=0.9141 F1(M)=0.8471\n",
      "Epoch 35/80 | Train  L=0.3913  A=94.03% | Valid  L=0.3947  A=93.75% Rec1=0.8911 Prec1=0.6478 Rec(M)=0.9170 F1(M)=0.8573\n",
      "Epoch 36/80 | Train  L=0.3900  A=94.21% | Valid  L=0.3937  A=94.03% Rec1=0.8890 Prec1=0.6609 Rec(M)=0.9176 F1(M)=0.8620\n",
      "Epoch 37/80 | Train  L=0.3880  A=94.26% | Valid  L=0.3947  A=93.54% Rec1=0.8946 Prec1=0.6379 Rec(M)=0.9174 F1(M)=0.8539\n",
      "Epoch 38/80 | Train  L=0.3882  A=94.22% | Valid  L=0.3940  A=94.10% Rec1=0.8897 Prec1=0.6642 Rec(M)=0.9184 F1(M)=0.8635\n",
      "Epoch 39/80 | Train  L=0.3881  A=94.28% | Valid  L=0.3929  A=93.72% Rec1=0.8925 Prec1=0.6462 Rec(M)=0.9175 F1(M)=0.8569\n",
      "Epoch 40/80 | Train  L=0.3868  A=94.33% | Valid  L=0.3933  A=94.24% Rec1=0.8861 Prec1=0.6715 Rec(M)=0.9176 F1(M)=0.8656\n",
      "Early stopping: no improvement in 10 epochs.\n",
      "\n",
      "[Fold 2] Best recall_abnormal=0.8996 at epoch 30\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 3/6\n",
      "============================================================\n",
      "Train counts: Normal=60061, Abnormal=7070 | weights: w0=1.000, w1=8.495\n",
      "Epoch  1/80 | Train  L=0.6624  A=89.21% | Valid  L=0.6316  A=89.94% Rec1=0.7581 Prec1=0.5151 Rec(M)=0.8371 F1(M)=0.7778\n",
      "Epoch  2/80 | Train  L=0.6149  A=89.89% | Valid  L=0.5970  A=90.45% Rec1=0.8105 Prec1=0.5306 Rec(M)=0.8630 F1(M)=0.7931\n",
      "Epoch  3/80 | Train  L=0.5944  A=90.41% | Valid  L=0.5942  A=90.47% Rec1=0.8218 Prec1=0.5308 Rec(M)=0.8681 F1(M)=0.7950\n",
      "Epoch  4/80 | Train  L=0.5509  A=90.51% | Valid  L=0.5395  A=90.95% Rec1=0.8352 Prec1=0.5460 Rec(M)=0.8767 F1(M)=0.8041\n",
      "Epoch  5/80 | Train  L=0.5375  A=90.65% | Valid  L=0.5383  A=91.30% Rec1=0.8303 Prec1=0.5585 Rec(M)=0.8765 F1(M)=0.8089\n",
      "Epoch  6/80 | Train  L=0.5237  A=90.71% | Valid  L=0.4921  A=91.15% Rec1=0.8487 Prec1=0.5520 Rec(M)=0.8838 F1(M)=0.8089\n",
      "Epoch  7/80 | Train  L=0.4734  A=90.79% | Valid  L=0.4284  A=91.54% Rec1=0.8479 Prec1=0.5656 Rec(M)=0.8856 F1(M)=0.8149\n",
      "Epoch  8/80 | Train  L=0.4261  A=90.91% | Valid  L=0.4207  A=91.40% Rec1=0.8515 Prec1=0.5603 Rec(M)=0.8864 F1(M)=0.8131\n",
      "Epoch  9/80 | Train  L=0.4206  A=91.22% | Valid  L=0.4165  A=92.50% Rec1=0.8529 Prec1=0.6015 Rec(M)=0.8932 F1(M)=0.8312\n",
      "Epoch 10/80 | Train  L=0.4167  A=91.58% | Valid  L=0.4112  A=92.13% Rec1=0.8699 Prec1=0.5852 Rec(M)=0.8986 F1(M)=0.8272\n",
      "Epoch 11/80 | Train  L=0.4146  A=91.90% | Valid  L=0.4090  A=92.40% Rec1=0.8727 Prec1=0.5950 Rec(M)=0.9014 F1(M)=0.8320\n",
      "Epoch 12/80 | Train  L=0.4127  A=92.14% | Valid  L=0.4084  A=93.15% Rec1=0.8692 Prec1=0.6258 Rec(M)=0.9040 F1(M)=0.8442\n",
      "Epoch 13/80 | Train  L=0.4109  A=92.48% | Valid  L=0.4055  A=92.82% Rec1=0.8784 Prec1=0.6106 Rec(M)=0.9062 F1(M)=0.8396\n",
      "Epoch 14/80 | Train  L=0.4098  A=92.53% | Valid  L=0.4036  A=92.63% Rec1=0.8805 Prec1=0.6029 Rec(M)=0.9061 F1(M)=0.8367\n",
      "Epoch 15/80 | Train  L=0.4086  A=92.62% | Valid  L=0.4043  A=93.13% Rec1=0.8755 Prec1=0.6240 Rec(M)=0.9067 F1(M)=0.8447\n",
      "Epoch 16/80 | Train  L=0.4065  A=92.71% | Valid  L=0.4022  A=92.71% Rec1=0.8840 Prec1=0.6053 Rec(M)=0.9081 F1(M)=0.8384\n",
      "Epoch 17/80 | Train  L=0.4054  A=92.89% | Valid  L=0.4000  A=92.66% Rec1=0.8840 Prec1=0.6036 Rec(M)=0.9078 F1(M)=0.8376\n",
      "Epoch 18/80 | Train  L=0.4038  A=92.92% | Valid  L=0.4000  A=92.70% Rec1=0.8868 Prec1=0.6046 Rec(M)=0.9093 F1(M)=0.8385\n",
      "Epoch 19/80 | Train  L=0.4028  A=92.84% | Valid  L=0.3983  A=93.07% Rec1=0.8890 Prec1=0.6189 Rec(M)=0.9123 F1(M)=0.8450\n",
      "Epoch 20/80 | Train  L=0.4023  A=93.07% | Valid  L=0.3965  A=92.84% Rec1=0.8946 Prec1=0.6091 Rec(M)=0.9135 F1(M)=0.8418\n",
      "Epoch 21/80 | Train  L=0.4014  A=93.04% | Valid  L=0.3981  A=93.08% Rec1=0.8854 Prec1=0.6201 Rec(M)=0.9108 F1(M)=0.8449\n",
      "Epoch 22/80 | Train  L=0.4002  A=93.04% | Valid  L=0.3977  A=92.75% Rec1=0.8883 Prec1=0.6062 Rec(M)=0.9102 F1(M)=0.8395\n",
      "Epoch 23/80 | Train  L=0.3993  A=93.37% | Valid  L=0.3971  A=92.83% Rec1=0.8890 Prec1=0.6093 Rec(M)=0.9109 F1(M)=0.8409\n",
      "Epoch 24/80 | Train  L=0.3990  A=93.03% | Valid  L=0.3963  A=92.83% Rec1=0.8932 Prec1=0.6087 Rec(M)=0.9128 F1(M)=0.8414\n",
      "Epoch 25/80 | Train  L=0.3983  A=93.31% | Valid  L=0.3963  A=93.04% Rec1=0.8911 Prec1=0.6176 Rec(M)=0.9131 F1(M)=0.8448\n",
      "Epoch 26/80 | Train  L=0.3973  A=93.51% | Valid  L=0.3966  A=92.89% Rec1=0.8911 Prec1=0.6117 Rec(M)=0.9122 F1(M)=0.8423\n",
      "Epoch 27/80 | Train  L=0.3976  A=93.07% | Valid  L=0.3947  A=92.95% Rec1=0.8960 Prec1=0.6130 Rec(M)=0.9147 F1(M)=0.8437\n",
      "Epoch 28/80 | Train  L=0.3965  A=93.27% | Valid  L=0.3952  A=93.33% Rec1=0.8904 Prec1=0.6295 Rec(M)=0.9143 F1(M)=0.8497\n",
      "Epoch 29/80 | Train  L=0.3961  A=93.31% | Valid  L=0.3946  A=92.93% Rec1=0.8953 Prec1=0.6125 Rec(M)=0.9143 F1(M)=0.8434\n",
      "Epoch 30/80 | Train  L=0.3961  A=93.27% | Valid  L=0.3933  A=93.50% Rec1=0.8960 Prec1=0.6357 Rec(M)=0.9178 F1(M)=0.8533\n",
      "Epoch 31/80 | Train  L=0.3949  A=93.41% | Valid  L=0.3938  A=93.71% Rec1=0.8982 Prec1=0.6443 Rec(M)=0.9199 F1(M)=0.8572\n",
      "Epoch 32/80 | Train  L=0.3945  A=93.29% | Valid  L=0.3933  A=93.30% Rec1=0.8960 Prec1=0.6275 Rec(M)=0.9167 F1(M)=0.8499\n",
      "Epoch 33/80 | Train  L=0.3940  A=93.47% | Valid  L=0.3926  A=93.42% Rec1=0.8975 Prec1=0.6320 Rec(M)=0.9180 F1(M)=0.8520\n",
      "Epoch 34/80 | Train  L=0.3933  A=93.43% | Valid  L=0.3918  A=92.75% Rec1=0.9038 Prec1=0.6043 Rec(M)=0.9171 F1(M)=0.8413\n",
      "Epoch 35/80 | Train  L=0.3930  A=93.45% | Valid  L=0.3918  A=94.03% Rec1=0.8967 Prec1=0.6590 Rec(M)=0.9211 F1(M)=0.8628\n",
      "Epoch 36/80 | Train  L=0.3918  A=93.51% | Valid  L=0.3924  A=93.76% Rec1=0.8939 Prec1=0.6475 Rec(M)=0.9183 F1(M)=0.8577\n",
      "Epoch 37/80 | Train  L=0.3919  A=94.02% | Valid  L=0.3897  A=94.46% Rec1=0.8904 Prec1=0.6813 Rec(M)=0.9207 F1(M)=0.8702\n",
      "Epoch 38/80 | Train  L=0.3900  A=94.13% | Valid  L=0.3878  A=94.53% Rec1=0.8897 Prec1=0.6848 Rec(M)=0.9207 F1(M)=0.8714\n",
      "Epoch 39/80 | Train  L=0.3888  A=94.17% | Valid  L=0.3868  A=94.31% Rec1=0.8982 Prec1=0.6720 Rec(M)=0.9233 F1(M)=0.8682\n",
      "Epoch 40/80 | Train  L=0.3894  A=94.08% | Valid  L=0.3869  A=94.43% Rec1=0.8897 Prec1=0.6800 Rec(M)=0.9202 F1(M)=0.8696\n",
      "Epoch 41/80 | Train  L=0.3873  A=94.25% | Valid  L=0.3854  A=94.45% Rec1=0.8996 Prec1=0.6784 Rec(M)=0.9247 F1(M)=0.8709\n",
      "Epoch 42/80 | Train  L=0.3867  A=94.28% | Valid  L=0.3858  A=94.52% Rec1=0.8953 Prec1=0.6828 Rec(M)=0.9232 F1(M)=0.8718\n",
      "Epoch 43/80 | Train  L=0.3863  A=94.22% | Valid  L=0.3847  A=94.24% Rec1=0.9017 Prec1=0.6679 Rec(M)=0.9245 F1(M)=0.8673\n",
      "Epoch 44/80 | Train  L=0.3865  A=94.26% | Valid  L=0.3858  A=94.38% Rec1=0.8939 Prec1=0.6767 Rec(M)=0.9218 F1(M)=0.8691\n",
      "Early stopping: no improvement in 10 epochs.\n",
      "\n",
      "[Fold 3] Best recall_abnormal=0.9038 at epoch 34\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 4/6\n",
      "============================================================\n",
      "Train counts: Normal=60061, Abnormal=7070 | weights: w0=1.000, w1=8.495\n",
      "Epoch  1/80 | Train  L=0.6627  A=89.32% | Valid  L=0.6323  A=89.05% Rec1=0.7709 Prec1=0.4875 Rec(M)=0.8377 F1(M)=0.7670\n",
      "Epoch  2/80 | Train  L=0.6152  A=89.90% | Valid  L=0.5991  A=90.31% Rec1=0.7970 Prec1=0.5264 Rec(M)=0.8563 F1(M)=0.7891\n",
      "Epoch  3/80 | Train  L=0.5945  A=90.43% | Valid  L=0.5947  A=90.03% Rec1=0.8267 Prec1=0.5166 Rec(M)=0.8678 F1(M)=0.7890\n",
      "Epoch  4/80 | Train  L=0.5520  A=90.48% | Valid  L=0.5417  A=90.77% Rec1=0.8352 Prec1=0.5400 Rec(M)=0.8757 F1(M)=0.8013\n",
      "Epoch  5/80 | Train  L=0.5379  A=90.69% | Valid  L=0.5380  A=90.85% Rec1=0.8388 Prec1=0.5423 Rec(M)=0.8777 F1(M)=0.8029\n",
      "Epoch  6/80 | Train  L=0.5245  A=90.98% | Valid  L=0.4972  A=91.55% Rec1=0.8204 Prec1=0.5686 Rec(M)=0.8736 F1(M)=0.8116\n",
      "Epoch  7/80 | Train  L=0.4759  A=90.98% | Valid  L=0.4319  A=91.09% Rec1=0.8522 Prec1=0.5497 Rec(M)=0.8850 F1(M)=0.8084\n",
      "Epoch  8/80 | Train  L=0.4266  A=90.98% | Valid  L=0.4247  A=91.25% Rec1=0.8522 Prec1=0.5550 Rec(M)=0.8859 F1(M)=0.8109\n",
      "Epoch  9/80 | Train  L=0.4205  A=91.23% | Valid  L=0.4199  A=92.21% Rec1=0.8494 Prec1=0.5905 Rec(M)=0.8900 F1(M)=0.8260\n",
      "Epoch 10/80 | Train  L=0.4170  A=91.72% | Valid  L=0.4157  A=92.69% Rec1=0.8557 Prec1=0.6087 Rec(M)=0.8955 F1(M)=0.8347\n",
      "Epoch 11/80 | Train  L=0.4147  A=92.12% | Valid  L=0.4154  A=93.13% Rec1=0.8536 Prec1=0.6277 Rec(M)=0.8970 F1(M)=0.8421\n",
      "Epoch 12/80 | Train  L=0.4136  A=92.44% | Valid  L=0.4142  A=93.36% Rec1=0.8557 Prec1=0.6378 Rec(M)=0.8993 F1(M)=0.8465\n",
      "Epoch 13/80 | Train  L=0.4117  A=92.50% | Valid  L=0.4125  A=93.41% Rec1=0.8586 Prec1=0.6393 Rec(M)=0.9008 F1(M)=0.8476\n",
      "Epoch 14/80 | Train  L=0.4105  A=92.58% | Valid  L=0.4100  A=92.80% Rec1=0.8628 Prec1=0.6121 Rec(M)=0.8992 F1(M)=0.8375\n",
      "Epoch 15/80 | Train  L=0.4089  A=92.68% | Valid  L=0.4085  A=93.42% Rec1=0.8628 Prec1=0.6387 Rec(M)=0.9027 F1(M)=0.8482\n",
      "Epoch 16/80 | Train  L=0.4070  A=92.96% | Valid  L=0.4082  A=92.90% Rec1=0.8692 Prec1=0.6154 Rec(M)=0.9026 F1(M)=0.8400\n",
      "Epoch 17/80 | Train  L=0.4062  A=92.99% | Valid  L=0.4061  A=92.87% Rec1=0.8777 Prec1=0.6128 Rec(M)=0.9062 F1(M)=0.8404\n",
      "Epoch 18/80 | Train  L=0.4047  A=93.17% | Valid  L=0.4041  A=93.07% Rec1=0.8791 Prec1=0.6209 Rec(M)=0.9079 F1(M)=0.8440\n",
      "Epoch 19/80 | Train  L=0.4038  A=93.07% | Valid  L=0.4026  A=93.30% Rec1=0.8812 Prec1=0.6302 Rec(M)=0.9102 F1(M)=0.8483\n",
      "Epoch 20/80 | Train  L=0.4028  A=93.15% | Valid  L=0.4024  A=93.08% Rec1=0.8805 Prec1=0.6209 Rec(M)=0.9086 F1(M)=0.8443\n",
      "Epoch 21/80 | Train  L=0.4019  A=93.16% | Valid  L=0.4032  A=93.76% Rec1=0.8755 Prec1=0.6516 Rec(M)=0.9102 F1(M)=0.8558\n",
      "Epoch 22/80 | Train  L=0.4015  A=93.19% | Valid  L=0.4015  A=93.41% Rec1=0.8819 Prec1=0.6346 Rec(M)=0.9111 F1(M)=0.8502\n",
      "Epoch 23/80 | Train  L=0.4003  A=93.27% | Valid  L=0.4012  A=93.62% Rec1=0.8798 Prec1=0.6446 Rec(M)=0.9113 F1(M)=0.8538\n",
      "Epoch 24/80 | Train  L=0.3995  A=93.38% | Valid  L=0.3988  A=93.03% Rec1=0.8932 Prec1=0.6167 Rec(M)=0.9139 F1(M)=0.8448\n",
      "Epoch 25/80 | Train  L=0.3988  A=93.51% | Valid  L=0.3984  A=92.98% Rec1=0.8883 Prec1=0.6154 Rec(M)=0.9115 F1(M)=0.8434\n",
      "Epoch 26/80 | Train  L=0.3983  A=93.46% | Valid  L=0.3986  A=93.51% Rec1=0.8868 Prec1=0.6382 Rec(M)=0.9138 F1(M)=0.8526\n",
      "Epoch 27/80 | Train  L=0.3978  A=93.10% | Valid  L=0.3986  A=92.79% Rec1=0.8833 Prec1=0.6087 Rec(M)=0.9082 F1(M)=0.8397\n",
      "Epoch 28/80 | Train  L=0.3975  A=93.03% | Valid  L=0.3984  A=92.77% Rec1=0.8868 Prec1=0.6073 Rec(M)=0.9097 F1(M)=0.8397\n",
      "Epoch 29/80 | Train  L=0.3961  A=93.51% | Valid  L=0.3976  A=93.25% Rec1=0.8826 Prec1=0.6278 Rec(M)=0.9105 F1(M)=0.8475\n",
      "Epoch 30/80 | Train  L=0.3962  A=93.48% | Valid  L=0.3968  A=92.72% Rec1=0.8932 Prec1=0.6043 Rec(M)=0.9122 F1(M)=0.8395\n",
      "Epoch 31/80 | Train  L=0.3957  A=93.39% | Valid  L=0.3973  A=93.22% Rec1=0.8876 Prec1=0.6256 Rec(M)=0.9125 F1(M)=0.8475\n",
      "Epoch 32/80 | Train  L=0.3951  A=93.59% | Valid  L=0.3953  A=93.13% Rec1=0.8946 Prec1=0.6207 Rec(M)=0.9151 F1(M)=0.8468\n",
      "Epoch 33/80 | Train  L=0.3952  A=93.37% | Valid  L=0.3956  A=92.89% Rec1=0.8953 Prec1=0.6110 Rec(M)=0.9141 F1(M)=0.8428\n",
      "Epoch 34/80 | Train  L=0.3945  A=93.45% | Valid  L=0.3954  A=92.53% Rec1=0.8897 Prec1=0.5976 Rec(M)=0.9096 F1(M)=0.8360\n",
      "Epoch 35/80 | Train  L=0.3952  A=93.09% | Valid  L=0.3959  A=92.92% Rec1=0.8868 Prec1=0.6135 Rec(M)=0.9105 F1(M)=0.8423\n",
      "Epoch 36/80 | Train  L=0.3947  A=93.21% | Valid  L=0.3939  A=92.83% Rec1=0.8996 Prec1=0.6080 Rec(M)=0.9157 F1(M)=0.8422\n",
      "Epoch 37/80 | Train  L=0.3938  A=93.76% | Valid  L=0.3938  A=93.94% Rec1=0.8861 Prec1=0.6577 Rec(M)=0.9159 F1(M)=0.8602\n",
      "Epoch 38/80 | Train  L=0.3931  A=93.83% | Valid  L=0.3934  A=93.77% Rec1=0.8904 Prec1=0.6490 Rec(M)=0.9168 F1(M)=0.8576\n",
      "Epoch 39/80 | Train  L=0.3933  A=93.72% | Valid  L=0.3939  A=93.03% Rec1=0.8939 Prec1=0.6166 Rec(M)=0.9142 F1(M)=0.8449\n",
      "Epoch 40/80 | Train  L=0.3928  A=93.72% | Valid  L=0.3938  A=92.91% Rec1=0.8953 Prec1=0.6116 Rec(M)=0.9142 F1(M)=0.8430\n",
      "Epoch 41/80 | Train  L=0.3932  A=93.76% | Valid  L=0.3924  A=92.63% Rec1=0.9031 Prec1=0.5995 Rec(M)=0.9160 F1(M)=0.8391\n",
      "Epoch 42/80 | Train  L=0.3918  A=93.99% | Valid  L=0.3917  A=92.47% Rec1=0.9024 Prec1=0.5938 Rec(M)=0.9149 F1(M)=0.8364\n",
      "Epoch 43/80 | Train  L=0.3924  A=94.01% | Valid  L=0.3906  A=93.27% Rec1=0.8932 Prec1=0.6268 Rec(M)=0.9153 F1(M)=0.8491\n",
      "Epoch 44/80 | Train  L=0.3908  A=94.04% | Valid  L=0.3909  A=93.89% Rec1=0.8876 Prec1=0.6550 Rec(M)=0.9163 F1(M)=0.8594\n",
      "Epoch 45/80 | Train  L=0.3908  A=94.11% | Valid  L=0.3909  A=92.62% Rec1=0.8996 Prec1=0.5997 Rec(M)=0.9144 F1(M)=0.8386\n",
      "Epoch 46/80 | Train  L=0.3903  A=94.10% | Valid  L=0.3910  A=93.80% Rec1=0.8890 Prec1=0.6503 Rec(M)=0.9163 F1(M)=0.8578\n",
      "Epoch 47/80 | Train  L=0.3905  A=94.07% | Valid  L=0.3880  A=94.18% Rec1=0.8868 Prec1=0.6688 Rec(M)=0.9176 F1(M)=0.8647\n",
      "Epoch 48/80 | Train  L=0.3884  A=94.26% | Valid  L=0.3865  A=94.21% Rec1=0.8932 Prec1=0.6686 Rec(M)=0.9205 F1(M)=0.8659\n",
      "Epoch 49/80 | Train  L=0.3869  A=94.33% | Valid  L=0.3858  A=94.07% Rec1=0.8918 Prec1=0.6623 Rec(M)=0.9191 F1(M)=0.8631\n",
      "Epoch 50/80 | Train  L=0.3864  A=94.42% | Valid  L=0.3852  A=94.43% Rec1=0.8883 Prec1=0.6804 Rec(M)=0.9196 F1(M)=0.8694\n",
      "Epoch 51/80 | Train  L=0.3860  A=94.33% | Valid  L=0.3855  A=94.18% Rec1=0.8911 Prec1=0.6674 Rec(M)=0.9194 F1(M)=0.8650\n",
      "Early stopping: no improvement in 10 epochs.\n",
      "\n",
      "[Fold 4] Best recall_abnormal=0.9031 at epoch 41\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 5/6\n",
      "============================================================\n",
      "Train counts: Normal=60061, Abnormal=7070 | weights: w0=1.000, w1=8.495\n",
      "Epoch  1/80 | Train  L=0.6616  A=89.29% | Valid  L=0.6304  A=90.05% Rec1=0.7758 Prec1=0.5184 Rec(M)=0.8455 F1(M)=0.7821\n",
      "Epoch  2/80 | Train  L=0.6145  A=89.94% | Valid  L=0.5958  A=90.32% Rec1=0.8303 Prec1=0.5255 Rec(M)=0.8710 F1(M)=0.7938\n",
      "Epoch  3/80 | Train  L=0.5942  A=90.45% | Valid  L=0.5696  A=91.02% Rec1=0.8281 Prec1=0.5487 Rec(M)=0.8740 F1(M)=0.8042\n",
      "Epoch  4/80 | Train  L=0.5506  A=90.56% | Valid  L=0.5400  A=91.26% Rec1=0.8359 Prec1=0.5568 Rec(M)=0.8788 F1(M)=0.8090\n",
      "Epoch  5/80 | Train  L=0.5380  A=90.66% | Valid  L=0.5353  A=91.08% Rec1=0.8465 Prec1=0.5496 Rec(M)=0.8824 F1(M)=0.8075\n",
      "Epoch  6/80 | Train  L=0.5220  A=90.53% | Valid  L=0.4881  A=91.12% Rec1=0.8543 Prec1=0.5506 Rec(M)=0.8861 F1(M)=0.8092\n",
      "Epoch  7/80 | Train  L=0.4709  A=90.70% | Valid  L=0.4259  A=91.29% Rec1=0.8586 Prec1=0.5559 Rec(M)=0.8889 F1(M)=0.8123\n",
      "Epoch  8/80 | Train  L=0.4264  A=90.79% | Valid  L=0.4225  A=91.62% Rec1=0.8607 Prec1=0.5674 Rec(M)=0.8917 F1(M)=0.8178\n",
      "Epoch  9/80 | Train  L=0.4229  A=90.92% | Valid  L=0.4176  A=91.55% Rec1=0.8692 Prec1=0.5640 Rec(M)=0.8950 F1(M)=0.8177\n",
      "Epoch 10/80 | Train  L=0.4198  A=90.94% | Valid  L=0.4133  A=91.36% Rec1=0.8784 Prec1=0.5570 Rec(M)=0.8981 F1(M)=0.8158\n",
      "Epoch 11/80 | Train  L=0.4168  A=91.02% | Valid  L=0.4114  A=92.29% Rec1=0.8769 Prec1=0.5902 Rec(M)=0.9026 F1(M)=0.8306\n",
      "Epoch 12/80 | Train  L=0.4141  A=91.10% | Valid  L=0.4087  A=92.42% Rec1=0.8784 Prec1=0.5948 Rec(M)=0.9040 F1(M)=0.8329\n",
      "Epoch 13/80 | Train  L=0.4124  A=91.24% | Valid  L=0.4067  A=92.21% Rec1=0.8826 Prec1=0.5865 Rec(M)=0.9047 F1(M)=0.8299\n",
      "Epoch 14/80 | Train  L=0.4106  A=91.38% | Valid  L=0.4055  A=92.69% Rec1=0.8798 Prec1=0.6051 Rec(M)=0.9061 F1(M)=0.8375\n",
      "Epoch 15/80 | Train  L=0.4087  A=91.49% | Valid  L=0.4045  A=92.25% Rec1=0.8833 Prec1=0.5878 Rec(M)=0.9052 F1(M)=0.8306\n",
      "Epoch 16/80 | Train  L=0.4071  A=91.52% | Valid  L=0.4033  A=91.58% Rec1=0.8911 Prec1=0.5633 Rec(M)=0.9049 F1(M)=0.8207\n",
      "Epoch 17/80 | Train  L=0.4056  A=91.68% | Valid  L=0.4029  A=91.79% Rec1=0.8911 Prec1=0.5707 Rec(M)=0.9061 F1(M)=0.8242\n",
      "Epoch 18/80 | Train  L=0.4052  A=91.71% | Valid  L=0.4025  A=91.96% Rec1=0.8876 Prec1=0.5770 Rec(M)=0.9055 F1(M)=0.8265\n",
      "Epoch 19/80 | Train  L=0.4033  A=91.89% | Valid  L=0.4008  A=92.13% Rec1=0.8890 Prec1=0.5830 Rec(M)=0.9071 F1(M)=0.8294\n",
      "Epoch 20/80 | Train  L=0.4025  A=92.05% | Valid  L=0.3998  A=91.90% Rec1=0.8939 Prec1=0.5740 Rec(M)=0.9079 F1(M)=0.8261\n",
      "Epoch 21/80 | Train  L=0.4009  A=92.04% | Valid  L=0.3996  A=92.45% Rec1=0.8904 Prec1=0.5944 Rec(M)=0.9094 F1(M)=0.8347\n",
      "Epoch 22/80 | Train  L=0.4003  A=92.15% | Valid  L=0.3988  A=92.13% Rec1=0.8967 Prec1=0.5822 Rec(M)=0.9105 F1(M)=0.8303\n",
      "Epoch 23/80 | Train  L=0.4000  A=92.16% | Valid  L=0.3985  A=92.47% Rec1=0.8925 Prec1=0.5950 Rec(M)=0.9105 F1(M)=0.8353\n",
      "Epoch 24/80 | Train  L=0.3993  A=92.18% | Valid  L=0.3973  A=92.42% Rec1=0.8982 Prec1=0.5924 Rec(M)=0.9127 F1(M)=0.8351\n",
      "Epoch 25/80 | Train  L=0.3979  A=92.24% | Valid  L=0.3975  A=92.76% Rec1=0.8932 Prec1=0.6060 Rec(M)=0.9124 F1(M)=0.8403\n",
      "Epoch 26/80 | Train  L=0.3984  A=92.34% | Valid  L=0.3972  A=92.13% Rec1=0.8982 Prec1=0.5820 Rec(M)=0.9111 F1(M)=0.8305\n",
      "Epoch 27/80 | Train  L=0.3979  A=92.26% | Valid  L=0.3976  A=92.93% Rec1=0.8925 Prec1=0.6129 Rec(M)=0.9131 F1(M)=0.8431\n",
      "Epoch 28/80 | Train  L=0.3967  A=92.44% | Valid  L=0.3959  A=92.22% Rec1=0.8989 Prec1=0.5852 Rec(M)=0.9119 F1(M)=0.8320\n",
      "Epoch 29/80 | Train  L=0.3968  A=92.43% | Valid  L=0.3961  A=92.89% Rec1=0.9017 Prec1=0.6100 Rec(M)=0.9169 F1(M)=0.8434\n",
      "Epoch 30/80 | Train  L=0.3966  A=92.61% | Valid  L=0.3957  A=92.45% Rec1=0.9003 Prec1=0.5935 Rec(M)=0.9138 F1(M)=0.8359\n",
      "Epoch 31/80 | Train  L=0.3964  A=92.73% | Valid  L=0.3947  A=93.01% Rec1=0.9017 Prec1=0.6148 Rec(M)=0.9176 F1(M)=0.8455\n",
      "Epoch 32/80 | Train  L=0.3955  A=92.85% | Valid  L=0.3958  A=92.88% Rec1=0.9024 Prec1=0.6094 Rec(M)=0.9172 F1(M)=0.8433\n",
      "Epoch 33/80 | Train  L=0.3948  A=92.85% | Valid  L=0.3950  A=92.71% Rec1=0.9017 Prec1=0.6028 Rec(M)=0.9159 F1(M)=0.8403\n",
      "Epoch 34/80 | Train  L=0.3950  A=92.97% | Valid  L=0.3938  A=93.29% Rec1=0.9059 Prec1=0.6252 Rec(M)=0.9210 F1(M)=0.8506\n",
      "Epoch 35/80 | Train  L=0.3937  A=93.33% | Valid  L=0.3934  A=93.99% Rec1=0.9038 Prec1=0.6557 Rec(M)=0.9240 F1(M)=0.8628\n",
      "Epoch 36/80 | Train  L=0.3939  A=93.19% | Valid  L=0.3941  A=93.74% Rec1=0.9010 Prec1=0.6454 Rec(M)=0.9214 F1(M)=0.8581\n",
      "Epoch 37/80 | Train  L=0.3934  A=93.26% | Valid  L=0.3941  A=94.21% Rec1=0.8967 Prec1=0.6677 Rec(M)=0.9221 F1(M)=0.8662\n",
      "Epoch 38/80 | Train  L=0.3935  A=93.52% | Valid  L=0.3934  A=93.45% Rec1=0.9024 Prec1=0.6323 Rec(M)=0.9203 F1(M)=0.8530\n",
      "Epoch 39/80 | Train  L=0.3934  A=93.41% | Valid  L=0.3939  A=93.93% Rec1=0.8953 Prec1=0.6549 Rec(M)=0.9199 F1(M)=0.8609\n",
      "Epoch 40/80 | Train  L=0.3926  A=93.46% | Valid  L=0.3925  A=93.81% Rec1=0.9031 Prec1=0.6479 Rec(M)=0.9227 F1(M)=0.8595\n",
      "Epoch 41/80 | Train  L=0.3928  A=93.46% | Valid  L=0.3927  A=94.56% Rec1=0.8975 Prec1=0.6841 Rec(M)=0.9243 F1(M)=0.8727\n",
      "Epoch 42/80 | Train  L=0.3931  A=93.42% | Valid  L=0.3918  A=93.62% Rec1=0.9074 Prec1=0.6386 Rec(M)=0.9235 F1(M)=0.8565\n",
      "Epoch 43/80 | Train  L=0.3936  A=93.54% | Valid  L=0.3923  A=93.81% Rec1=0.9017 Prec1=0.6482 Rec(M)=0.9220 F1(M)=0.8594\n",
      "Epoch 44/80 | Train  L=0.3922  A=93.41% | Valid  L=0.3906  A=93.98% Rec1=0.9024 Prec1=0.6557 Rec(M)=0.9233 F1(M)=0.8626\n",
      "Epoch 45/80 | Train  L=0.3921  A=93.54% | Valid  L=0.3910  A=93.92% Rec1=0.9066 Prec1=0.6521 Rec(M)=0.9249 F1(M)=0.8619\n",
      "Epoch 46/80 | Train  L=0.3914  A=93.51% | Valid  L=0.3919  A=93.97% Rec1=0.8996 Prec1=0.6560 Rec(M)=0.9220 F1(M)=0.8621\n",
      "Epoch 47/80 | Train  L=0.3911  A=93.66% | Valid  L=0.3900  A=93.95% Rec1=0.9109 Prec1=0.6525 Rec(M)=0.9269 F1(M)=0.8629\n",
      "Epoch 48/80 | Train  L=0.3907  A=93.67% | Valid  L=0.3898  A=93.89% Rec1=0.9095 Prec1=0.6502 Rec(M)=0.9259 F1(M)=0.8617\n",
      "Epoch 49/80 | Train  L=0.3918  A=93.49% | Valid  L=0.3917  A=93.80% Rec1=0.9045 Prec1=0.6469 Rec(M)=0.9232 F1(M)=0.8594\n",
      "Epoch 50/80 | Train  L=0.3918  A=93.61% | Valid  L=0.3896  A=94.15% Rec1=0.9066 Prec1=0.6622 Rec(M)=0.9261 F1(M)=0.8660\n",
      "Epoch 51/80 | Train  L=0.3918  A=93.64% | Valid  L=0.3911  A=93.98% Rec1=0.9038 Prec1=0.6554 Rec(M)=0.9239 F1(M)=0.8627\n",
      "Epoch 52/80 | Train  L=0.3909  A=93.58% | Valid  L=0.3926  A=92.86% Rec1=0.9074 Prec1=0.6081 Rec(M)=0.9193 F1(M)=0.8435\n",
      "Epoch 53/80 | Train  L=0.3906  A=93.77% | Valid  L=0.3913  A=93.88% Rec1=0.9017 Prec1=0.6512 Rec(M)=0.9224 F1(M)=0.8606\n",
      "Epoch 54/80 | Train  L=0.3916  A=93.57% | Valid  L=0.3901  A=93.73% Rec1=0.9010 Prec1=0.6447 Rec(M)=0.9213 F1(M)=0.8579\n",
      "Epoch 55/80 | Train  L=0.3907  A=93.58% | Valid  L=0.3886  A=93.94% Rec1=0.9095 Prec1=0.6525 Rec(M)=0.9262 F1(M)=0.8626\n",
      "Epoch 56/80 | Train  L=0.3908  A=93.69% | Valid  L=0.3897  A=93.40% Rec1=0.9102 Prec1=0.6290 Rec(M)=0.9235 F1(M)=0.8530\n",
      "Epoch 57/80 | Train  L=0.3909  A=93.77% | Valid  L=0.3887  A=94.43% Rec1=0.9052 Prec1=0.6758 Rec(M)=0.9271 F1(M)=0.8711\n",
      "Early stopping: no improvement in 10 epochs.\n",
      "\n",
      "[Fold 5] Best recall_abnormal=0.9109 at epoch 47\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 6/6\n",
      "============================================================\n",
      "Train counts: Normal=60061, Abnormal=7070 | weights: w0=1.000, w1=8.495\n",
      "Epoch  1/80 | Train  L=0.6619  A=89.10% | Valid  L=0.6329  A=89.94% Rec1=0.7468 Prec1=0.5156 Rec(M)=0.8321 F1(M)=0.7762\n",
      "Epoch  2/80 | Train  L=0.6148  A=89.94% | Valid  L=0.5983  A=90.02% Rec1=0.8083 Prec1=0.5167 Rec(M)=0.8597 F1(M)=0.7864\n",
      "Epoch  3/80 | Train  L=0.5943  A=90.27% | Valid  L=0.5741  A=91.00% Rec1=0.8098 Prec1=0.5492 Rec(M)=0.8658 F1(M)=0.8014\n",
      "Epoch  4/80 | Train  L=0.5504  A=90.49% | Valid  L=0.5441  A=91.34% Rec1=0.8091 Prec1=0.5616 Rec(M)=0.8674 F1(M)=0.8066\n",
      "Epoch  5/80 | Train  L=0.5375  A=90.76% | Valid  L=0.5380  A=91.26% Rec1=0.8359 Prec1=0.5565 Rec(M)=0.8788 F1(M)=0.8089\n",
      "Epoch  6/80 | Train  L=0.5218  A=90.76% | Valid  L=0.4958  A=91.75% Rec1=0.8182 Prec1=0.5762 Rec(M)=0.8737 F1(M)=0.8145\n",
      "Epoch  7/80 | Train  L=0.4705  A=90.81% | Valid  L=0.4317  A=90.70% Rec1=0.8487 Prec1=0.5369 Rec(M)=0.8812 F1(M)=0.8019\n",
      "Epoch  8/80 | Train  L=0.4236  A=90.88% | Valid  L=0.4260  A=91.83% Rec1=0.8402 Prec1=0.5770 Rec(M)=0.8838 F1(M)=0.8186\n",
      "Epoch  9/80 | Train  L=0.4191  A=91.15% | Valid  L=0.4233  A=92.28% Rec1=0.8465 Prec1=0.5935 Rec(M)=0.8891 F1(M)=0.8267\n",
      "Epoch 10/80 | Train  L=0.4161  A=91.58% | Valid  L=0.4206  A=91.79% Rec1=0.8508 Prec1=0.5745 Rec(M)=0.8883 F1(M)=0.8193\n",
      "Epoch 11/80 | Train  L=0.4145  A=91.87% | Valid  L=0.4175  A=92.08% Rec1=0.8607 Prec1=0.5843 Rec(M)=0.8943 F1(M)=0.8253\n",
      "Epoch 12/80 | Train  L=0.4129  A=92.18% | Valid  L=0.4169  A=93.07% Rec1=0.8536 Prec1=0.6251 Rec(M)=0.8967 F1(M)=0.8410\n",
      "Epoch 13/80 | Train  L=0.4109  A=92.26% | Valid  L=0.4159  A=92.78% Rec1=0.8571 Prec1=0.6121 Rec(M)=0.8966 F1(M)=0.8364\n",
      "Epoch 14/80 | Train  L=0.4095  A=92.34% | Valid  L=0.4141  A=92.95% Rec1=0.8571 Prec1=0.6193 Rec(M)=0.8976 F1(M)=0.8394\n",
      "Epoch 15/80 | Train  L=0.4085  A=92.27% | Valid  L=0.4128  A=93.47% Rec1=0.8600 Prec1=0.6417 Rec(M)=0.9017 F1(M)=0.8489\n",
      "Epoch 16/80 | Train  L=0.4072  A=92.54% | Valid  L=0.4097  A=92.45% Rec1=0.8699 Prec1=0.5971 Rec(M)=0.9004 F1(M)=0.8324\n",
      "Epoch 17/80 | Train  L=0.4056  A=92.77% | Valid  L=0.4081  A=93.42% Rec1=0.8663 Prec1=0.6384 Rec(M)=0.9043 F1(M)=0.8488\n",
      "Epoch 18/80 | Train  L=0.4042  A=92.57% | Valid  L=0.4071  A=93.07% Rec1=0.8741 Prec1=0.6217 Rec(M)=0.9058 F1(M)=0.8435\n",
      "Epoch 19/80 | Train  L=0.4033  A=92.91% | Valid  L=0.4065  A=93.09% Rec1=0.8798 Prec1=0.6214 Rec(M)=0.9083 F1(M)=0.8444\n",
      "Epoch 20/80 | Train  L=0.4016  A=93.07% | Valid  L=0.4044  A=93.14% Rec1=0.8791 Prec1=0.6237 Rec(M)=0.9083 F1(M)=0.8452\n",
      "Epoch 21/80 | Train  L=0.4009  A=93.08% | Valid  L=0.4058  A=92.31% Rec1=0.8890 Prec1=0.5896 Rec(M)=0.9081 F1(M)=0.8323\n",
      "Epoch 22/80 | Train  L=0.4003  A=93.00% | Valid  L=0.4048  A=92.51% Rec1=0.8826 Prec1=0.5980 Rec(M)=0.9064 F1(M)=0.8350\n",
      "Epoch 23/80 | Train  L=0.3994  A=93.18% | Valid  L=0.4037  A=93.63% Rec1=0.8769 Prec1=0.6455 Rec(M)=0.9101 F1(M)=0.8536\n",
      "Epoch 24/80 | Train  L=0.3986  A=93.35% | Valid  L=0.4042  A=93.60% Rec1=0.8769 Prec1=0.6442 Rec(M)=0.9100 F1(M)=0.8531\n",
      "Epoch 25/80 | Train  L=0.3986  A=93.42% | Valid  L=0.4031  A=93.84% Rec1=0.8727 Prec1=0.6560 Rec(M)=0.9094 F1(M)=0.8570\n",
      "Epoch 26/80 | Train  L=0.3975  A=93.51% | Valid  L=0.4030  A=93.51% Rec1=0.8734 Prec1=0.6409 Rec(M)=0.9079 F1(M)=0.8511\n",
      "Epoch 27/80 | Train  L=0.3964  A=93.46% | Valid  L=0.4014  A=93.37% Rec1=0.8777 Prec1=0.6338 Rec(M)=0.9090 F1(M)=0.8491\n",
      "Epoch 28/80 | Train  L=0.3960  A=93.69% | Valid  L=0.4016  A=93.83% Rec1=0.8748 Prec1=0.6548 Rec(M)=0.9103 F1(M)=0.8569\n",
      "Epoch 29/80 | Train  L=0.3941  A=93.91% | Valid  L=0.3985  A=93.94% Rec1=0.8805 Prec1=0.6591 Rec(M)=0.9134 F1(M)=0.8597\n",
      "Epoch 30/80 | Train  L=0.3922  A=93.97% | Valid  L=0.3989  A=94.17% Rec1=0.8748 Prec1=0.6712 Rec(M)=0.9122 F1(M)=0.8632\n",
      "Epoch 31/80 | Train  L=0.3923  A=93.81% | Valid  L=0.3984  A=94.29% Rec1=0.8769 Prec1=0.6765 Rec(M)=0.9138 F1(M)=0.8656\n",
      "Early stopping: no improvement in 10 epochs.\n",
      "\n",
      "[Fold 6] Best recall_abnormal=0.8890 at epoch 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_steps = 10\n",
    "epochs = 15\n",
    "k_folds=6\n",
    "\n",
    "# Step 1: Load Data\n",
    "train_data, train_targets = load_train_test_data()\n",
    "\n",
    "# Step 2: Convert Data to Tensors\n",
    "dataset = create_csnn_datasets(train_data, train_targets) \n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Run CV - this creates fresh models per fold\n",
    "fold_history, fold_ckpts = train_model_cv(\n",
    "    model_factory=create_qcsnn_model,\n",
    "    epochs=80,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    loss_func=None,\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    optimizer_kwargs={'lr': 0.0001},\n",
    "    num_steps=10,\n",
    "    k_folds=6,\n",
    "    batch_size=256,\n",
    "    monitor=\"recall_abnormal\",  # ← Optimize for ABNORMAL recall specifically!\n",
    "    mode=\"max\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37027f7d-7fc3-4a35-a610-116a0712a375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f83681-7b52-4e85-b104-9943940d4ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2956a952-45a5-4ac1-9c24-c247cb5754be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: MIT-BIH (20161, 180)\n",
      "Data range after z-score normalization: [-5.26, 6.12]\n",
      "Data mean: 0.0000, std: 1.0000\n",
      "Class distribution: Counter({0: 18052, 1: 2109})\n",
      "X train shape: (20161, 1, 180)\n",
      "y_train shape: (20161,)\n",
      "Data tensor loaded successfully\n",
      "dataset created successfully\n",
      "Test set size: 20161\n",
      "Test normal samples: 18052\n",
      "Test abnormal samples: 2109\n"
     ]
    }
   ],
   "source": [
    "# Load test data (adjust function name if different)\n",
    "mitbih_test_path='/..../mitbih_processed_intra_patient_4class_180_center90_filtered/test'\n",
    "\n",
    "test_data, test_targets = load_train_test_data(mitbih_test_path)\n",
    "test_dataset = create_csnn_datasets(test_data, test_targets)\n",
    "\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "print(f\"Test normal samples: {(test_targets == 0).sum()}\")\n",
    "print(f\"Test abnormal samples: {(test_targets == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd6701-a6b6-4e31-939a-7b3da8b889bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818515e-be23-4e76-be8e-b7f5ef64a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dump_brevitas_all_layers(qcsnet2_eval, test_data, num_steps=10, out_dir=\"../../../compare_outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4ae213b-4b02-421a-9924-f50d517acabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "import snntorch as snn\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "\n",
    "def _ensure_dir(d):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def _to_one(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.numel() == 1:\n",
    "            return float(x.detach().cpu().item())\n",
    "        return x.detach().cpu().numpy()\n",
    "    if isinstance(x, (float, int)):\n",
    "        return x\n",
    "    return x\n",
    "\n",
    "def _get_bit_scale_zp_from_quant(q):\n",
    "    \"\"\"Return (bit_width, scale, zero_point, signed) from a brevitas quantizer-like object.\"\"\"\n",
    "    bit_width = None\n",
    "    scale = None\n",
    "    zero_point = None\n",
    "    signed = True\n",
    "    # bit width\n",
    "    for k in ('bit_width', 'bit_width_impl', 'bit_width_f'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            bit_width = int(_to_one(v))\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # scale\n",
    "    for k in ('scale', 'tensor_scale', 'scale_impl', 'act_scale', 'weight_scale'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            v = _to_one(v)\n",
    "            if isinstance(v, (float, int)):\n",
    "                scale = float(v)\n",
    "                break\n",
    "            if isinstance(v, np.ndarray) and v.size == 1:\n",
    "                scale = float(v.item())\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # zero-point\n",
    "    for k in ('zero_point', 'zero_point_impl', 'zp'):\n",
    "        v = getattr(q, k, None)\n",
    "        if v is None: continue\n",
    "        try:\n",
    "            v = v() if callable(v) else v\n",
    "            zero_point = int(round(_to_one(v)))\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    # signed\n",
    "    sattr = getattr(q, 'signed', None)\n",
    "    if isinstance(sattr, bool):\n",
    "        signed = sattr\n",
    "    elif zero_point is None:\n",
    "        signed = True\n",
    "    # defaults\n",
    "    if bit_width is None: bit_width = 8\n",
    "    if scale is None:     scale = 1.0\n",
    "    if zero_point is None: zero_point = 0\n",
    "    return bit_width, float(scale), int(zero_point), bool(signed)\n",
    "\n",
    "def _quantize_multiplier(real_multiplier: float):\n",
    "    \"\"\"TFLite-style integer multiplier/shift approximation for a positive real multiplier.\"\"\"\n",
    "    if real_multiplier <= 0.0:\n",
    "        return 0, 0\n",
    "    mantissa, exponent = math.frexp(real_multiplier)  # real = mantissa * 2^exponent, mantissa in [0.5,1)\n",
    "    q = int(round(mantissa * (1 << 31)))\n",
    "    if q == (1 << 31):\n",
    "        q //= 2\n",
    "        exponent += 1\n",
    "    shift = 31 - exponent\n",
    "    if shift < 0:\n",
    "        q <<= (-shift)\n",
    "        shift = 0\n",
    "    return int(q), int(shift)\n",
    "\n",
    "# helper types used in headers (names only; arrays use ap_int in C++)\n",
    "def _sum_weights_per_out(W_int8: np.ndarray) -> np.ndarray:\n",
    "    # W_int8 shape: [OUT_CH, IN_CH, K]\n",
    "    # returns int32 sums per OUT_CH\n",
    "    return W_int8.reshape(W_int8.shape[0], -1).sum(axis=1).astype(np.int32)\n",
    "\n",
    "def _bias_int32_vector(b_f: np.ndarray, s_in: float, s_w: float, out_ch: int) -> np.ndarray:\n",
    "    # Quantize bias or return zeros if no bias provided\n",
    "    if b_f is None:\n",
    "        return np.zeros(out_ch, dtype=np.int32)\n",
    "    s_bias = s_in * s_w if (s_in and s_w) else 1.0\n",
    "    return np.round(b_f / s_bias).astype(np.int32)\n",
    "\n",
    "\n",
    "def _qt_weight_int8_per_tensor(W_f: np.ndarray, scale: float, zero_point: int = 0):\n",
    "    Wq = np.round(W_f / scale) + zero_point\n",
    "    return np.clip(Wq, -128, 127).astype(np.int8)\n",
    "\n",
    "def _bias_int32_from_float(b_f: np.ndarray, s_in: float, s_w: float):\n",
    "    s_bias = s_in * s_w\n",
    "    if s_bias == 0.0: s_bias = 1.0\n",
    "    bq = np.round(b_f / s_bias).astype(np.int32)\n",
    "    return bq\n",
    "\n",
    "def _as1(x):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return int(x[0])\n",
    "    return int(x)\n",
    "\n",
    "def _guard_out_scale(name: str, out_scale: float):\n",
    "    if out_scale is None or not np.isfinite(out_scale) or out_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid out_scale={out_scale}\")\n",
    "\n",
    "def _guard_in_scale(name: str, in_scale: float):\n",
    "    if in_scale is None or not np.isfinite(in_scale) or in_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid in_scale={in_scale}\")\n",
    "\n",
    "def _guard_weight_scale(name: str, w_scale: float):\n",
    "    if w_scale is None or not np.isfinite(w_scale) or w_scale <= 0.0:\n",
    "        raise ValueError(f\"{name}: invalid weight_scale={w_scale}\")\n",
    "\n",
    "def _id_guard_macro(base: str):\n",
    "    return base.upper().replace('/', '_').replace('.', '_')\n",
    "\n",
    "def _sym(name: str):\n",
    "    \"\"\"C identifier from layer name (keep as-is but safe for C).\"\"\"\n",
    "    return name.replace('/', '_').replace('.', '_')\n",
    "\n",
    "def _fmt_int_list(vals, per_line=16):\n",
    "    out = []\n",
    "    line = []\n",
    "    for i, v in enumerate(vals):\n",
    "        line.append(str(int(v)))\n",
    "        if (i + 1) % per_line == 0:\n",
    "            out.append(\", \".join(line))\n",
    "            line = []\n",
    "    if line:\n",
    "        out.append(\", \".join(line))\n",
    "    return \",\\n    \".join(out)\n",
    "\n",
    "def _fmt_array_2d(arr2d):\n",
    "    rows = []\n",
    "    for r in arr2d:\n",
    "        rows.append(\"{ \" + _fmt_int_list(r) + \" }\")\n",
    "    return \"{\\n  \" + \",\\n  \".join(rows) + \"\\n}\"\n",
    "\n",
    "def _fmt_array_3d(arr3d):\n",
    "    blocks = []\n",
    "    for b in arr3d:\n",
    "        blocks.append(_fmt_array_2d(b))\n",
    "    return \"{\\n\" + \",\\n\".join(blocks) + \"\\n}\"\n",
    "\n",
    "# -----------------------\n",
    "# Emitters\n",
    "# -----------------------\n",
    "\n",
    "def _emit_header_open(fp, guard, ns=\"hls4csnn1d_cblk_sd\"):\n",
    "    fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "    fp.write(\"#include <ap_int.h>\\n\\n\")\n",
    "    fp.write(f\"namespace {ns} {{\\n\\n\")\n",
    "\n",
    "def _emit_header_close(fp, guard, ns=\"hls4csnn1d_cblk_sd\"):\n",
    "    fp.write(f\"}} // namespace\\n#endif // {guard}\\n\")\n",
    "\n",
    "def _emit_conv1d_header(path, lname, W_int8, rq_mult, rq_shift,\n",
    "                        bias_int32_vec, input_zp, weight_sum_vec, m):\n",
    "    # Guard: QCSNET2_CBLK1_QCONV1D_WEIGHTS_H style\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_WEIGHTS_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        _emit_header_open(fp, guard)  # writes includes + namespace line\n",
    "\n",
    "        sym   = _sym(lname)\n",
    "        OC, IC, K = W_int8.shape\n",
    "        stride = _as1(m.stride)\n",
    "\n",
    "        # Structural constants (optional but handy for TBs)\n",
    "        fp.write(f\"const int {sym}_OUT_CH = {OC};\\n\")\n",
    "        fp.write(f\"const int {sym}_IN_CH  = {IC};\\n\")\n",
    "        fp.write(f\"const int {sym}_KERNEL_SIZE = {K};\\n\")\n",
    "        fp.write(f\"const int {sym}_STRIDE = {stride};\\n\\n\")\n",
    "\n",
    "        # Input ZP (INT8)\n",
    "        fp.write(f\"const ap_int<8> {sym}_input_zero_point = {int(input_zp)};\\n\\n\")\n",
    "\n",
    "        # Requantization arrays (duplicated per OUT_CH to match your API)\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_mult]*OC))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const int {sym}_right_shift[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_shift]*OC))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Bias (INT32)\n",
    "        fp.write(f\"const acc32_t {sym}_bias[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(bias_int32_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weight sums for asymmetric correction (INT32)\n",
    "        fp.write(f\"const acc32_t {sym}_weight_sum[{OC}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(weight_sum_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weights (INT8): [OUT_CH][IN_CH][K]\n",
    "        fp.write(f\"const ap_int<8> {sym}_weights[{OC}][{IC}][{K}] = \")\n",
    "        fp.write(_fmt_array_3d(W_int8))\n",
    "        fp.write(\";\\n\\n\")\n",
    "\n",
    "        _emit_header_close(fp, guard)  # closes namespace + guard\n",
    "\n",
    "\n",
    "def _emit_linear_header(path, lname,\n",
    "                           W_int8,                 # [OUT][IN] int8\n",
    "                           rq_mult, rq_shift,      # per-tensor constants, repeated per OUT\n",
    "                           bias_int32_vec,         # [OUT] int32\n",
    "                           input_zp,               # int (will be placed as ap_int<8>)\n",
    "                           weight_sum_vec):        # [OUT] int32\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_WEIGHTS_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "\n",
    "        sym = _sym(lname)\n",
    "        OUT, IN = W_int8.shape\n",
    "\n",
    "        # Structural (optional helpers)\n",
    "        fp.write(f\"const int {sym}_OUTPUT_SIZE = {OUT};\\n\")\n",
    "        fp.write(f\"const int {sym}_INPUT_SIZE  = {IN};\\n\\n\")\n",
    "\n",
    "        # Input zero-point (matches template type)\n",
    "        fp.write(f\"const ap_int<8> {sym}_input_zero_point = {int(input_zp)};\\n\\n\")\n",
    "\n",
    "        # Requant arrays (match template types; repeat the per-tensor constants)\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_mult] * OUT))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const int {sym}_right_shift[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list([rq_shift] * OUT))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Bias and weight_sum (acc domain)\n",
    "        fp.write(f\"using acc32_t = ap_int<32>;\\n\")\n",
    "        fp.write(f\"const acc32_t {sym}_bias[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(bias_int32_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const acc32_t {sym}_weight_sum[{OUT}] = {{\\n  \")\n",
    "        fp.write(_fmt_int_list(weight_sum_vec))\n",
    "        fp.write(\"\\n};\\n\\n\")\n",
    "\n",
    "        # Weights (use ap_int8_c to match your template)\n",
    "        fp.write(f\"const ap_int8_c {sym}_weights[{OUT}][{IN}] = \")\n",
    "        fp.write(_fmt_array_2d(W_int8))\n",
    "        fp.write(\";\\n\\n\")\n",
    "\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def _emit_bn_header(path, lname, w_q, b32, mult_arr, shift_arr):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_BN_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        sym = _sym(lname); C = len(w_q)\n",
    "\n",
    "        fp.write(f\"const int {sym}_C = {C};\\n\\n\")\n",
    "\n",
    "        fp.write(f\"const ap_int8_c {sym}_weight[{C}] = {{\\n  {_fmt_int_list(w_q)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<32> {sym}_bias[{C}] = {{\\n  {_fmt_int_list(b32)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<32> {sym}_scale_multiplier[{C}] = {{\\n  {_fmt_int_list(mult_arr)}\\n}};\\n\\n\")\n",
    "        fp.write(f\"const int {sym}_right_shift[{C}] = {{\\n  {_fmt_int_list(shift_arr)}\\n}};\\n\\n\")\n",
    "\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def _emit_lif_header_scalar_sd(path, lname, beta_q, theta_q, scale_q, frac_bits):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_LIF_H\")\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        sym = _sym(lname)\n",
    "        fp.write(f\"enum {{ {sym}_FRAC_BITS = {int(frac_bits)} }};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_beta_int   = {int(beta_q)};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_theta_int  = {int(theta_q)};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_scale_int  = {int(scale_q)};\\n\\n\")\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "def _emit_lif_header_vector_sd(path, lname, beta_arr_q, theta_arr_q, scale_q, frac_bits):\n",
    "    guard = _id_guard_macro(f\"{_sym(lname)}_LIF_H\")\n",
    "    sym   = _sym(lname)\n",
    "    N     = len(beta_arr_q)\n",
    "    if len(theta_arr_q) != N:\n",
    "        raise ValueError(\"beta/theta array lengths must match\")\n",
    "\n",
    "    def _fmt_list(vals, per_line=16):\n",
    "        rows = []\n",
    "        for i in range(0, len(vals), per_line):\n",
    "            chunk = \", \".join(str(int(v)) for v in vals[i:i+per_line])\n",
    "            rows.append(\"    \" + chunk)\n",
    "        return \"{\\n\" + \",\\n\".join(rows) + \"\\n}\"\n",
    "\n",
    "    with open(path, \"w\") as fp:\n",
    "        fp.write(f\"#ifndef {guard}\\n#define {guard}\\n\\n\")\n",
    "        fp.write(\"#include <hls_stream.h>\\n#include <ap_int.h>\\n#include \\\"../constants_sd.h\\\"\\n\\n\")\n",
    "        fp.write(\"namespace hls4csnn1d_cblk_sd {\\n\\n\")\n",
    "        fp.write(f\"enum {{ {sym}_FRAC_BITS = {int(frac_bits)}, {sym}_OUT_CH = {int(N)} }};\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_scale_int = {int(scale_q)};\\n\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_beta_int[{sym}_OUT_CH] = \"  + _fmt_list(beta_arr_q)  + \";\\n\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym}_theta_int[{sym}_OUT_CH] = \" + _fmt_list(theta_arr_q) + \";\\n\\n\")\n",
    "        fp.write(\"} // namespace hls4csnn1d_cblk_sd\\n\")\n",
    "        fp.write(f\"#endif // {guard}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "INT16_MIN, INT16_MAX = -32768, 32767\n",
    "\n",
    "def _to_q_i16(x: float, Q: int) -> int:\n",
    "    return int(np.clip(round(float(x) * Q), INT16_MIN, INT16_MAX))\n",
    "\n",
    "def _tensor_to_q_i16_list(t: torch.Tensor, Q: int):\n",
    "    flat = t.detach().float().reshape(-1).cpu().tolist()\n",
    "    return [_to_q_i16(v, Q) for v in flat]\n",
    "\n",
    "\n",
    "_Q_SCALE = 1 << 12   # e.g., FRAC_BITS = 12\n",
    "\n",
    "def _emit_qparams_header(path, lname, bit_w, scale, zp):\n",
    "    guard = _id_guard_macro(f\"QPARAMS_{_sym(lname)}_H\")\n",
    "    sym = _sym(lname)\n",
    "    sym_base = sym  # no renaming, no suffix stripping\n",
    "\n",
    "    # Q-encode the activation scale for HLS QuantIdentity (uses _Q_SCALE; not emitted)\n",
    "    act_scale_int = _to_q_i16(float(scale), _Q_SCALE)\n",
    "\n",
    "    with open(path, \"w\") as fp:\n",
    "        _emit_header_open(fp, guard)  # must include <ap_int.h> and open your namespace\n",
    "\n",
    "        fp.write(\"// Activation quantization parameters (optional for kernels)\\n\")\n",
    "        fp.write(f\"const int   {sym}_bit_width = {int(bit_w)};\\n\")\n",
    "        fp.write(f\"// const float {sym}_scale     = {float(scale):.10g};  // kept for reference only\\n\")\n",
    "        fp.write(f\"const ap_int<16> {sym_base}_act_scale_int = {int(act_scale_int)};\\n\")\n",
    "        fp.write(f\"const int   {sym}_zero_point= {int(zp)};\\n\\n\")\n",
    "\n",
    "        _emit_header_close(fp, guard)  # close namespace and guard\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Orchestrator\n",
    "# -----------------------\n",
    "\n",
    "def emit_headers_for_model(model: torch.nn.Module,\n",
    "                           example_input: torch.Tensor,\n",
    "                           out_dir: str = \"headers_int\",\n",
    "                           lif_frac_bits: int = 12):\n",
    "    model.eval()\n",
    "    _ensure_dir(out_dir)\n",
    "\n",
    "    # Run one dry forward to initialize any lazy buffers (ignore output tuples)\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            _ = model(example_input)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Track current activation qparams (propagated as in your graph)\n",
    "    current_act = {\"bit_width\": 8, \"scale\": 1.0, \"zero_point\": 0}\n",
    "    \n",
    "    last_out_ch = None  # initialize outside the loop\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        if m is model:\n",
    "            continue\n",
    "\n",
    "        # QuantIdentity (export activation qparams as optional header)\n",
    "        if isinstance(m, qnn.QuantIdentity):\n",
    "            aq = getattr(m, 'act_quant', getattr(m, 'output_quant', None))\n",
    "            bit_w, s, zp, _ = _get_bit_scale_zp_from_quant(aq)\n",
    "            _emit_qparams_header(os.path.join(out_dir, f\"qparams_{name}.h\"), name, bit_w, s, zp)\n",
    "            current_act = {\"bit_width\": bit_w, \"scale\": s, \"zero_point\": zp}\n",
    "            continue\n",
    "\n",
    "\n",
    "        if isinstance(m, qnn.QuantConv1d):\n",
    "            # Float → INT8 weights\n",
    "            Wf = m.weight.detach().cpu().numpy()            # [OUT, IN, K]\n",
    "            wq = getattr(m, 'weight_quant', None)\n",
    "            wb, s_w, z_w, _ = _get_bit_scale_zp_from_quant(wq)\n",
    "        \n",
    "            _guard_in_scale(name, current_act['scale'])\n",
    "            _guard_weight_scale(name, s_w)\n",
    "        \n",
    "            W_int8 = _qt_weight_int8_per_tensor(Wf, s_w, z_w)\n",
    "        \n",
    "            # Output activation qparams (for requant)\n",
    "            oq = getattr(m, 'output_quant', None)\n",
    "            ob, s_out, z_out, _ = _get_bit_scale_zp_from_quant(oq)\n",
    "            _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Integer requant constants (per-tensor → repeat per OUT_CH)\n",
    "            M = (current_act['scale'] * s_w) / s_out\n",
    "            rq_mult, rq_shift = _quantize_multiplier(M)\n",
    "        \n",
    "            # Bias (if present) → INT32 vector (length OUT_CH); else zeros\n",
    "            b_f = m.bias.detach().cpu().numpy() if (hasattr(m, 'bias') and m.bias is not None) else None\n",
    "            bias_int32_vec = _bias_int32_vector(b_f, current_act['scale'], s_w, W_int8.shape[0])\n",
    "        \n",
    "            # Weight sums per output channel (for asymmetric correction)\n",
    "            weight_sum_vec = _sum_weights_per_out(W_int8)\n",
    "        \n",
    "            # Input zero point (INT8) for asymmetric correction\n",
    "            input_zp = current_act['zero_point']\n",
    "        \n",
    "            # Emit header that matches your Conv1D_SD::forward signature\n",
    "            _emit_conv1d_header(\n",
    "                os.path.join(out_dir, f\"{name}_weights.h\"),\n",
    "                name,\n",
    "                W_int8,\n",
    "                rq_mult, rq_shift,\n",
    "                bias_int32_vec,\n",
    "                input_zp,\n",
    "                weight_sum_vec,\n",
    "                m\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams for the next layer\n",
    "            current_act = {\"bit_width\": ob, \"scale\": s_out, \"zero_point\": z_out}\n",
    "            continue\n",
    "\n",
    "\n",
    "        # BN -> ScaleBias (for BatchNorm1dToQuantScaleBias)\n",
    "        if hasattr(qnn, 'BatchNorm1dToQuantScaleBias') and isinstance(m, qnn.BatchNorm1dToQuantScaleBias):\n",
    "            # gamma, beta (float, per channel)\n",
    "            gamma = _to_one(getattr(m, 'weight', None)) if getattr(m, 'weight', None) is not None else _to_one(getattr(m, 'scale', None))\n",
    "            beta  = _to_one(getattr(m, 'bias',   None)) if getattr(m, 'bias',   None) is not None else _to_one(getattr(m, 'beta',  None))\n",
    "            if gamma is None: gamma = 1.0\n",
    "            if beta  is None: beta  = 0.0\n",
    "            gamma = np.array(gamma, dtype=np.float32).reshape(-1)\n",
    "            beta  = np.array(beta,  dtype=np.float32).reshape(-1)\n",
    "            C = gamma.shape[0]\n",
    "        \n",
    "            # Input/output quant\n",
    "            s_in = float(current_act['scale']);  z_in = int(current_act['zero_point']);  _guard_in_scale(name, s_in)\n",
    "            oq   = getattr(m, 'output_quant', None)\n",
    "            _, s_out, _, _ = _get_bit_scale_zp_from_quant(oq);  _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Brevitas BN quantized weight\n",
    "            qW = m.quant_weight()  # IntQuantTensor\n",
    "            w_q = qW.int().detach().cpu().numpy().astype(np.int8)    # [C]\n",
    "            s_w = float(_to_one(qW.scale))                            # scalar\n",
    "        \n",
    "            # Shared requant scale S and its integer pair\n",
    "            S = s_w * (s_in / s_out)\n",
    "            mult_S, shift_S = _quantize_multiplier(S)\n",
    "        \n",
    "            # Int32 bias per channel (no int8 clipping)\n",
    "            # M_c = (s_in/s_out) * gamma_c  = S * w_q[c]  (approximately)\n",
    "            M = gamma * (s_in / s_out)                          # [C]\n",
    "            b32 = np.round((beta / s_out - M * z_in) / S).astype(np.int32)  # [C]\n",
    "        \n",
    "            _emit_bn_header(\n",
    "                os.path.join(out_dir, f\"{name}_bn.h\"),\n",
    "                name,\n",
    "                w_q.tolist(),\n",
    "                b32.tolist(),\n",
    "                [mult_S] * C,\n",
    "                [shift_S] * C\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams\n",
    "            current_act = {\"bit_width\": current_act['bit_width'], \"scale\": s_out, \"zero_point\": 0}\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # QuantLinear\n",
    "        if isinstance(m, qnn.QuantLinear):\n",
    "            # Float → INT8 weights\n",
    "            Wf = m.weight.detach().cpu().numpy()             # [OUT, IN]\n",
    "            wq = getattr(m, 'weight_quant', None)\n",
    "            wb, s_w, z_w, _ = _get_bit_scale_zp_from_quant(wq)\n",
    "        \n",
    "            _guard_in_scale(name, current_act['scale'])\n",
    "            _guard_weight_scale(name, s_w)\n",
    "        \n",
    "            W_int8 = _qt_weight_int8_per_tensor(Wf, s_w, z_w)\n",
    "        \n",
    "            # Output activation qparams (for requant)\n",
    "            oq = getattr(m, 'output_quant', None)\n",
    "            ob, s_out, z_out, _ = _get_bit_scale_zp_from_quant(oq)\n",
    "            _guard_out_scale(name, s_out)\n",
    "        \n",
    "            # Requant: M = (s_in * s_w) / s_out\n",
    "            M = (current_act['scale'] * s_w) / s_out\n",
    "            rq_mult, rq_shift = _quantize_multiplier(M)\n",
    "        \n",
    "            # Bias (if present) → INT32; else zeros\n",
    "            b_f = m.bias.detach().cpu().numpy() if (hasattr(m, 'bias') and m.bias is not None) else None\n",
    "            bias_int32_vec = _bias_int32_vector(b_f, current_act['scale'], s_w, W_int8.shape[0])\n",
    "        \n",
    "            # Weight sums for asymmetric correction: sum over input dim\n",
    "            weight_sum_vec = W_int8.sum(axis=1).astype(np.int32)\n",
    "        \n",
    "            # Input zero-point for asymmetric correction\n",
    "            input_zp = current_act['zero_point']\n",
    "        \n",
    "            # Emit header that matches Linear1D_SD::forward\n",
    "            _emit_linear_header(\n",
    "                os.path.join(out_dir, f\"{name}_weights.h\"),\n",
    "                name,\n",
    "                W_int8,\n",
    "                rq_mult, rq_shift,\n",
    "                bias_int32_vec,\n",
    "                input_zp,\n",
    "                weight_sum_vec\n",
    "            )\n",
    "        \n",
    "            # Advance activation qparams\n",
    "            current_act = {\"bit_width\": ob, \"scale\": s_out, \"zero_point\": z_out}\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if isinstance(m, snn.Leaky):\n",
    "            scale_in = float(current_act['scale'])\n",
    "            Q = 1 << lif_frac_bits\n",
    "        \n",
    "            # Grab beta/threshold as tensors (handles both scalar and vector)\n",
    "            beta_t  = m.beta if isinstance(m.beta, torch.Tensor) else torch.as_tensor(m.beta)\n",
    "            thr_t   = m.threshold if isinstance(m.threshold, torch.Tensor) else torch.as_tensor(m.threshold)\n",
    "        \n",
    "            beta_t  = beta_t.detach().float()\n",
    "            print(\"beta: \", beta_t)\n",
    "            thr_t   = thr_t.detach().float()\n",
    "        \n",
    "            # Number of “neurons” (channels) from parameter size\n",
    "            n_beta  = int(beta_t.numel())\n",
    "            n_thr   = int(thr_t.numel())\n",
    "            if n_beta != n_thr and n_beta != 1 and n_thr != 1:\n",
    "                raise ValueError(f\"LIF param size mismatch: beta has {n_beta}, threshold has {n_thr}\")\n",
    "        \n",
    "            # Broadcast if one is scalar\n",
    "            out_ch = max(n_beta, n_thr)\n",
    "            if n_beta == 1 and out_ch > 1:\n",
    "                beta_t = beta_t.expand(out_ch)\n",
    "            if n_thr == 1 and out_ch > 1:\n",
    "                thr_t = thr_t.expand(out_ch)\n",
    "        \n",
    "            # Quantize\n",
    "            beta_arr_q  = _tensor_to_q_i16_list(beta_t, Q)\n",
    "            theta_arr_q = _tensor_to_q_i16_list(thr_t, Q)\n",
    "            scale_q     = _to_q_i16(scale_in, Q)   # keep scale scalar for now\n",
    "        \n",
    "            # Emit vector or scalar header depending on out_ch\n",
    "            out_path = os.path.join(out_dir, f\"{name}_lif.h\")\n",
    "            if out_ch > 1:\n",
    "                _emit_lif_header_vector_sd(\n",
    "                    out_path, name, beta_arr_q, theta_arr_q, scale_q, lif_frac_bits\n",
    "                )\n",
    "            else:\n",
    "                _emit_lif_header_scalar_sd(\n",
    "                    out_path, name, beta_arr_q[0], theta_arr_q[0], scale_q, lif_frac_bits\n",
    "                )\n",
    "        \n",
    "            # LIF outputs binary spikes {0,1} → treat next op input scale as 1.0\n",
    "            current_act = {\"bit_width\": 8, \"scale\": 1.0, \"zero_point\": 0}\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"[emit] C++ headers written to: {os.path.abspath(out_dir)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3c136c7-1dd4-46f3-985d-4ad6344dbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emit_headers_for_model(qcsnet2_eval, torch.randn(1,1,180), out_dir=\"weights_sd/headers_int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccebaf99-dd35-4527-8e6d-4c64af3888ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  tensor(0.4135)\n",
      "beta:  tensor(0.8281)\n",
      "beta:  tensor(1.0004)\n",
      "[emit] C++ headers written to: /home/velox-217533/Projects/fau_projects/research/snn_quant/model2/intra_patient_models/weights_sd/intra_patient/headers_stage1\n"
     ]
    }
   ],
   "source": [
    "# Extract weights from Fold 5\n",
    "model_stage1 = create_qcsnn_model()\n",
    "model_stage1.load_state_dict(fold_ckpts['fold5']['state_dict_cpu'])\n",
    "model_stage1.eval()\n",
    "\n",
    "# Create example input\n",
    "example_input = torch.randn(1, 1, 180)\n",
    "\n",
    "# Extract to C++ headers\n",
    "emit_headers_for_model(\n",
    "    model=model_stage1,\n",
    "    example_input=example_input,\n",
    "    out_dir=\"weights_sd/intra_patient/headers_stage1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff2c21-896a-45a1-b6fb-e2f11765f180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb656a3d-92f4-4ac3-ad68-2d2fcd86fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary - Plain argmax:       0.9362 (93.62%)\n",
      "Binary - Threshold 0.70:     0.9484 (94.84%)\n",
      "\n",
      "Abnormal class:\n",
      "Argmax      - Prec: 0.6375, Rec: 0.9037\n",
      "Threshold   - Prec: 0.7067, Rec: 0.8658\n"
     ]
    }
   ],
   "source": [
    "# Binary model - plain argmax test\n",
    "model_binary = create_qcsnn_model()  # 2-class model\n",
    "model_binary.load_state_dict(fold_ckpts['fold5']['state_dict_cpu'])\n",
    "model_binary.to(device)\n",
    "model_binary.eval()\n",
    "\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True).long()\n",
    "        \n",
    "        out, _ = forward_pass(model_binary, num_steps, x)\n",
    "        out = out.mean(0)  # (B, 2) - logits\n",
    "        \n",
    "        all_logits.append(out.cpu())\n",
    "        all_labels.append(y.cpu())\n",
    "\n",
    "all_logits = torch.cat(all_logits)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "# Plain argmax (no threshold)\n",
    "preds_argmax = all_logits.argmax(dim=1)\n",
    "\n",
    "# Softmax + threshold 0.70\n",
    "probs = torch.softmax(all_logits, dim=1)\n",
    "preds_thresh = (probs[:, 1] >= 0.70).long()\n",
    "\n",
    "# Compare\n",
    "acc_argmax = (preds_argmax == all_labels).float().mean().item()\n",
    "acc_thresh = (preds_thresh == all_labels).float().mean().item()\n",
    "\n",
    "print(f\"Binary - Plain argmax:       {acc_argmax:.4f} ({acc_argmax*100:.2f}%)\")\n",
    "print(f\"Binary - Threshold 0.70:     {acc_thresh:.4f} ({acc_thresh*100:.2f}%)\")\n",
    "\n",
    "# Abnormal class metrics\n",
    "mask_abnormal = all_labels == 1\n",
    "mask_normal = all_labels == 0\n",
    "\n",
    "# Argmax\n",
    "rec_argmax_abn = (preds_argmax[mask_abnormal] == 1).float().mean().item()\n",
    "prec_argmax_abn = (preds_argmax == 1).sum().item() / ((preds_argmax == 1).sum().item() + 1e-8)\n",
    "tp_argmax = ((preds_argmax == 1) & (all_labels == 1)).sum().item()\n",
    "fp_argmax = ((preds_argmax == 1) & (all_labels == 0)).sum().item()\n",
    "prec_argmax_abn = tp_argmax / (tp_argmax + fp_argmax + 1e-8)\n",
    "\n",
    "# Threshold\n",
    "rec_thresh_abn = (preds_thresh[mask_abnormal] == 1).float().mean().item()\n",
    "tp_thresh = ((preds_thresh == 1) & (all_labels == 1)).sum().item()\n",
    "fp_thresh = ((preds_thresh == 1) & (all_labels == 0)).sum().item()\n",
    "prec_thresh_abn = tp_thresh / (tp_thresh + fp_thresh + 1e-8)\n",
    "\n",
    "print(f\"\\nAbnormal class:\")\n",
    "print(f\"Argmax      - Prec: {prec_argmax_abn:.4f}, Rec: {rec_argmax_abn:.4f}\")\n",
    "print(f\"Threshold   - Prec: {prec_thresh_abn:.4f}, Rec: {rec_thresh_abn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91234745-fe31-4f3f-a7cd-23dd6beb9a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3153ec2c-48b2-4bae-8ec4-6a0de83c62ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: fold5_binary_fpga_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# SAVE EXTRACTED WEIGHTS FOR FPGA DEPLOYMENT (for verification)\n",
    "# ===================================================================\n",
    "\n",
    "# 1. Binary model (Fold 5)\n",
    "model_binary = create_qcsnn_model()\n",
    "model_binary.load_state_dict(fold_ckpts['fold5']['state_dict_cpu'])\n",
    "torch.save(model_binary.state_dict(), 'weights_sd/fold5_binary_fpga_weights.pth')\n",
    "print(\"Saved: fold5_binary_fpga_weights.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240ebe4-03f9-402a-bd58-2313b282b743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
